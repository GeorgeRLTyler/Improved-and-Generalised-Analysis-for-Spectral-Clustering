{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "from graph import Graph\n",
    "from helpers import get_hermitian_adjacency_matrix, get_adjacency_matrix, get_degree_matrix, get_laplacian_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "font = {'size'   : 20}\n",
    "\n",
    "plt.rc('font', **font)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b18f96023d7457f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_bounds_RST(eigenvalues, rayleigh_quotients, q):\n",
    "    \"\"\"eigenvalues is an array of the eigenvalues of the matrix\n",
    "    expected_eigenvalues is an array of the eigenvalues of the expected matrix\n",
    "    q is a list of indices. If p = len(q), then we return the lower bound for A_1, A_2, ... A_p.\n",
    "    For example, if q = [2,5] then we return the lower bounds for A_1 and A_2 where A_1 corresponds to the first two rows of alpha and A_2 corresponds to the next three rows of alpha.\"\"\"\n",
    "    A_1_lower_bound =(q[0]*eigenvalues[q[0]] - np.sum(rayleigh_quotients[0:q[0]]))/(eigenvalues[q[0]] - eigenvalues[0])\n",
    "    lower_bounds = [A_1_lower_bound]\n",
    "    for i in range(1,len(q)):\n",
    "        width = q[i] - q[i-1]\n",
    "        lower_bound = (width*eigenvalues[q[i]] - np.sum(rayleigh_quotients[q[i-1]:q[i]]) - eigenvalues[q[i]]*(q[i-1] - np.sum(lower_bounds)))/(eigenvalues[q[i]] - eigenvalues[q[i-1]])\n",
    "        lower_bounds.append(lower_bound)\n",
    "    return lower_bounds"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b219872c73fcf3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def k_means_indicator_vectors(eigenvectors, K):\n",
    "    assert eigenvectors.shape[1] >= K, 'Number of eigenvectors should be greater than or equal to K'\n",
    "    kmeans = KMeans(n_clusters=K, random_state=0).fit(eigenvectors)\n",
    "    indicator_vectors = np.zeros((eigenvectors.shape[0], K))\n",
    "    for i in range(K):\n",
    "        indicator_vectors[:,i] = kmeans.labels_ == i\n",
    "    return indicator_vectors\n",
    "\n",
    "def degree_correction(vectors, D_sqrt):\n",
    "    vectors_corrected = vectors.copy()\n",
    "    for i in range(vectors.shape[1]):\n",
    "        vectors_corrected[:,i] = D_sqrt @ vectors[:,i]\n",
    "        vectors_corrected[:,i] = vectors_corrected[:,i] / np.linalg.norm(vectors_corrected[:,i])\n",
    "    return vectors_corrected"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4af2ad25eccfaf4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_k_way_estimate(normalised_L, indicator_vectors, K):\n",
    "    k_way_possibilities = []\n",
    "    assert indicator_vectors.shape[1] == K, 'Indicator vectors should have K columns'\n",
    "    for i in range(K):\n",
    "        indicator = indicator_vectors[:, i]\n",
    "        val = indicator.T @ normalised_L @ indicator\n",
    "        k_way_possibilities.append(val)\n",
    "    return max(k_way_possibilities)\n",
    "\n",
    "\n",
    "def compute_rayleigh_quotients(normalised_L, indicator_vectors, K):\n",
    "    rayleigh_quotients = []\n",
    "    assert indicator_vectors.shape[1] == K, 'Indicator vectors should have K columns'\n",
    "    for i in range(K):\n",
    "        indicator = indicator_vectors[:, i]\n",
    "        val = (indicator.T @ normalised_L @ indicator) / (indicator.T @ indicator)\n",
    "        rayleigh_quotients.append(val)\n",
    "    return rayleigh_quotients\n",
    "\n",
    "\n",
    "def compute_all_bounds(G: Graph, K: int, q: list, true_clusters: list):\n",
    "    D = get_degree_matrix(G)\n",
    "    D_sqrt = np.sqrt(D)\n",
    "    normalized_L = get_laplacian_matrix(G, normalized=True)\n",
    "\n",
    "    # compute first K eigenvectors of the normalized Laplacian\n",
    "    normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_L)\n",
    "    idx = normalized_L_eigenvalues.argsort()\n",
    "    normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "    normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "    indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "    indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "    beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "    combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "    for i in range(K):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "            combined_indicator_vectors[:, i])\n",
    "        for j in range(i):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                        combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                             i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "    rayleigh_quotients = compute_rayleigh_quotients(normalized_L, combined_indicator_vectors, K)\n",
    "    rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "    ST_standard = K * max(rayleigh_quotients) / normalized_L_eigenvalues[K]\n",
    "\n",
    "    # create indicator vectors from true clusters\n",
    "    true_indicator_vectors = np.zeros((len(G.vertices), K))\n",
    "    for i in range(K):\n",
    "        true_indicator_vectors[true_clusters[i], i] = 1\n",
    "    true_indicator_vectors = degree_correction(true_indicator_vectors, D_sqrt)\n",
    "    alpha = true_indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "    true_value = K - np.sum(np.sum(alpha ** 2, axis=1), axis=0)\n",
    "\n",
    "    recursive_ST = K - np.sum(compute_bounds_RST(normalized_L_eigenvalues, rayleigh_quotients, q))\n",
    "    general_ST = K - np.sum(compute_bounds_RST(normalized_L_eigenvalues, rayleigh_quotients, [K]))\n",
    "\n",
    "    return {'Recursive ST': recursive_ST,\n",
    "            'General ST': general_ST,\n",
    "            'ST Standard': ST_standard,\n",
    "            'True Value': true_value} "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b018c7a8414fb1f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# construct a graph from the data points using a threshold\n",
    "def construct_graph(X, threshold):\n",
    "    N = X.shape[0]\n",
    "    A = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i != j:\n",
    "                dist = np.linalg.norm(X[i] - X[j])\n",
    "                if dist < threshold:\n",
    "                    A[i, j] = 1\n",
    "    return A"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f2a81819c455a16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTING ERRORS FOR GEOMETRIC GRAPHS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fab8bd9c15660ad6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rst_bounds = {}\n",
    "st_bounds = {}\n",
    "k_way_bounds = {}\n",
    "true_values = {}\n",
    "n_clusters = 4\n",
    "sample_size = 10\n",
    "for n in np.arange(100,1000,100):\n",
    "    rst_bounds_temp = []\n",
    "    st_bounds_temp = []\n",
    "    k_way_bounds_temp = []\n",
    "    true_values_temp = []\n",
    "    print(f\"commencing computation for n = {n}\")\n",
    "    for p in range(sample_size):\n",
    "    \n",
    "        X1 = np.random.multivariate_normal(mean = [0,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X2 = np.random.multivariate_normal(mean = [0,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X3 = np.random.multivariate_normal(mean = [8,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X4 = np.random.multivariate_normal(mean = [8,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X = np.concatenate((X1, X2, X3, X4))\n",
    "        \n",
    "        A = construct_graph(X, 4)\n",
    "        degrees = np.sum(A, axis = 1)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "        norm_L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        labels = kmeans.fit_predict(eigvecs[:, 0:n_clusters])\n",
    "        \n",
    "        #compute exact indicator vectors (from clusters from distributions not k means)\n",
    "        exact_indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[i*n:(i+1)*n,i] = 1\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[:, i] = D_sqrt @ exact_indicator_vectors[:, i]\n",
    "            exact_indicator_vectors[:, i] = exact_indicator_vectors[:, i] / np.linalg.norm(exact_indicator_vectors[:, i])\n",
    "        \n",
    "        true_val = np.linalg.norm(eigvecs[:,:n_clusters] - exact_indicator_vectors @ (exact_indicator_vectors.T @ eigvecs[:,:n_clusters]), ord = 'fro')**2\n",
    "        \n",
    "        # indicator vectors from k means\n",
    "        indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = (labels == i).astype(int)\n",
    "        \n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = D_sqrt @ indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i] / np.linalg.norm(indicator_vectors[:, i])\n",
    "        \n",
    "        # project indicator vectors onto the eigenvectors\n",
    "        beta_K_by_K = (indicator_vectors.T @ eigvecs[:,:n_clusters])\n",
    "        \n",
    "        combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "                combined_indicator_vectors[:, i])\n",
    "            for j in range(i):\n",
    "                combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                            combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                                 i]) * combined_indicator_vectors[:, j]\n",
    "            \n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(combined_indicator_vectors[:, i])\n",
    "            \n",
    "            \n",
    "        # compute the rayleigh quotients\n",
    "        rayleigh_quotients = []\n",
    "        for i in range(n_clusters):\n",
    "            indicator = combined_indicator_vectors[:, i]\n",
    "            val = (indicator.T @ norm_L @ indicator) / (indicator.T @ indicator)\n",
    "            rayleigh_quotients.append(val)\n",
    "        \n",
    "        # sort the rayleigh quotients\n",
    "        sorted_rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        B_1 = (sorted_rayleigh_quotients[0] + sorted_rayleigh_quotients[1])/eigvals[2]\n",
    "        B_2 = ((sorted_rayleigh_quotients[2] + sorted_rayleigh_quotients[3]) - 2*eigvals[2] + eigvals[4]*B_1)/(eigvals[4] - eigvals[2])\n",
    "        \n",
    "        rst_bound = B_1 + B_2\n",
    "        st_bound = (np.sum(sorted_rayleigh_quotients))/(eigvals[4])\n",
    "        rho = compute_k_way_estimate(norm_L, indicator_vectors, 4)\n",
    "        k_way_bound = 4*rho/eigvals[4]\n",
    "        \n",
    "        rst_bounds_temp.append(rst_bound)\n",
    "        st_bounds_temp.append(st_bound)\n",
    "        k_way_bounds_temp.append(k_way_bound)\n",
    "        true_values_temp.append(true_val)\n",
    "    \n",
    "    rst_bounds[n] = np.mean(rst_bounds_temp)\n",
    "    st_bounds[n] = np.mean(st_bounds_temp)\n",
    "    k_way_bounds[n] = np.mean(k_way_bounds_temp)\n",
    "    true_values[n] = np.mean(true_values_temp)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b741bead5bee0a14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds_df = pd.DataFrame([rst_bounds, st_bounds, k_way_bounds, true_values]).T\n",
    "bounds_df.columns = [r'Theorem 4',r'Theorem 1', r'MacGregor & Sun', r'True Value']\n",
    "bounds_df.to_csv(\"Review-Data/4GeometricClusters2Pairs.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11b573ea345efc14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "(bounds_df/4).plot(linestyle='--', marker='o', figsize = (8,8))\n",
    "plt.ylim(0, 0.3)\n",
    "plt.xlim(100,910)\n",
    "#plt.legend(bbox_to_anchor=(1,1.05))\n",
    "plt.xlabel(\"cluster size, n\")\n",
    "plt.ylabel(\"Value\")\n",
    "#plt.title(r\"Bounds for $\\frac{1}{4}\\sum_{i=1}^4\\|f_i - \\hat{g}_i\\|^2$ generated from Gaussian mixture model (4 clusters, 2 pairs)\", y=1.03)\n",
    "plt.savefig(\"Review-Data/BoundsGaussianMixtureModel4Clusters.png\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95c69feb88dccfd7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy = pd.read_csv(\"Review-Data/4GeometricClusters2Pairs.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "(df_copy.loc[200:,:] / 4).plot(marker = 'o', xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$', fontsize = 20)\n",
    "plt.ylabel(r'Error', fontsize = 20)\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,1.2,0.1))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Review-Data/4GeometricClusters2Pairs.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "682658a70eca8bf8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Driving the means of the gaussians apart from each other."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96dbde7ac9f18a51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rst_bounds = {}\n",
    "st_bounds = {}\n",
    "k_way_bounds = {}\n",
    "true_values = {}\n",
    "n_clusters = 4\n",
    "n = 100\n",
    "sample_size = 10\n",
    "for d in np.arange(5, 12, 0.5):\n",
    "    rst_bounds_temp = []\n",
    "    st_bounds_temp = []\n",
    "    k_way_bounds_temp = []\n",
    "    true_values_temp = []\n",
    "    print(f\"commencing computation for d = {d}\")\n",
    "    for p in range(sample_size):\n",
    "    \n",
    "        X1 = np.random.multivariate_normal(mean = [0,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X2 = np.random.multivariate_normal(mean = [0,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X3 = np.random.multivariate_normal(mean = [d,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X4 = np.random.multivariate_normal(mean = [d,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X = np.concatenate((X1, X2, X3, X4))\n",
    "        \n",
    "        A = construct_graph(X, 4)\n",
    "        degrees = np.sum(A, axis = 1)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "        norm_L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        labels = kmeans.fit_predict(eigvecs[:, 0:n_clusters])\n",
    "        \n",
    "        #compute exact indicator vectors (from clusters from distributions not k means)\n",
    "        exact_indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[i*n:(i+1)*n,i] = 1\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[:, i] = D_sqrt @ exact_indicator_vectors[:, i]\n",
    "            exact_indicator_vectors[:, i] = exact_indicator_vectors[:, i] / np.linalg.norm(exact_indicator_vectors[:, i])\n",
    "        \n",
    "        true_val = np.linalg.norm(eigvecs[:,:n_clusters] - exact_indicator_vectors @ (exact_indicator_vectors.T @ eigvecs[:,:n_clusters]), ord = 'fro')**2\n",
    "        \n",
    "        # indicator vectors from k means\n",
    "        indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = (labels == i).astype(int)\n",
    "        \n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = D_sqrt @ indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i] / np.linalg.norm(indicator_vectors[:, i])\n",
    "        \n",
    "        # project indicator vectors onto the eigenvectors\n",
    "        beta_K_by_K = (indicator_vectors.T @ eigvecs[:,:n_clusters])\n",
    "        \n",
    "        combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "                combined_indicator_vectors[:, i])\n",
    "            for j in range(i):\n",
    "                combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                            combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                                 i]) * combined_indicator_vectors[:, j]\n",
    "            \n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(combined_indicator_vectors[:, i])\n",
    "            \n",
    "            \n",
    "        # compute the rayleigh quotients\n",
    "        rayleigh_quotients = []\n",
    "        for i in range(n_clusters):\n",
    "            indicator = combined_indicator_vectors[:, i]\n",
    "            val = (indicator.T @ norm_L @ indicator) / (indicator.T @ indicator)\n",
    "            rayleigh_quotients.append(val)\n",
    "        \n",
    "        # sort the rayleigh quotients\n",
    "        sorted_rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        B_1 = (sorted_rayleigh_quotients[0] + sorted_rayleigh_quotients[1])/eigvals[2]\n",
    "        B_2 = ((sorted_rayleigh_quotients[2] + sorted_rayleigh_quotients[3]) - 2*eigvals[2] + eigvals[4]*B_1)/(eigvals[4] - eigvals[2])\n",
    "        \n",
    "        rst_bound = B_1 + B_2\n",
    "        st_bound = (np.sum(sorted_rayleigh_quotients))/(eigvals[4])\n",
    "        rho = compute_k_way_estimate(norm_L, indicator_vectors, 4)\n",
    "        k_way_bound = 4*rho/eigvals[4]\n",
    "        \n",
    "        rst_bounds_temp.append(rst_bound)\n",
    "        st_bounds_temp.append(st_bound)\n",
    "        k_way_bounds_temp.append(k_way_bound)\n",
    "        true_values_temp.append(true_val)\n",
    "    \n",
    "    rst_bounds[d] = np.mean(rst_bounds_temp)\n",
    "    st_bounds[d] = np.mean(st_bounds_temp)\n",
    "    k_way_bounds[d] = np.mean(k_way_bounds_temp)\n",
    "    true_values[d] = np.mean(true_values_temp)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f25da38c84618db7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#bounds_df = pd.DataFrame([rst_bounds, st_bounds, k_way_bounds, true_values]).T\n",
    "#bounds_df.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "#bounds_df.to_csv(\"Review-Data/4GeometricClusters2PairsDriftingApart.csv\")\n",
    "\n",
    "bounds_df = pd.read_csv(\"Review-Data/4GeometricClusters2PairsDriftingApart.csv\", usecols=lambda column: column != \"Unnamed: 0\")\n",
    "\n",
    "fig = plt.figure()\n",
    "(bounds_df / 4).plot(marker='x', markersize=10, figsize=(12, 10))\n",
    "\n",
    "#plt.legend(bbox_to_anchor=(1,1.05))\n",
    "plt.xlabel(\"Distance, d\", fontsize=30)\n",
    "plt.ylabel(\"Error\", fontsize=30)\n",
    "plt.legend(fontsize=25)\n",
    "plt.grid(visible=True, which='both', linewidth=1.5)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1), fontsize=25)\n",
    "# plt.title(\n",
    "#     r\"Bounds for $\\frac{1}{4}\\sum_{i=1}^4\\|f_i - \\hat{g}_i\\|^2$ generated from Gaussian mixture model (4 clusters, 2 pairs)\" + \"\\n Distance between pairs of clusters increased\",\n",
    "#     y=1.03)\n",
    "plt.savefig(\"Review-Data/BoundsGaussianMixtureModel2PairsDriftingApart.png\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cd3eb06016831a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTING ERRORS FOR STOCHASTIC BLOCK MODELS\n",
    "Our initial choice is an SBM with 4 clusters and where two pairs have a high affinity for each other. We use \n",
    "$$\n",
    "P = \\begin{pmatrix}\n",
    "    0.5 & 0.4 & 0.1 & 0.1 \\\\\n",
    "    0.4 & 0.5 & 0.1 & 0.1 \\\\\n",
    "    0.1 & 0.1 & 0.5 & 0.4 \\\\\n",
    "    0.1 & 0.1 & 0.4 & 0.5 \\\\\n",
    "    \\end{pmatrix}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a93af12e2cbd59e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "P = np.array([[0.5, 0.4, 0.1, 0.1],\n",
    "              [0.4, 0.5, 0.1, 0.1],\n",
    "              [0.1, 0.1, 0.5, 0.4],\n",
    "              [0.1, 0.1, 0.4, 0.5]])\n",
    "K = 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28f0506b8be3f1b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds = {}\n",
    "sample_size = 10\n",
    "for n in [200,300,400,500,600,700,800,900]:\n",
    "    bounds[n] = 0\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i,K):\n",
    "                prob_existing_edge = P[i,j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u+1,n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "                                \n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "        \n",
    "        true_clusters = [list(range(i*n, (i+1)*n)) for i in range(K)]                        \n",
    "        G = Graph(vertices = list(range(n * K)), edges = edges)\n",
    "        bounds[n] = bounds[n] + pd.Series(compute_all_bounds(G, K, [1,2,4], true_clusters = true_clusters))\n",
    "    bounds[n] = bounds[n] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4faa9134dc6668ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "columns = ['Recursive ST', 'General ST', r'$\\frac{\\rho(4)}{\\lambda_5}$', 'True Value']\n",
    "df.columns = columns\n",
    "#df = df.drop(columns = ['General ST'])\n",
    "df.to_csv('Review-Data/Bounds4Clusters2PairsRST.csv')\n",
    "\n",
    "df = pd.read_csv(\"Review-Data/Bounds4Clusters2PairsRST.csv\")\n",
    "df = df.set_index(\"Unnamed: 0\")\n",
    "\n",
    "(df.loc[200:, :] / 4).plot(marker='o', xlabel='n', ylabel='Bound Value', figsize=(10, 10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{4}\\sum_{i=1}^4\\|f_i - \\hat{g}_i\\|^2$ for SBM with four clusters in two pairs', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$')\n",
    "plt.ylabel(r'Error')\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "plt.xticks(np.arange(200, 1000, 100))\n",
    "plt.yticks(np.arange(0, 1.2, 0.1))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='small', bbox_to_anchor=(1.0, 1.05))\n",
    "plt.savefig('Review-Data/Bounds4Clusters2PairsRST.png', bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4af52881635faf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy = pd.read_csv(\"Review-Data/Bounds4Clusters2PairsRST.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "(df_copy.loc[200:,:] / 4).plot(marker='x', markersize=10, xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$', fontsize=30)\n",
    "plt.ylabel(r'Error',fontsize=30)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,1.2,0.1))\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0,1.1,0.1),fontsize=25)\n",
    "plt.grid(visible=True, which='both', linewidth=1.5)\n",
    "# make legend larger\n",
    "plt.legend(fontsize=25, bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Review-Data/Bounds4Clusters2PairsRST.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55b7f690e7a5b29e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Repeating the experiment for an SBM with 8 clusters with a single pair that have an affinity for each other."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ef25a1449ce1471"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 8\n",
    "p1 = 0.5\n",
    "p2 = 0.3\n",
    "q = 0.05\n",
    "P = np.ones((8,8))\n",
    "P = q*P\n",
    "np.fill_diagonal(P,p1)\n",
    "P[0,1] = p2\n",
    "P[1,0] = p2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3af6bca96c95732"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds = {}\n",
    "sample_size = 10\n",
    "for n in [200,300,400,500,600,700,800,900]:\n",
    "    bounds[n] = 0\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i,K):\n",
    "                prob_existing_edge = P[i,j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u+1,n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "                                \n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "        \n",
    "        true_clusters = [list(range(i*n, (i+1)*n)) for i in range(K)]                        \n",
    "        G = Graph(vertices = list(range(n * K)), edges = edges)\n",
    "        bounds[n] = bounds[n] + pd.Series(compute_all_bounds(G, K, [1,2,7,8], true_clusters = true_clusters))\n",
    "    bounds[n] = bounds[n] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c604fffb83f27f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(bounds).T\n",
    "# df = df.loc[200:900,:]\n",
    "# columns = ['Recursive ST', 'General ST', r'$\\frac{\\rho(8)}{\\lambda_9}$', 'True Value']\n",
    "# df.columns = columns\n",
    "# #df = df.drop(columns = ['General ST'])\n",
    "# df.to_csv(\"Review-Data/Bounds8Clusters1PairRST.csv\")\n",
    "\n",
    "(df.loc[200:,:] / 8).plot(marker = 'o', xlabel = 'n', ylabel = 'Bound Value', figsize = (10,10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$')\n",
    "plt.ylabel(r'Bound Value')\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,1.2,0.1))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Review-Data/Bounds8Clusters1PairRST.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd5cd3fe85054972"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy = pd.read_csv(\"Review-Data/Bounds8Clusters1PairRST.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "(df_copy.loc[200:,:] / 8).plot(marker = 'x', markersize=10, xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$', fontsize = 30)\n",
    "plt.ylabel(r'Error', fontsize = 30)\n",
    "plt.grid(True, which='both', linewidth=1.5)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,1.2,0.1))\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0,1.1,0.1), fontsize=25)\n",
    "# make legend larger\n",
    "plt.legend(fontsize=25, bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Review-Data/Bounds8Clusters1PairRST.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60cc2aaeef8c82c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60f54d34444ae503"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Varying n at the Threshold\n",
    "we take an SBM with two clusters and P defined as\n",
    "$$ P = \\begin{pmatrix}\n",
    "p & q \\\\\n",
    "q & p \\\\\n",
    "\\end{pmatrix}$$\n",
    "where $p = \\frac{\\alpha\\log(N)}{N}$ and $q = \\frac{\\beta \\log(N)}{N}$\n",
    "and $\\sqrt{\\alpha} - \\sqrt{\\beta} \\geq \\sqrt{2}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ad8deb162b4b8ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this experiment we will fix the values of $\\alpha, \\beta$ at the threshold with $\\beta = 20$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24b0bde6750d50fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 2\n",
    "b = 20\n",
    "a = (np.sqrt(b) + np.sqrt(K))**2\n",
    "\n",
    "def get_P(a,b,N):\n",
    "    p = a * np.log(N) / N\n",
    "    q = b * np.log(N) / N\n",
    "    P = np.array([[p,q],[q,p]])\n",
    "    return P\n",
    "\n",
    "bounds = {}\n",
    "sample_size = 10\n",
    "for n in [200,300,400,500,600,700,800,900,1000]:\n",
    "    N = K*n\n",
    "    bounds[n] = 0\n",
    "    P = get_P(a,b,N)\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i,K):\n",
    "                prob_existing_edge = P[i,j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u+1,n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "                                \n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "        \n",
    "        true_clusters = [list(range(i*n, (i+1)*n)) for i in range(K)]                        \n",
    "        G = Graph(vertices = list(range(n * K)), edges = edges)\n",
    "        bounds[n] = bounds[n] + pd.Series(compute_all_bounds(G, K, [1,2], true_clusters = true_clusters))\n",
    "    bounds[n] = bounds[n] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b16378f40e6240"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(bounds).T\n",
    "# columns = ['Corollary 5', 'General ST', r'$\\frac{\\rho(2)}{\\lambda_3}$', 'True Value']\n",
    "# df.columns = columns\n",
    "# df.to_csv(\"Review-Data/ThresholdVaryingNBeta20.csv\")\n",
    "df = pd.read_csv(\"Review-Data/ThresholdVaryingNBeta20.csv\").set_index([\"Unnamed: 0\"])\n",
    "# df = df.drop(columns = ['General ST'])\n",
    "(df.loc[200:,:] / 2).plot(marker = 'o', xlabel = 'n', ylabel = 'Error', figsize = (10,10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{2}\\sum_{i=1}^2\\|f_i - \\hat{g}_i\\|^2$ for SBM with 2 clusters at threshold' + '\\n' + r'$\\beta = 20, \\alpha = (\\sqrt{2} + \\sqrt{\\beta})^2$', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$')\n",
    "plt.ylabel(r'Error')\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,0.2,0.01))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig(\"Review-Data/ThresholdVaryingNBeta20.png\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bcf9066bebcacf8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "columns = ['Recursive ST', 'General ST', r'$\\frac{\\rho(2)}{\\lambda_3}$', 'True Value']\n",
    "df.columns = columns\n",
    "# df = df.drop(columns = ['General ST'])\n",
    "(df.loc[200:,:] / 2).plot(marker = 'o', xlabel = 'n', ylabel = 'Bound Value', figsize = (10,10), logy = True)\n",
    "# plt.title(r'Bounds of $\\frac{1}{2}\\sum_{i=1}^2\\|f_i - \\hat{g}_i\\|^2$ for SBM with 2 clusters at threshold' + '\\n' + r'$\\beta = 20, \\alpha = (\\sqrt{2} + \\sqrt{\\beta})^2$', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$')\n",
    "plt.ylabel(r'Error')\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,0.2,0.01))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig(\"Review-Data/ThresholdVaryingNBeta20LogScale.png\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbf027c27066fc6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy = pd.read_csv(\"Review-Data/ThresholdVaryingNBeta20.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "(df_copy.loc[200:,:] / 2).plot(marker = 'o', xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10), logy=True)\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$', fontsize = 30)\n",
    "plt.ylabel(r'Error', fontsize = 30)\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,1.2,0.1))\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "# make legend larger\n",
    "plt.legend(fontsize=25, bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Review-Data/ThresholdVaryingNBeta20LogScale.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d5ab10769aecd7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 2\n",
    "b = 1\n",
    "a = (np.sqrt(b) + np.sqrt(K)) ** 2\n",
    "\n",
    "\n",
    "def get_P(a,b,N):\n",
    "    p = a * np.log(N) / N\n",
    "    q = b * np.log(N) / N\n",
    "    P = np.array([[p,q],[q,p]])\n",
    "    return P\n",
    "\n",
    "\n",
    "bounds = {}\n",
    "sample_size = 10\n",
    "for n in [200, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "    N = K*n\n",
    "    bounds[n] = 0\n",
    "    P = get_P(a, b, N)\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i, K):\n",
    "                prob_existing_edge = P[i, j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u + 1, n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "\n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "\n",
    "        true_clusters = [list(range(i * n, (i + 1) * n)) for i in range(K)]\n",
    "        G = Graph(vertices=list(range(n * K)), edges=edges)\n",
    "        bounds[n] = bounds[n] + pd.Series(compute_all_bounds(G, K, [1, 2], true_clusters=true_clusters))\n",
    "    bounds[n] = bounds[n] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fd0e95b7d776d21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "columns = ['Corollary 5', 'General ST', r'$\\frac{\\rho(2)}{\\lambda_3}$', 'True Value']\n",
    "df.columns = columns\n",
    "df.to_csv(\"Review-Data/ThresholdVaryingNBeta1.csv\")\n",
    "#df = df.drop(columns=['General ST'])\n",
    "(df.loc[200:, :] / 2).plot(marker='o', xlabel='n', ylabel='Bound Value', figsize=(10, 10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{2}\\sum_{i=1}^2\\|f_i - \\hat{g}_i\\|^2$ for SBM with 2 clusters at threshold' + '\\n' + r'$\\beta = 20, \\alpha = (\\sqrt{2} + \\sqrt{\\beta})^2$', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$')\n",
    "plt.ylabel(r'Error')\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "plt.xticks(np.arange(200, 1000, 100))\n",
    "#plt.yticks(np.arange(0,0.2,0.01))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor=(1.0, 1.05))\n",
    "plt.savefig(\"ThresholdVaryingNBeta1.png\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d4176911ee87b66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "columns = ['Corollary 5', 'General ST', r'$\\frac{\\rho(2)}{\\lambda_3}$', 'True Value']\n",
    "df.columns = columns\n",
    "#df = df.drop(columns=['General ST'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8874a12d70c0bc7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"Review-Data/ThresholdVaryingNBeta1.csv\")\n",
    "df = df.set_index([\"Unnamed: 0\"])\n",
    "(df.loc[200:, :] / 2).plot(marker='o', xlabel='n', ylabel='Bound Value', figsize=(10, 10), logy=True)\n",
    "# plt.title(r'Bounds of $\\frac{1}{2}\\sum_{i=1}^2\\|f_i - \\hat{g}_i\\|^2$ for SBM with 2 clusters at threshold' + '\\n' + r'$\\beta = 20, \\alpha = (\\sqrt{2} + \\sqrt{\\beta})^2$', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$',fontsize=30)\n",
    "plt.ylabel(r'Error',fontsize=30)\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "plt.xticks(np.arange(200, 1000, 100), fontsize=25)\n",
    "\n",
    "plt.yticks(fontsize=25)\n",
    "# make legend larger\n",
    "plt.legend(fontsize=25, bbox_to_anchor = (1.0,1.05))\n",
    "#plt.yticks(np.arange(0,0.2,0.01))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor=(1.0, 1.05))\n",
    "plt.savefig(\"Review-Data/ThresholdVaryingNBeta1LogScale.png\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2d8b20531381972"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Repeating the same experiment but varying $\\beta$ with n fixed. We fix n = 500 and vary $\\beta$ from 1 to 20. We set $\\alpha = (\\sqrt{\\beta} + \\sqrt{2})^2$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56e75402649ba7f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 2\n",
    "\n",
    "def get_P(b,N):\n",
    "    a = (np.sqrt(b) + np.sqrt(K))**2\n",
    "    p = a * np.log(N) / N\n",
    "    q = b * np.log(N) / N\n",
    "    P = np.array([[p,q],[q,p]])\n",
    "    return P\n",
    "\n",
    "bounds = {}\n",
    "sample_size = 1\n",
    "n = 500\n",
    "for b in range(5,20):\n",
    "    bounds[b] = 0\n",
    "    N = K*n\n",
    "    P = get_P(b,N)\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i,K):\n",
    "                prob_existing_edge = P[i,j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u+1,n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "                                \n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "        \n",
    "        true_clusters = [list(range(i*n, (i+1)*n)) for i in range(K)]                        \n",
    "        G = Graph(vertices = list(range(n * K)), edges = edges)\n",
    "        bounds[b] = bounds[b] + pd.Series(compute_all_bounds(G, K, [1,2], true_clusters = true_clusters))\n",
    "    bounds[b] = bounds[b] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c06b89d9c05705d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "columns = ['Corollary 5', 'General ST', r'$\\frac{\\rho(2)}{\\lambda_3}$', 'True Value']\n",
    "df.columns = columns\n",
    "# df = df.drop(columns = ['General ST'])\n",
    "df.to_csv(\"Review-Data/ThresholdVaryingBetaAtThreshold.csv\")\n",
    "(df.loc[1:,:] / 2).plot(marker = 'o', xlabel = 'n', ylabel = 'Bound Value', figsize = (10,10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{2}\\sum_{i=1}^2\\|f_i - \\hat{g}_i\\|^2$ for SBM with 2 clusters at threshold' + '\\n' + r' varying $\\beta$, $\\alpha = (\\sqrt{2} + \\sqrt{\\beta})^2$, cluster size $n=500$', y=1.03)\n",
    "plt.xlabel(r'$\\beta$')\n",
    "plt.ylabel(r'Error')\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,0.2,0.01))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig(\"Review-Data/ThresholdVaryingBetaAtThreshold.png\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bca7f197d3ca06f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "columns = ['Corollary 5', 'General ST', r'$\\frac{\\rho(2)}{\\lambda_3}$', 'True Value']\n",
    "df.columns = columns\n",
    "# df = df.drop(columns = ['General ST'])\n",
    "(df.loc[1:,:] / 2).plot(marker = 'o', xlabel = 'n', ylabel = 'Bound Value', figsize = (10,10), logy=True)\n",
    "# plt.title(r'Bounds of $\\frac{1}{2}\\sum_{i=1}^2\\|f_i - \\hat{g}_i\\|^2$ for SBM with 2 clusters at threshold' + '\\n' + r' varying $\\beta$, $\\alpha = (\\sqrt{2} + \\sqrt{\\beta})^2$, cluster size $n=500$', y=1.03)\n",
    "plt.xlabel(r'$\\beta$')\n",
    "plt.ylabel(r'Error')\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,0.2,0.01))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig(\"Review-Data/ThresholdVaryingBetaAtThresholdLogScale.png\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ea367cfabd8248f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy = pd.read_csv(\"Review-Data/ThresholdVaryingBetaAtThreshold.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "(df_copy.loc[1:,:] / 2).plot(marker = 'o', xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10), logy=True)\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'$\\beta$', fontsize = 20)\n",
    "plt.ylabel(r'Error', fontsize = 20)\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,1.2,0.1))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Review-Data/ThresholdVaryingBetaAtThresholdLogScale.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4303969341fa6e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we consider fixing larger $\\alpha$ and increasing $\\beta$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "433049e461077c37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 2\n",
    "max_b = 20\n",
    "a = 35\n",
    "\n",
    "def get_P(b, n):\n",
    "    \n",
    "    p = a * np.log(n) / n\n",
    "    q = b * np.log(n) / n\n",
    "    P = np.array([[p, q], [q, p]])\n",
    "    return P\n",
    "\n",
    "\n",
    "bounds = {}\n",
    "sample_size = 10\n",
    "n = 500\n",
    "\n",
    "for b in range(10, max_b + 10):\n",
    "    bounds[b] = 0\n",
    "    P = get_P(b, n)\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i, K):\n",
    "                prob_existing_edge = P[i, j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u + 1, n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "\n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "\n",
    "        true_clusters = [list(range(i * n, (i + 1) * n)) for i in range(K)]\n",
    "        G = Graph(vertices=list(range(n * K)), edges=edges)\n",
    "        bounds[b] = bounds[b] + pd.Series(compute_all_bounds(G, K, [1, 2], true_clusters=true_clusters))\n",
    "    bounds[b] = bounds[b] / sample_size\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa13714221c40ad1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    \n",
    "df = pd.DataFrame(bounds).T\n",
    "columns = ['Corollary 5', 'General ST', r'$\\frac{\\rho(2)}{\\lambda_3}$', 'True Value']\n",
    "df.columns = columns\n",
    "#df = df.drop(columns=['General ST'])\n",
    "df.to_csv(\"Review-Data/ThresholdVaryingBetaFixedAlpha.csv\")\n",
    "(df.loc[1:, :] / 2).plot(marker='o', xlabel='n', ylabel='Bound Value', figsize=(10, 10))\n",
    "#plt.title(\n",
    " #   r'Bounds of $\\frac{1}{2}\\sum_{i=1}^2\\|f_i - \\hat{g}_i\\|^2$ for SBM with 2 clusters at threshold' + '\\n' + r' varying $\\beta$, $\\alpha = (\\sqrt{2} + \\sqrt{20})^2$, cluster size $n=500$',\n",
    " #   y=1.03)\n",
    "plt.xlabel(r'$\\beta$')\n",
    "plt.ylabel(r'Error')\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,0.2,0.01))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor=(1.0, 1.05))\n",
    "plt.savefig(\"Review-Data/ThresholdVaryingBetaFixedAlpha.png\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3af75e7111ecbc96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "columns = ['Corollary 5', 'General ST', r'$\\frac{\\rho(2)}{\\lambda_3}$', 'True Value']\n",
    "df.columns = columns\n",
    "# df = df.drop(columns=['General ST'])\n",
    "df.to_csv(\"Review-Data/ThresholdVaryingBetaFixedAlpha.csv\")\n",
    "(df.loc[1:, :] / 2).plot(marker='o', xlabel='n', ylabel='Bound Value', figsize=(10, 10), logy=True)\n",
    "#plt.title(\n",
    " #   r'Bounds of $\\frac{1}{2}\\sum_{i=1}^2\\|f_i - \\hat{g}_i\\|^2$ for SBM with 2 clusters at threshold' + '\\n' + r' varying $\\beta$, $\\alpha = (\\sqrt{2} + \\sqrt{20})^2$, cluster size $n=500$',\n",
    " #   y=1.03)\n",
    "plt.xlabel(r'$\\beta$')\n",
    "plt.ylabel(r'Error')\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,0.2,0.01))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor=(1.0, 1.05))\n",
    "plt.savefig(\"Review-Data/ThresholdVaryingBetaFixedAlphaLogScale.png\", bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab1ecaa673bcb834"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_copy = pd.read_csv(\"Review-Data/ThresholdVaryingBetaFixedAlpha.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "(df_copy.loc[1:,:] / 2).plot(marker = 'o', xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10), logy=True)\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'$\\beta$', fontsize = 20)\n",
    "plt.ylabel(r'Error', fontsize = 20)\n",
    "plt.grid(True)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,1.2,0.1))\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Review-Data/ThresholdVaryingBetaFixedAlphaLogScale.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83164178242d3480"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Real-World Graphs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1d0ea9d25baa6e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data: Flatten the images to 1D arrays\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "\n",
    "# Select 3 digits (e.g., 0, 1, and 2) and restrict to 200 samples each\n",
    "digits = [0, 1, 2]\n",
    "samples_per_digit = 200\n",
    "selected_samples = []\n",
    "selected_labels = []\n",
    "\n",
    "for digit in digits:\n",
    "    indices = np.where(y_train == digit)[0][:samples_per_digit]\n",
    "    selected_samples.append(x_train[indices])\n",
    "    selected_labels.extend(y_train[indices])\n",
    "\n",
    "# Combine the samples into a single dataset\n",
    "selected_samples = np.vstack(selected_samples)\n",
    "selected_labels = np.array(selected_labels)\n",
    "\n",
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix = np.corrcoef(selected_samples)\n",
    "\n",
    "# Create a graph using the correlation matrix\n",
    "threshold = 0.8  # Define a threshold for edge creation\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with their corresponding digit label\n",
    "for i, label in enumerate(selected_labels):\n",
    "    G.add_node(i, label=int(label))\n",
    "\n",
    "# Add edges for high correlations\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i + 1, correlation_matrix.shape[0]):\n",
    "        if correlation_matrix[i, j] > threshold:\n",
    "            G.add_edge(i, j, weight=correlation_matrix[i, j])\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Assign colors based on digit labels\n",
    "colors = [\"red\" if selected_labels[node] == 0 else \"blue\" if selected_labels[node] == 1 else \"green\" for node in G.nodes()]\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # Use a spring layout for visualization\n",
    "nx.draw(G, pos, node_color=colors, with_labels=False, node_size=50, edge_color=\"gray\", alpha=0.7)\n",
    "plt.title(\"Graph of MNIST Samples by Correlation\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14fbc62910cf22df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the MNIST dataset using OpenML\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "x_data, y_data = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Select 3 digits (e.g., 0, 1, and 2) and restrict to 200 samples each\n",
    "digits = [0, 1, 2]\n",
    "samples_per_digit = 200\n",
    "selected_samples = []\n",
    "selected_labels = []\n",
    "\n",
    "for digit in digits:\n",
    "    indices = np.where(y_data == digit)[0][:samples_per_digit]\n",
    "    selected_samples.append(x_data[indices])\n",
    "    selected_labels.extend(y_data[indices])\n",
    "\n",
    "# Combine the samples into a single dataset\n",
    "selected_samples = np.vstack(selected_samples) / 255.0  # Normalize pixel values\n",
    "selected_labels = np.array(selected_labels)\n",
    "\n",
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix = np.corrcoef(selected_samples)\n",
    "\n",
    "# Create a graph using the correlation matrix\n",
    "threshold = 0.7  # Define a threshold for edge creation\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with their corresponding digit label\n",
    "for i, label in enumerate(selected_labels):\n",
    "    G.add_node(i, label=int(label))\n",
    "\n",
    "# Add edges for high correlations\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i + 1, correlation_matrix.shape[0]):\n",
    "        if correlation_matrix[i, j] > threshold:\n",
    "            G.add_edge(i, j, weight=correlation_matrix[i, j])\n",
    "\n",
    "# Remove disconnected components (keep the largest connected component)\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Assign colors based on digit labels\n",
    "colors = [\"red\" if G.nodes[node]['label'] == 0 else\n",
    "          \"blue\" if G.nodes[node]['label'] == 1 else\n",
    "          \"green\" for node in G.nodes()]\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # Use a spring layout for visualization\n",
    "nx.draw(G, pos, node_color=colors, with_labels=False, node_size=50, edge_color=\"gray\", alpha=0.7)\n",
    "plt.title(\"Graph of MNIST Samples by Correlation (Largest Connected Component)\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9440006285a9ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the adjacency matrix of the graph\n",
    "adjacency_matrix = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "# Plot the adjacency matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(np.array(adjacency_matrix), cmap='viridis', square=True, cbar=True, xticklabels=False, yticklabels=False)\n",
    "plt.title(\"Adjacency Matrix Heatmap\")\n",
    "plt.xlabel(\"Nodes\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4318fe1abad25976"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalized_laplacian = nx.normalized_laplacian_matrix(G).todense()\n",
    "# Compute the eigenvalues and eigenvectors of the normalized Laplacian\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "\n",
    "# Print the eigenvalues\n",
    "print(\"Eigenvalues of the Normalized Laplacian:\")\n",
    "print(eigenvalues)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57281a000e789de1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.scatterplot(x= np.arange(5),y=eigenvalues[:5])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b725abf965a376d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute first K eigenvectors of the normalized Laplacian\n",
    "K=3\n",
    "normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "idx = normalized_L_eigenvalues.argsort()\n",
    "normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "degrees = np.sum(adjacency_matrix, axis = 0)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "\n",
    "indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "for i in range(K):\n",
    "    combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "        combined_indicator_vectors[:, i])\n",
    "    for j in range(i):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                    combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                         i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "rayleigh_quotients = compute_rayleigh_quotients(normalized_laplacian, combined_indicator_vectors, K)\n",
    "rayleigh_quotients = np.sort(rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0a08d0256c0ef97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rayleigh_quotients"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dff1cd77e4bf56a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.scatterplot(normalized_L_eigenvalues[0:5])\n",
    "sns.scatterplot(rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd514b26e37b6224"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B_1 = (rayleigh_quotients[0]+rayleigh_quotients[1])/(normalized_L_eigenvalues[2])\n",
    "B_2 = (rayleigh_quotients[2] - normalized_L_eigenvalues[2] + normalized_L_eigenvalues[3]*B_1)/(normalized_L_eigenvalues[3] - normalized_L_eigenvalues[2])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c47fa20b29d6ebae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B_1 + B_2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c55405b57d5132f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B_1 = rayleigh_quotients[0]/normalized_L_eigenvalues[1]\n",
    "B_2 = ((rayleigh_quotients[1] - normalized_L_eigenvalues[1]) + normalized_L_eigenvalues[2]*B_1)/(normalized_L_eigenvalues[2] - normalized_L_eigenvalues[1])\n",
    "B_3 = ((rayleigh_quotients[2] - normalized_L_eigenvalues[2]) + normalized_L_eigenvalues[3]*(B_1+B_2))/(normalized_L_eigenvalues[3] - normalized_L_eigenvalues[2])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b16d71eb52e8858"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(B_1 + B_2 + B_3)/3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "729916c0bd06ebb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(np.sum(rayleigh_quotients)/normalized_L_eigenvalues[3])/3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "929a8f8873322848"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max(rayleigh_quotients)/normalized_L_eigenvalues[3]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "208462f6042d353a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(adjacency_matrix).to_csv(\"Review-Data/MNIST_adjacency_matrix.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53ae1489c2874bf2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Extract the labels of nodes in the largest connected component\n",
    "remaining_labels = [G.nodes[node]['label'] for node in G.nodes()]\n",
    "\n",
    "# Count the occurrences of each digit\n",
    "digit_counts = Counter(remaining_labels)\n",
    "\n",
    "# Print the counts\n",
    "print(\"Counts of each digit in the largest connected component:\")\n",
    "print(digit_counts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1ad47ecdfc580df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming remaining_labels is already populated with the labels of the nodes in the largest connected component\n",
    "# For example:\n",
    "# remaining_labels = [0, 1, 1, 2, 0, 2, 1, 0, ...]\n",
    "\n",
    "# Convert remaining labels to numpy array for easier manipulation\n",
    "remaining_labels = np.array(remaining_labels)\n",
    "\n",
    "# Number of clusters (in this case, 3)\n",
    "num_clusters = 3\n",
    "\n",
    "# Initialize the indicator vectors (one per cluster)\n",
    "indicator_vectors = np.zeros((num_clusters, len(remaining_labels)))\n",
    "\n",
    "# Assign each node in remaining_labels to the appropriate indicator vector\n",
    "for i, label in enumerate(remaining_labels):\n",
    "    if label < num_clusters:\n",
    "        indicator_vectors[label, i] = 1\n",
    "\n",
    "# Print the indicator vectors\n",
    "for i, vec in enumerate(indicator_vectors):\n",
    "    print(f\"Indicator vector for cluster {i}: {vec}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42b6b4f3c341efb3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ind_vecs_transposed = indicator_vectors.T"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "476c35771fa690d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ind_vecs_transposed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40d5eb5be9919eec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    ind_vecs_transposed[:,i] = D_sqrt @ ind_vecs_transposed[:,i]\n",
    "    ind_vecs_transposed[:,i] = ind_vecs_transposed[:,i]/ np.linalg.norm(ind_vecs_transposed[:,i])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d45932663705e91e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_val_matrix_combined = normalized_L_eigenvectors[:,:3] - ind_vecs_transposed @ (ind_vecs_transposed.T @ normalized_L_eigenvectors[:,:3]) \n",
    "# true val is frobenius norm of the matrix\n",
    "true_val_combined = np.linalg.norm(true_val_matrix_combined)**2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85809a5615db1ccf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_val_combined/3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6587a1b74681e382"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(normalized_L_eigenvectors[:,:3])\n",
    "kmeans.labels_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "707de43262695bda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_labels = kmeans.labels_\n",
    "misclassified_vertices = []\n",
    "# Step 3: Measure how many were misclassified\n",
    "# We will compare the predicted cluster labels to the true labels\n",
    "misclassifications = 0\n",
    "for i in range(num_clusters):\n",
    "    # Get the indices of the points that belong to cluster i\n",
    "    cluster_indices = np.where(predicted_labels == i)[0]\n",
    "    \n",
    "    # Get the majority label in the current cluster\n",
    "    cluster_true_labels = remaining_labels[cluster_indices]\n",
    "    majority_label = np.bincount(cluster_true_labels).argmax()\n",
    "    \n",
    "    # Count how many points in this cluster were misclassified\n",
    "    misclassified_vertices.append(cluster_true_labels != majority_label)\n",
    "    \n",
    "    misclassified_points_sum = np.sum(cluster_true_labels != majority_label)\n",
    "    misclassifications += misclassified_points_sum\n",
    "\n",
    "# Step 4: Print the number of misclassified points\n",
    "print(f\"Number of misclassified points: {misclassifications}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d3fdbb95e9a09ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "misclassified_vertices"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b15bf7b8a60118ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have `remaining_labels` (true labels) and `predicted_labels` from K-means\n",
    "# G is the graph (networkx Graph object)\n",
    "\n",
    "# Step 1: Identify misclassified vertices\n",
    "misclassified_vertices = np.where(remaining_labels != predicted_labels)[0]\n",
    "\n",
    "# Step 2: Compute the volume of the misclassified vertices\n",
    "volume = 0\n",
    "for vertex in misclassified_vertices:\n",
    "    volume += G.degree(vertex)\n",
    "\n",
    "# Step 3: Print the volume of the misclassified vertices\n",
    "print(f\"Volume of the misclassified vertices: {volume}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b47f9ec400f0ae2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "misclassified_vertices"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c69c726dfbb8831"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Step 1: Create a boolean vector for misclassifications\n",
    "misclassified_vector = np.zeros(len(remaining_labels), dtype=bool)\n",
    "\n",
    "# Step 2: Find the majority label for each cluster\n",
    "for cluster_id in range(3):  # For each cluster\n",
    "    # Get the indices of the nodes assigned to this cluster\n",
    "    cluster_indices = np.where(predicted_labels == cluster_id)[0]\n",
    "    \n",
    "    # Get the true labels of the nodes in this cluster\n",
    "    cluster_true_labels = remaining_labels[cluster_indices]\n",
    "    \n",
    "    # Find the majority true label for the nodes in this cluster\n",
    "    majority_label = np.bincount(cluster_true_labels).argmax()\n",
    "    \n",
    "    # Step 3: Mark misclassified nodes in the boolean vector\n",
    "    for idx in cluster_indices:\n",
    "        if remaining_labels[idx] != majority_label:\n",
    "            misclassified_vector[idx] = True\n",
    "\n",
    "# Step 4: Print the misclassified vector and count misclassifications\n",
    "print(\"Misclassified vector:\", misclassified_vector)\n",
    "print(f\"Number of misclassified points: {np.sum(misclassified_vector)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78dd7ed456962cf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(D_sqrt @ misclassified_vector).T @ adjacency_matrix @ D_sqrt @ misclassified_vector"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47123c85445dcb7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "misclassified_vector"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29b7b8326485bc88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "volume = 0\n",
    "nodes = list(G.nodes)\n",
    "for vertex in range(len(misclassified_vector)):\n",
    "    if misclassified_vector[vertex]:\n",
    "        volume += G.degree(nodes[vertex])\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d39aab95e0d00d26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "volume"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23a6c39f6fa545b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "degrees = [G.degree(n) for n in nodes]\n",
    "np.mean(degrees)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8e7a012cb71a505"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fashion MNIST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbaf1cd34929b0ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the MNIST dataset using OpenML\n",
    "mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "x_data, y_data = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Select 3 digits (e.g., 0, 1, and 2) and restrict to 200 samples each\n",
    "digits = [0, 1, 2]\n",
    "samples_per_digit = 200\n",
    "selected_samples = []\n",
    "selected_labels = []\n",
    "\n",
    "for digit in digits:\n",
    "    indices = np.where(y_data == digit)[0][:samples_per_digit]\n",
    "    selected_samples.append(x_data[indices])\n",
    "    selected_labels.extend(y_data[indices])\n",
    "\n",
    "# Combine the samples into a single dataset\n",
    "selected_samples = np.vstack(selected_samples) / 255.0  # Normalize pixel values\n",
    "selected_labels = np.array(selected_labels)\n",
    "\n",
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix = np.corrcoef(selected_samples)\n",
    "\n",
    "# Create a graph using the correlation matrix\n",
    "threshold = 0.7  # Define a threshold for edge creation\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with their corresponding digit label\n",
    "for i, label in enumerate(selected_labels):\n",
    "    G.add_node(i, label=int(label))\n",
    "\n",
    "# Add edges for high correlations\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i + 1, correlation_matrix.shape[0]):\n",
    "        if correlation_matrix[i, j] > threshold:\n",
    "            G.add_edge(i, j, weight=correlation_matrix[i, j])\n",
    "\n",
    "# Remove disconnected components (keep the largest connected component)\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Assign colors based on digit labels\n",
    "colors = [\"red\" if G.nodes[node]['label'] == 0 else\n",
    "          \"blue\" if G.nodes[node]['label'] == 1 else\n",
    "          \"green\" for node in G.nodes()]\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # Use a spring layout for visualization\n",
    "nx.draw(G, pos, node_color=colors, with_labels=False, node_size=50, edge_color=\"gray\", alpha=0.7)\n",
    "plt.title(\"Graph of MNIST Samples by Correlation (Largest Connected Component)\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43484d18611e10ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the adjacency matrix of the graph\n",
    "adjacency_matrix = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "# Plot the adjacency matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(np.array(adjacency_matrix), cmap='viridis', square=True, cbar=True, xticklabels=False, yticklabels=False)\n",
    "plt.title(\"Adjacency Matrix Heatmap\")\n",
    "plt.xlabel(\"Nodes\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.show()\n",
    "\n",
    "normalized_laplacian = nx.normalized_laplacian_matrix(G).todense()\n",
    "# Compute the eigenvalues and eigenvectors of the normalized Laplacian\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "\n",
    "# Print the eigenvalues\n",
    "print(\"Eigenvalues of the Normalized Laplacian:\")\n",
    "print(eigenvalues)\n",
    "\n",
    "sns.scatterplot(x=np.arange(5), y=eigenvalues[:5])\n",
    "# compute first K eigenvectors of the normalized Laplacian\n",
    "K = 3\n",
    "normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "idx = normalized_L_eigenvalues.argsort()\n",
    "normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "degrees = np.sum(adjacency_matrix, axis=0)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "\n",
    "indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "for i in range(K):\n",
    "    combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "        combined_indicator_vectors[:, i])\n",
    "    for j in range(i):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                     i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "rayleigh_quotients = compute_rayleigh_quotients(normalized_laplacian, combined_indicator_vectors, K)\n",
    "rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "rayleigh_quotients\n",
    "sns.scatterplot(normalized_L_eigenvalues[0:5])\n",
    "sns.scatterplot(rayleigh_quotients)\n",
    "B_1 = (rayleigh_quotients[0] + rayleigh_quotients[1]) / (normalized_L_eigenvalues[2])\n",
    "B_2 = (rayleigh_quotients[2] - normalized_L_eigenvalues[2] + normalized_L_eigenvalues[3] * B_1) / (\n",
    "            normalized_L_eigenvalues[3] - normalized_L_eigenvalues[2])\n",
    "B_1 + B_2\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "359ed66f693e77bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B_1 = rayleigh_quotients[0] / normalized_L_eigenvalues[1]\n",
    "B_2 = ((rayleigh_quotients[1] - normalized_L_eigenvalues[1]) + normalized_L_eigenvalues[2] * B_1) / (\n",
    "            normalized_L_eigenvalues[2] - normalized_L_eigenvalues[1])\n",
    "B_3 = ((rayleigh_quotients[2] - normalized_L_eigenvalues[2]) + normalized_L_eigenvalues[3] * (B_1 + B_2)) / (\n",
    "            normalized_L_eigenvalues[3] - normalized_L_eigenvalues[2])\n",
    "(B_1 + B_2 + B_3) / 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df88f25ad6c1ed5b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IRIS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87e4d2c9c111e5bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gaussian_kernel(X, sigma):\n",
    "    pairwise_sq_dists = np.sum(X**2, axis=1)[:, np.newaxis] + np.sum(X**2, axis=1) - 2 * np.dot(X, X.T)\n",
    "    K = np.exp(-pairwise_sq_dists / (2 * sigma**2))\n",
    "    return K"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdbe4ed62519f5b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def knn_adjacency_matrix(features, k, metric='euclidean'):\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be a positive integer.\")\n",
    "\n",
    "    # Compute the pairwise distances\n",
    "    distances = pairwise_distances(features, metric=metric)\n",
    "\n",
    "    # Get the indices of the k-nearest neighbors for each point\n",
    "    knn_indices = np.argsort(distances, axis=1)[:, 1:k+1]\n",
    "\n",
    "    # Initialize the adjacency matrix\n",
    "    n_points = features.shape[0]\n",
    "    adjacency_matrix = np.zeros((n_points, n_points), dtype=float)\n",
    "\n",
    "    # Fill the adjacency matrix\n",
    "    for i in range(n_points):\n",
    "        for j in knn_indices[i]:\n",
    "            adjacency_matrix[i, j] = 1\n",
    "            adjacency_matrix[j, i] = 1  # Ensure the graph is undirected\n",
    "\n",
    "    return adjacency_matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37914b5e5d0d846"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(knn_adjacency_matrix(x_data,6))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26771e1fc8edc269"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mnist = fetch_openml('iris', version=1, as_frame=False)\n",
    "x_data, y_data = mnist.data, mnist.target\n",
    "\n",
    "targets = np.unique(y_data).tolist()\n",
    "\n",
    "selected_samples = []\n",
    "selected_labels = []\n",
    "\n",
    "for target in targets:\n",
    "    indices = np.where(y_data == target)[0]\n",
    "    selected_samples.append(x_data[indices])\n",
    "    selected_labels.extend(y_data[indices])\n",
    "\n",
    "selected_samples = np.vstack(selected_samples)\n",
    "selected_labels = np.array(selected_labels)\n",
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix = knn_adjacency_matrix(x_data,6)\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "color_list = [\"red\",\"blue\",\"green\",\"yellow\"]\n",
    "\n",
    "colors = []\n",
    "for node in G.nodes():\n",
    "    target_ind = targets.index(G.nodes[node]['label'])\n",
    "    colors.append(color_list[target_ind])\n",
    "    \n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # Use a spring layout for visualization\n",
    "nx.draw(G, pos, node_color=colors, with_labels=False, node_size=50, edge_color=\"gray\", alpha=0.7)\n",
    "plt.title(\"Graph of MNIST Samples by Correlation (Largest Connected Component)\")\n",
    "plt.show()\n",
    "\n",
    "# Get the adjacency matrix of the graph\n",
    "adjacency_matrix = correlation_matrix.copy()\n",
    "\n",
    "# Plot the adjacency matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(np.array(adjacency_matrix), cmap='viridis', square=True, cbar=True, xticklabels=False, yticklabels=False)\n",
    "plt.title(\"Adjacency Matrix Heatmap\")\n",
    "plt.xlabel(\"Nodes\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.show()\n",
    "\n",
    "degrees = np.sum(adjacency_matrix, axis = 1)\n",
    "D_inv_sqrt = np.diag([1/np.sqrt(d) for d in degrees])\n",
    "normalized_laplacian = np.eye(len(degrees)) - D_inv_sqrt @ adjacency_matrix @ D_inv_sqrt\n",
    "# Compute the eigenvalues and eigenvectors of the normalized Laplacian\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "\n",
    "# Print the eigenvalues\n",
    "print(\"Eigenvalues of the Normalized Laplacian:\")\n",
    "print(eigenvalues)\n",
    "\n",
    "sns.scatterplot(x=np.arange(5), y=eigenvalues[:5])\n",
    "# compute first K eigenvectors of the normalized Laplacian\n",
    "K = 3\n",
    "normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "idx = normalized_L_eigenvalues.argsort()\n",
    "normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "degrees = np.sum(adjacency_matrix, axis=0)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "\n",
    "indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "for i in range(K):\n",
    "    combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "        combined_indicator_vectors[:, i])\n",
    "    for j in range(i):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                     i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "rayleigh_quotients = compute_rayleigh_quotients(normalized_laplacian, combined_indicator_vectors, K)\n",
    "rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "rayleigh_quotients\n",
    "sns.scatterplot(normalized_L_eigenvalues[0:5])\n",
    "sns.scatterplot(rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75b1e5c034aad9e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B_1 = (rayleigh_quotients[0] + rayleigh_quotients[1]) / (normalized_L_eigenvalues[2])\n",
    "B_2 = (rayleigh_quotients[2] - normalized_L_eigenvalues[2] + normalized_L_eigenvalues[3] * B_1) / (\n",
    "            normalized_L_eigenvalues[3] - normalized_L_eigenvalues[2])\n",
    "B_1 + B_2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "473dc4f48f49173"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(B_1 + B_2)/3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36ecd8157a57e73b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "95b6a8ee75c7a104"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HAR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f051cfb49d46e25a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mnist = fetch_openml('har', version=1, as_frame=False)\n",
    "x_data, y_data = mnist.data, mnist.target.astype(int)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7385a5975be5da41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.unique(y_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a8d0e74a10866f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "digits = [1, 2, 3]\n",
    "samples_per_digit = 200\n",
    "selected_samples = []\n",
    "selected_labels = []\n",
    "\n",
    "for digit in digits:\n",
    "    indices = np.where(y_data == digit)[0][:samples_per_digit]\n",
    "    selected_samples.append(x_data[indices])\n",
    "    selected_labels.extend(y_data[indices])\n",
    "\n",
    "# Combine the samples into a single dataset\n",
    "selected_samples = np.vstack(selected_samples)\n",
    "selected_labels = np.array(selected_labels)\n",
    "\n",
    "# Calculate the Pearson correlation matrix\n",
    "correlation_matrix = np.corrcoef(selected_samples)\n",
    "\n",
    "# Create a graph using the correlation matrix\n",
    "threshold = 0.9  # Define a threshold for edge creation\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with their corresponding digit label\n",
    "for i, label in enumerate(selected_labels):\n",
    "    G.add_node(i, label=int(label))\n",
    "\n",
    "# Add edges for high correlations\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i + 1, correlation_matrix.shape[0]):\n",
    "        if correlation_matrix[i, j] > threshold:\n",
    "            G.add_edge(i, j, weight=correlation_matrix[i, j])\n",
    "\n",
    "# Remove disconnected components (keep the largest connected component)\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Assign colors based on digit labels\n",
    "colors = [\"red\" if G.nodes[node]['label'] == 0 else\n",
    "          \"blue\" if G.nodes[node]['label'] == 1 else\n",
    "          \"green\" for node in G.nodes()]\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # Use a spring layout for visualization\n",
    "nx.draw(G, pos, node_color=colors, with_labels=False, node_size=50, edge_color=\"gray\", alpha=0.7)\n",
    "plt.title(\"Graph of MNIST Samples by Correlation (Largest Connected Component)\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d37874958f1d94d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the adjacency matrix of the graph\n",
    "adjacency_matrix = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "# Plot the adjacency matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(np.array(adjacency_matrix), cmap='viridis', square=True, cbar=True, xticklabels=False, yticklabels=False)\n",
    "plt.title(\"Adjacency Matrix Heatmap\")\n",
    "plt.xlabel(\"Nodes\")\n",
    "plt.ylabel(\"Nodes\")\n",
    "plt.show()\n",
    "\n",
    "normalized_laplacian = nx.normalized_laplacian_matrix(G).todense()\n",
    "# Compute the eigenvalues and eigenvectors of the normalized Laplacian\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "\n",
    "# Print the eigenvalues\n",
    "print(\"Eigenvalues of the Normalized Laplacian:\")\n",
    "print(eigenvalues)\n",
    "\n",
    "sns.scatterplot(x=np.arange(5), y=eigenvalues[:5])\n",
    "# compute first K eigenvectors of the normalized Laplacian\n",
    "K = 3\n",
    "normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "idx = normalized_L_eigenvalues.argsort()\n",
    "normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "degrees = np.sum(adjacency_matrix, axis=0)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "\n",
    "indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "for i in range(K):\n",
    "    combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "        combined_indicator_vectors[:, i])\n",
    "    for j in range(i):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                     i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "rayleigh_quotients = compute_rayleigh_quotients(normalized_laplacian, combined_indicator_vectors, K)\n",
    "rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "rayleigh_quotients\n",
    "sns.scatterplot(normalized_L_eigenvalues[0:5])\n",
    "sns.scatterplot(rayleigh_quotients)\n",
    "B_1 = (rayleigh_quotients[0] + rayleigh_quotients[1]) / (normalized_L_eigenvalues[2])\n",
    "B_2 = (rayleigh_quotients[2] - normalized_L_eigenvalues[2] + normalized_L_eigenvalues[3] * B_1) / (\n",
    "            normalized_L_eigenvalues[3] - normalized_L_eigenvalues[2])\n",
    "B_1 + B_2\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9150ac48a77b2b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fa3ef752475d63f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
