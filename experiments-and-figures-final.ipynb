{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports and Settings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67c950ce93f96d12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse.linalg\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_openml\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from graph import Graph\n",
    "from helpers import get_hermitian_adjacency_matrix, get_adjacency_matrix, get_degree_matrix, get_laplacian_matrix, compute_k_way_estimate, get_normalised_projected_indicator_vectors, degree_correction, k_means_indicator_vectors, compute_rayleigh_quotients, apply_recursive_st_brute_force, gram_schmidt, get_thresholded_correlation_matrix, largest_connected_component, get_normalised_laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make text larger\n",
    "plt.rc('font', size=20)\n",
    "# set random seed\n",
    "np.random.seed(9)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7dbe90e1495360e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1 style=\"font-size: 50px\">Undirected Graphs</h1>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4d4e61068a9d0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating Graphs From Gaussians\n",
    "We create a plot of 4 gaussian clusters with 100 nodes each."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6b9dc119608b300"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X1 = np.random.multivariate_normal(mean = [0,0], cov = [[1,0.5],[0.5,1]], size = 100)\n",
    "X2 = np.random.multivariate_normal(mean = [0,5], cov = [[1,-0.5],[-0.5,1]], size = 100)\n",
    "X3 = np.random.multivariate_normal(mean = [8,0], cov = [[1.5,0],[0,1]], size = 100)\n",
    "X4 = np.random.multivariate_normal(mean = [7,5], cov = [[1,0.2],[0.2,1]], size = 100)\n",
    "X = np.concatenate((X1, X2, X3, X4))\n",
    "# plot with each cluster in a different color\n",
    "plt.scatter(X1[:,0], X1[:,1], color = 'r')\n",
    "plt.scatter(X2[:,0], X2[:,1], color = 'b')\n",
    "plt.scatter(X3[:,0], X3[:,1], color = 'g')\n",
    "plt.scatter(X4[:,0], X4[:,1], color = 'y')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "#plt.title('4 Gaussian Clusters')\n",
    "plt.savefig('Figures/4GaussianClusters.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29eb7b4f4fac44f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# construct a graph from the data points using a threshold\n",
    "def construct_graph(X, threshold):\n",
    "    N = X.shape[0]\n",
    "    A = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i != j:\n",
    "                dist = np.linalg.norm(X[i] - X[j])\n",
    "                if dist < threshold:\n",
    "                    A[i, j] = 1\n",
    "    return A\n",
    "\n",
    "A = construct_graph(X, 4)\n",
    "plt.figure()\n",
    "G = nx.from_numpy_array(A)\n",
    "pos = X\n",
    "# make edges thin and transparent. \n",
    "colormap = ['r'] * 100 + ['b'] * 100 + ['g'] * 100 + ['y'] * 100\n",
    "nx.draw(G, pos = pos, node_size = 10, edge_color = 'grey', width = 0.2, node_color = colormap)\n",
    "#plt.title('Graph Constructed from 4 Gaussian Clusters \\n Threshold = 4')\n",
    "plt.savefig('Figures/4GaussianClustersGraphThreshold4.png', bbox_inches = 'tight')\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b10431b7d978d08c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the normalized laplacian\n",
    "\n",
    "degrees = np.sum(A, axis = 1)\n",
    "D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "norm_L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "idx = np.argsort(eigvals)\n",
    "eigvals = eigvals[idx]\n",
    "eigvecs = eigvecs[:, idx]\n",
    "\n",
    "# plot first 10 eigenvalues\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "plt.grid(True)\n",
    "plt.scatter(range(1,9), eigvals[:8])\n",
    "plt.xlabel('Index')\n",
    "plt.xlim(1, 8.6)\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "plt.yticks(np.arange(0, 1.1, 0.1), minor=True)\n",
    "#plt.title(r\"First 8 Eigenvalues of $\\mathcal{L}$\")\n",
    "for i in range(8):\n",
    "    plt.text(i + 1, eigvals[i], f'{eigvals[i]:.2f}', fontsize=10)\n",
    "plt.savefig('Figures/4GaussianClusters8Eigenvalues.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c72c0874363de8a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bounds for Geometric Graphs\n",
    "We take 4 multivariate normally distributed random variables $X_1, X_2,X_3$ and $X_4$ with following distributions:\n",
    "$X_1 \\sim \\mathcal{N}((0,0), I)$,\n",
    "$X_2 \\sim \\mathcal{N}((0,5), I)$,\n",
    "$X_3 \\sim \\mathcal{N}((8,0), I)$,\n",
    "$X_4 \\sim \\mathcal{N}((8,5), I)$\n",
    "We study our bounds for varied number of samples of each distribution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a32e8bbcac3ac2a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rst_bounds = {}\n",
    "st_bounds = {}\n",
    "k_way_bounds = {}\n",
    "true_values = {}\n",
    "n_clusters = 4\n",
    "sample_size = 10\n",
    "for n in np.arange(200,1000,100):\n",
    "    rst_bounds_temp = []\n",
    "    st_bounds_temp = []\n",
    "    k_way_bounds_temp = []\n",
    "    true_values_temp = []\n",
    "    print(f\"commencing computation for n = {n}\")\n",
    "    for p in range(sample_size):\n",
    "    \n",
    "        X1 = np.random.multivariate_normal(mean = [0,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X2 = np.random.multivariate_normal(mean = [0,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X3 = np.random.multivariate_normal(mean = [8,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X4 = np.random.multivariate_normal(mean = [8,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X = np.concatenate((X1, X2, X3, X4))\n",
    "        \n",
    "        A = construct_graph(X, 4)\n",
    "        degrees = np.sum(A, axis = 1)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "        norm_L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        labels = kmeans.fit_predict(eigvecs[:, 0:n_clusters])\n",
    "        \n",
    "        #compute exact indicator vectors (from clusters from distributions not k means)\n",
    "        exact_indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[i*n:(i+1)*n,i] = 1\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[:, i] = D_sqrt @ exact_indicator_vectors[:, i]\n",
    "            exact_indicator_vectors[:, i] = exact_indicator_vectors[:, i] / np.linalg.norm(exact_indicator_vectors[:, i])\n",
    "        \n",
    "        true_val = np.linalg.norm(eigvecs[:,:n_clusters] - exact_indicator_vectors @ (exact_indicator_vectors.T @ eigvecs[:,:n_clusters]), ord = 'fro')**2\n",
    "        \n",
    "        # indicator vectors from k means\n",
    "        indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = (labels == i).astype(int)\n",
    "        \n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = D_sqrt @ indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i] / np.linalg.norm(indicator_vectors[:, i])\n",
    "        \n",
    "        # project indicator vectors onto the eigenvectors\n",
    "        beta_K_by_K = (indicator_vectors.T @ eigvecs[:,:n_clusters])\n",
    "        \n",
    "        combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "                combined_indicator_vectors[:, i])\n",
    "            for j in range(i):\n",
    "                combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                            combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                                 i]) * combined_indicator_vectors[:, j]\n",
    "            \n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(combined_indicator_vectors[:, i])\n",
    "            \n",
    "            \n",
    "        # compute the rayleigh quotients\n",
    "        rayleigh_quotients = []\n",
    "        for i in range(n_clusters):\n",
    "            indicator = combined_indicator_vectors[:, i]\n",
    "            val = (indicator.T @ norm_L @ indicator) / (indicator.T @ indicator)\n",
    "            rayleigh_quotients.append(val)\n",
    "        \n",
    "        # sort the rayleigh quotients\n",
    "        sorted_rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        B_1 = (sorted_rayleigh_quotients[0] + sorted_rayleigh_quotients[1])/eigvals[2]\n",
    "        B_2 = ((sorted_rayleigh_quotients[2] + sorted_rayleigh_quotients[3]) - 2*eigvals[2] + eigvals[4]*B_1)/(eigvals[4] - eigvals[2])\n",
    "        \n",
    "        rst_bound = B_1 + B_2\n",
    "        st_bound = (np.sum(sorted_rayleigh_quotients))/(eigvals[4])\n",
    "        rho = compute_k_way_estimate(norm_L, indicator_vectors, 4)\n",
    "        k_way_bound = 4*rho/eigvals[4]\n",
    "        \n",
    "        rst_bounds_temp.append(rst_bound)\n",
    "        st_bounds_temp.append(st_bound)\n",
    "        k_way_bounds_temp.append(k_way_bound)\n",
    "        true_values_temp.append(true_val)\n",
    "    \n",
    "    rst_bounds[n] = np.mean(rst_bounds_temp)\n",
    "    st_bounds[n] = np.mean(st_bounds_temp)\n",
    "    k_way_bounds[n] = np.mean(k_way_bounds_temp)\n",
    "    true_values[n] = np.mean(true_values_temp)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc6914d0fcd75086"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds_df = pd.DataFrame([rst_bounds, st_bounds, k_way_bounds, true_values]).T\n",
    "bounds_df.columns = [r'Theorem 4',r'Theorem 1', r'MacGregor & Sun', r'True Value']\n",
    "bounds_df.to_csv(\"Data/4GeometricClusters2Pairs.csv\")\n",
    "\n",
    "df_copy = pd.read_csv(\"Data/4GeometricClusters2Pairs.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "(df_copy / 4).plot(marker = 'o', xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10))\n",
    "plt.xlabel(r'Cluster size $n$', fontsize = 20)\n",
    "plt.ylabel(r'Error', fontsize = 20)\n",
    "plt.grid(True)\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Data/4GeometricClusters2Pairs.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd620164e342e3b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "No we try a similar experiment but rather than driving the means apart we drive the means of the distributions apart. So,\n",
    "$X_1 \\sim \\mathcal{N}((0,0), I)$,\n",
    "$X_2 \\sim \\mathcal{N}((0,5), I)$,\n",
    "$X_3 \\sim \\mathcal{N}((d,0), I)$,\n",
    "$X_4 \\sim \\mathcal{N}((d,5), I)$\n",
    "d is a parameter that is varied and cluster size n is fixed at 100.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c394c5e82cec1304"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9cff1278d6e281aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rst_bounds = {}\n",
    "st_bounds = {}\n",
    "k_way_bounds = {}\n",
    "true_values = {}\n",
    "n_clusters = 4\n",
    "n = 100\n",
    "sample_size = 1\n",
    "for d in np.arange(5, 12, 0.5):\n",
    "    rst_bounds_temp = []\n",
    "    st_bounds_temp = []\n",
    "    k_way_bounds_temp = []\n",
    "    true_values_temp = []\n",
    "    print(f\"commencing computation for d = {d}\")\n",
    "    for p in range(sample_size):\n",
    "    \n",
    "        X1 = np.random.multivariate_normal(mean = [0,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X2 = np.random.multivariate_normal(mean = [0,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X3 = np.random.multivariate_normal(mean = [d,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X4 = np.random.multivariate_normal(mean = [d,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X = np.concatenate((X1, X2, X3, X4))\n",
    "        \n",
    "        A = construct_graph(X, 4)\n",
    "        degrees = np.sum(A, axis = 1)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "        norm_L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        labels = kmeans.fit_predict(eigvecs[:, 0:n_clusters])\n",
    "        \n",
    "        #compute exact indicator vectors (from clusters from distributions not k means)\n",
    "        exact_indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[i*n:(i+1)*n,i] = 1\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[:, i] = D_sqrt @ exact_indicator_vectors[:, i]\n",
    "            exact_indicator_vectors[:, i] = exact_indicator_vectors[:, i] / np.linalg.norm(exact_indicator_vectors[:, i])\n",
    "        \n",
    "        true_val = np.linalg.norm(eigvecs[:,:n_clusters] - exact_indicator_vectors @ (exact_indicator_vectors.T @ eigvecs[:,:n_clusters]), ord = 'fro')**2\n",
    "        \n",
    "        # indicator vectors from k means\n",
    "        indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = (labels == i).astype(int)\n",
    "        \n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = D_sqrt @ indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i] / np.linalg.norm(indicator_vectors[:, i])\n",
    "        \n",
    "        # project indicator vectors onto the eigenvectors\n",
    "        beta_K_by_K = (indicator_vectors.T @ eigvecs[:,:n_clusters])\n",
    "        \n",
    "        combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "                combined_indicator_vectors[:, i])\n",
    "            for j in range(i):\n",
    "                combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                            combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                                 i]) * combined_indicator_vectors[:, j]\n",
    "            \n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(combined_indicator_vectors[:, i])\n",
    "            \n",
    "            \n",
    "        # compute the rayleigh quotients\n",
    "        rayleigh_quotients = []\n",
    "        for i in range(n_clusters):\n",
    "            indicator = combined_indicator_vectors[:, i]\n",
    "            val = (indicator.T @ norm_L @ indicator) / (indicator.T @ indicator)\n",
    "            rayleigh_quotients.append(val)\n",
    "        \n",
    "        # sort the rayleigh quotients\n",
    "        sorted_rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        B_1 = (sorted_rayleigh_quotients[0] + sorted_rayleigh_quotients[1])/eigvals[2]\n",
    "        B_2 = ((sorted_rayleigh_quotients[2] + sorted_rayleigh_quotients[3]) - 2*eigvals[2] + eigvals[4]*B_1)/(eigvals[4] - eigvals[2])\n",
    "        \n",
    "        rst_bound = B_1 + B_2\n",
    "        st_bound = (np.sum(sorted_rayleigh_quotients))/(eigvals[4])\n",
    "        rho = compute_k_way_estimate(norm_L, indicator_vectors, 4)\n",
    "        k_way_bound = 4*rho/eigvals[4]\n",
    "        \n",
    "        rst_bounds_temp.append(rst_bound)\n",
    "        st_bounds_temp.append(st_bound)\n",
    "        k_way_bounds_temp.append(k_way_bound)\n",
    "        true_values_temp.append(true_val)\n",
    "    \n",
    "    rst_bounds[d] = np.mean(rst_bounds_temp)\n",
    "    st_bounds[d] = np.mean(st_bounds_temp)\n",
    "    k_way_bounds[d] = np.mean(k_way_bounds_temp)\n",
    "    true_values[d] = np.mean(true_values_temp)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8514e208bb18d310"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds_df = pd.DataFrame([rst_bounds, st_bounds, k_way_bounds, true_values]).T\n",
    "bounds_df.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "bounds_df.to_csv(\"Data/4GeometricClusters2PairsDriftingApart.csv\")\n",
    "\n",
    "bounds_df = pd.read_csv(\"Data/4GeometricClusters2PairsDriftingApart.csv\", usecols=lambda column: column != \"Unnamed: 0\")\n",
    "\n",
    "fig = plt.figure()\n",
    "(bounds_df / 4).plot(marker='x', markersize=10, figsize=(12, 10))\n",
    "\n",
    "#plt.legend(bbox_to_anchor=(1,1.05))\n",
    "plt.xlabel(\"Distance, d\", fontsize=30)\n",
    "plt.ylabel(\"Error\", fontsize=30)\n",
    "plt.legend(fontsize=25)\n",
    "plt.grid(visible=True, which='both', linewidth=1.5)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1), fontsize=25)\n",
    "# plt.title(\n",
    "#     r\"Bounds for $\\frac{1}{4}\\sum_{i=1}^4\\|f_i - \\hat{g}_i\\|^2$ generated from Gaussian mixture model (4 clusters, 2 pairs)\" + \"\\n Distance between pairs of clusters increased\",\n",
    "#     y=1.03)\n",
    "plt.savefig(\"Data/BoundsGaussianMixtureModel2PairsDriftingApart.png\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d0566a4d47b9fcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bounds for Stochastic Block Models\n",
    "Our initial choice is an SBM with 4 clusters and where two pairs have a high affinity for each other. We use \n",
    "$$\n",
    "P = \\begin{pmatrix}\n",
    "    0.5 & 0.4 & 0.1 & 0.1 \\\\\n",
    "    0.4 & 0.5 & 0.1 & 0.1 \\\\\n",
    "    0.1 & 0.1 & 0.5 & 0.4 \\\\\n",
    "    0.1 & 0.1 & 0.4 & 0.5 \\\\\n",
    "    \\end{pmatrix}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d61fde072491fb1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "P = np.array([[0.5, 0.4, 0.1, 0.1],\n",
    "              [0.4, 0.5, 0.1, 0.1],\n",
    "              [0.1, 0.1, 0.5, 0.4],\n",
    "              [0.1, 0.1, 0.4, 0.5]])\n",
    "K = 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9065f801e076b6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds = {}\n",
    "sample_size = 10\n",
    "for n in [200,300,400,500,600,700,800,900]:\n",
    "    bounds[n] = 0\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i,K):\n",
    "                prob_existing_edge = P[i,j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u+1,n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "                                \n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "        \n",
    "        true_clusters = [list(range(i*n, (i+1)*n)) for i in range(K)]                        \n",
    "        G = Graph(vertices = list(range(n * K)), edges = edges)\n",
    "        norm_L = get_laplacian_matrix(G, normalized=True)\n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        eigvecs = eigvecs[:, 0:K]\n",
    "        \n",
    "        D = get_degree_matrix(G)\n",
    "        degrees = np.diag(D)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        indicator_vectors = k_means_indicator_vectors(eigvecs, K)\n",
    "        degree_corrected_indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "        combined_indicator_vectors = get_normalised_projected_indicator_vectors(eigvecs,degree_corrected_indicator_vectors, K)\n",
    "        rayleigh_quotients = compute_rayleigh_quotients(norm_L, combined_indicator_vectors, K)\n",
    "        rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        \n",
    "        true_value = (1/K) * np.linalg.norm(eigvecs - degree_corrected_indicator_vectors @ (degree_corrected_indicator_vectors.T @ eigvecs), ord = 'fro')**2\n",
    "        general_ST = (1/K) * (np.sum(rayleigh_quotients))/eigvals[K]\n",
    "        ST_standard = compute_k_way_estimate(norm_L, degree_corrected_indicator_vectors, K)/ eigvals[K]\n",
    "        min_split_indices, min_split = apply_recursive_st_brute_force(rayleigh_quotients,eigvals, K)\n",
    "        recursive_ST = (1/K) * min_split\n",
    "        \n",
    "        \n",
    "        \n",
    "        bounds[n] = bounds[n] + pd.Series({'Recursive ST': recursive_ST,\n",
    "            'General ST': general_ST,\n",
    "            'ST Standard': ST_standard,\n",
    "            'True Value': true_value} )\n",
    "        \n",
    "    bounds[n] = bounds[n] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dd66be83da6818e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "columns = ['Recursive ST', 'General ST', r'$\\frac{\\rho(4)}{\\lambda_5}$', 'True Value']\n",
    "df.columns = columns\n",
    "#df = df.drop(columns = ['General ST'])\n",
    "df.to_csv('Data/Bounds4Clusters2PairsRST.csv')\n",
    "\n",
    "df_copy = pd.read_csv(\"Data/Bounds4Clusters2PairsRST.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "df_copy.plot(marker='x', markersize=10, xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10))\n",
    "\n",
    "plt.xlabel(r'Cluster size $n$', fontsize=30)\n",
    "plt.ylabel(r'Error',fontsize=30)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,1.2,0.1))\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0,1.1,0.1),fontsize=25)\n",
    "plt.grid(visible=True, which='both', linewidth=1.5)\n",
    "# make legend larger\n",
    "plt.legend(fontsize=25, bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Data/Bounds4Clusters2PairsRST.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abab89c8120c9197"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Repeating the experiment for an SBM with 8 clusters with a single pair that have an affinity for each other."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb9c802d40a06121"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 8\n",
    "p1 = 0.5\n",
    "p2 = 0.3\n",
    "q = 0.05\n",
    "P = np.ones((8,8))\n",
    "P = q*P\n",
    "np.fill_diagonal(P,p1)\n",
    "P[0,1] = p2\n",
    "P[1,0] = p2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e8cd03ea5ae6abe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds = {}\n",
    "sample_size = 1\n",
    "for n in [200,300,400,500,600,700,800,900]:\n",
    "    bounds[n] = 0\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i,K):\n",
    "                prob_existing_edge = P[i,j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u+1,n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "                                \n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "        \n",
    "        true_clusters = [list(range(i*n, (i+1)*n)) for i in range(K)]                        \n",
    "        G = Graph(vertices = list(range(n * K)), edges = edges)\n",
    "        norm_L = get_laplacian_matrix(G, normalized=True)\n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        eigvecs = eigvecs[:, 0:K]\n",
    "        \n",
    "        D = get_degree_matrix(G)\n",
    "        degrees = np.diag(D)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        indicator_vectors = k_means_indicator_vectors(eigvecs, K)\n",
    "        degree_corrected_indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "        combined_indicator_vectors = get_normalised_projected_indicator_vectors(eigvecs,degree_corrected_indicator_vectors, K)\n",
    "        rayleigh_quotients = compute_rayleigh_quotients(norm_L, combined_indicator_vectors, K)\n",
    "        rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        \n",
    "        true_value = (1/K) * np.linalg.norm(eigvecs - degree_corrected_indicator_vectors @ (degree_corrected_indicator_vectors.T @ eigvecs), ord = 'fro')**2\n",
    "        general_ST = (1/K) * (np.sum(rayleigh_quotients))/eigvals[K]\n",
    "        ST_standard = compute_k_way_estimate(norm_L, degree_corrected_indicator_vectors, K)/ eigvals[K]\n",
    "        min_split_indices, min_split = apply_recursive_st_brute_force(rayleigh_quotients,eigvals, K)\n",
    "        recursive_ST = (1/K) * min_split\n",
    "        \n",
    "        \n",
    "        \n",
    "        bounds[n] = bounds[n] + pd.Series({'Recursive ST': recursive_ST,\n",
    "            'General ST': general_ST,\n",
    "            'ST Standard': ST_standard,\n",
    "            'True Value': true_value} )\n",
    "        \n",
    "    bounds[n] = bounds[n] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bacbf4567eac890"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "\n",
    "columns = ['Recursive ST', 'General ST', r'$\\frac{\\rho(8)}{\\lambda_9}$', 'True Value']\n",
    "df.columns = columns\n",
    "#df = df.drop(columns = ['General ST'])\n",
    "df.to_csv(\"Data/Bounds8Clusters1PairRST.csv\")\n",
    "df_copy = pd.read_csv(\"Data/Bounds8Clusters1PairRST.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "df_copy.plot(marker = 'x', markersize=10, xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$', fontsize = 30)\n",
    "plt.ylabel(r'Error', fontsize = 30)\n",
    "plt.grid(True, which='both', linewidth=1.5)\n",
    "# add ticks\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0,1.1,0.1), fontsize=25)\n",
    "# make legend larger\n",
    "plt.legend(fontsize=25, bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Data/Bounds8Clusters1PairRST.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0916cd549fb2d30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Repeating the experiment for an  SBM with 12 clusters and the following probability matrix:\n",
    "$$P = \\begin{pmatrix}\n",
    "0.9 & 0.7 & 0.2 & 0.2 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.7 & 0.9 & 0.2 & 0.2 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.2 & 0.2 & 0.9 & 0.7 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.2 & 0.2 & 0.7 & 0.9 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.9 & 0.7 & 0.2 & 0.2 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.7 & 0.9 & 0.2 & 0.2 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.2 & 0.2 & 0.9 & 0.7 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.2 & 0.2 & 0.7 & 0.9 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.9 & 0.7 & 0.2 & 0.2 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.7 & 0.9 & 0.2 & 0.2 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.2 & 0.2 & 0.9 & 0.7 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.2 & 0.2 & 0.7 & 0.9 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbaecf77287be34e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p1, p2, p3, p4 = 0.9, 0.7, 0.2, 0.05\n",
    "K = 12\n",
    "P = np.array([\n",
    "    [p1, p2, p3, p3, p4, p4, p4, p4, p4, p4, p4, p4],\n",
    "    [p2, p1, p3, p3, p4, p4, p4, p4, p4, p4, p4, p4],\n",
    "    [p3, p3, p1, p2, p4, p4, p4, p4, p4, p4, p4, p4],\n",
    "    [p3, p3, p2, p1, p4, p4, p4, p4, p4, p4, p4, p4],\n",
    "    [p4, p4, p4, p4, p1, p2, p3, p3, p4, p4, p4, p4],\n",
    "    [p4, p4, p4, p4, p2, p1, p3, p3, p4, p4, p4, p4],\n",
    "    [p4, p4, p4, p4, p3, p3, p1, p2, p4, p4, p4, p4],\n",
    "    [p4, p4, p4, p4, p3, p3, p2, p1, p4, p4, p4, p4],\n",
    "    [p4, p4, p4, p4, p4, p4, p4, p4, p1, p2, p3, p3],\n",
    "    [p4, p4, p4, p4, p4, p4, p4, p4, p2, p1, p3, p3],\n",
    "    [p4, p4, p4, p4, p4, p4, p4, p4, p3, p3, p1, p2],\n",
    "    [p4, p4, p4, p4, p4, p4, p4, p4, p3, p3, p2, p1]\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "939a7e62ac571092"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy\n",
    "bounds = {}\n",
    "sample_size = 5\n",
    "for n in [200,300,400,500,600,700]:\n",
    "    bounds[n] = 0\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i,K):\n",
    "                prob_existing_edge = P[i,j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u+1,n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "                                \n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "        \n",
    "        true_clusters = [list(range(i*n, (i+1)*n)) for i in range(K)]                        \n",
    "        G = Graph(vertices = list(range(n * K)), edges = edges)\n",
    "        norm_L = get_laplacian_matrix(G, normalized=True)\n",
    "        eigvals, eigvecs = scipy.sparse.linalg.eigsh(norm_L,15, which='SM')\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        eigvecs = eigvecs[:, 0:K]\n",
    "        \n",
    "        D = get_degree_matrix(G)\n",
    "        degrees = np.diag(D)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        indicator_vectors = k_means_indicator_vectors(eigvecs, K)\n",
    "        degree_corrected_indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "        combined_indicator_vectors = get_normalised_projected_indicator_vectors(eigvecs,degree_corrected_indicator_vectors, K)\n",
    "        rayleigh_quotients = compute_rayleigh_quotients(norm_L, combined_indicator_vectors, K)\n",
    "        rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        \n",
    "        true_value = (1/K) * np.linalg.norm(eigvecs - degree_corrected_indicator_vectors @ (degree_corrected_indicator_vectors.T @ eigvecs), ord = 'fro')**2\n",
    "        general_ST = (1/K) * (np.sum(rayleigh_quotients))/eigvals[K]\n",
    "        ST_standard = compute_k_way_estimate(norm_L, degree_corrected_indicator_vectors, K)/ eigvals[K]\n",
    "        min_split_indices, min_split = apply_recursive_st_brute_force(rayleigh_quotients,eigvals, K)\n",
    "        recursive_ST = (1/K) * min_split\n",
    "        \n",
    "        \n",
    "        \n",
    "        bounds[n] = bounds[n] + pd.Series({'Recursive ST': recursive_ST,\n",
    "            'General ST': general_ST,\n",
    "            'ST Standard': ST_standard,\n",
    "            'True Value': true_value} )\n",
    "        \n",
    "    bounds[n] = bounds[n] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7737feb8ade2b9b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "\n",
    "columns = ['Recursive ST', 'General ST', r'$\\frac{\\rho(8)}{\\lambda_9}$', 'True Value']\n",
    "df.columns = columns\n",
    "#df = df.drop(columns = ['General ST'])\n",
    "df.to_csv(\"Data/Bounds12Clusters3QuartetsRST.csv\")\n",
    "df_copy = pd.read_csv(\"Data/Bounds12Clusters3QuartetsRST.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "df_copy.plot(marker='x', markersize=10, xlabel='n', ylabel='Bound Value', figsize=(12, 10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$', fontsize=30)\n",
    "plt.ylabel(r'Error', fontsize=30)\n",
    "plt.grid(True, which='both', linewidth=1.5)\n",
    "# add ticks\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1), fontsize=25)\n",
    "# make legend larger\n",
    "plt.legend(fontsize=25, bbox_to_anchor=(1.0, 1.05))\n",
    "plt.savefig('Data/Bounds12Clusters3QuartetsRST.png', bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e033821854503237"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Real-World Networks\n",
    "In this section we produce our bounds for a collection of real-world networks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dc057242c1fca91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MNIST Dataset\n",
    "Sourced from openml. We sample 500 images each of digits 0,1,2,3 and 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3af90e5730fe4c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_results = {}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a9889693fec249a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the MNIST dataset using OpenML\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "x_data, y_data = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Select 3 digits (e.g., 0, 1, and 2) and restrict to 200 samples each\n",
    "digits = [0, 1, 2, 3, 4]\n",
    "samples_per_digit = 500\n",
    "selected_samples = []\n",
    "selected_labels = []\n",
    "\n",
    "# taking a sample of each digit\n",
    "for digit in digits:\n",
    "    indices = np.where(y_data == digit)[0][:samples_per_digit]\n",
    "    selected_samples.append(x_data[indices])\n",
    "    selected_labels.extend(y_data[indices])\n",
    "\n",
    "# Combine the samples into a single dataset\n",
    "selected_samples = np.vstack(selected_samples) / 255.0  # Normalize pixel values\n",
    "selected_labels = np.array(selected_labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74fe69ebcb3e43c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a graph using the correlation matrix\n",
    "threshold = 0.7  # Define a threshold for edge creation\n",
    "adjacency_matrix = get_thresholded_correlation_matrix(selected_samples, threshold)\n",
    "adjacency_matrix_largest_cc, largest_cc = largest_connected_component(adjacency_matrix) \n",
    "normalized_laplacian = get_normalised_laplacian(adjacency_matrix_largest_cc)\n",
    "\n",
    "# compute first K eigenvectors of the normalized Laplacian\n",
    "K=5\n",
    "normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "idx = normalized_L_eigenvalues.argsort()\n",
    "normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "degrees = np.sum(adjacency_matrix_largest_cc, axis = 0)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "\n",
    "indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "for i in range(K):\n",
    "    combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "        combined_indicator_vectors[:, i])\n",
    "    for j in range(i):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                    combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                         i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "rayleigh_quotients = compute_rayleigh_quotients(normalized_laplacian, combined_indicator_vectors, K)\n",
    "rayleigh_quotients = np.sort(rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5b5a655f9aee4fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_val_matrix_combined = normalized_L_eigenvectors[:, :K] - indicator_vectors @ (\n",
    "            indicator_vectors.T @ normalized_L_eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, normalized_L_eigenvalues[0:K+1], K)\n",
    "\n",
    "mnist_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "mnist_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(adjacency_matrix_largest_cc))\n",
    "mnist_dataset_results[\"N\"] = len(adjacency_matrix_largest_cc)\n",
    "print(\"Number of edges M: \", np.sum(np.sum(adjacency_matrix_largest_cc, axis=1), axis=0))\n",
    "mnist_dataset_results[\"M\"] = np.sum(np.sum(adjacency_matrix_largest_cc, axis=1), axis=0)\n",
    "print(\"True value: \", true_val_combined)\n",
    "mnist_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \",\n",
    "      max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors)) / normalized_L_eigenvalues[K])\n",
    "mnist_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors)) / \\\n",
    "                                  normalized_L_eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients) / (K * normalized_L_eigenvalues[K]))\n",
    "mnist_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients) / (K * normalized_L_eigenvalues[K])\n",
    "print(\"Recursive ST: \", v / K)\n",
    "mnist_dataset_results[\"Recursive ST\"] = v / K\n",
    "\n",
    "dataset_results[\"MNIST\"] = mnist_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec3856f5b367edc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fashion MNIST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322bc8fcc4e8ff0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the MNIST dataset using OpenML\n",
    "mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "x_data, y_data = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Select 3 digits (e.g., 0, 1, and 2) and restrict to 200 samples each\n",
    "digits = [0, 1, 2]\n",
    "samples_per_digit = 1000\n",
    "selected_samples = []\n",
    "selected_labels = []\n",
    "\n",
    "# taking a sample of each digit\n",
    "for digit in digits:\n",
    "    indices = np.where(y_data == digit)[0][:samples_per_digit]\n",
    "    selected_samples.append(x_data[indices])\n",
    "    selected_labels.extend(y_data[indices])\n",
    "\n",
    "# Combine the samples into a single dataset\n",
    "selected_samples = np.vstack(selected_samples) / 255.0  # Normalize pixel values\n",
    "selected_labels = np.array(selected_labels)\n",
    "\n",
    "# Create a graph using the correlation matrix\n",
    "threshold = 0.7  # Define a threshold for edge creation\n",
    "adjacency_matrix = get_thresholded_correlation_matrix(selected_samples, threshold)\n",
    "adjacency_matrix_largest_cc, largest_cc = largest_connected_component(adjacency_matrix) \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49e450ad8a9f342"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This dataset had some vertices that need to be removed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15331d734ac5adfa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vertices_to_remove = [318,319,320]\n",
    "adjacency_matrix_largest_cc = np.delete(adjacency_matrix_largest_cc, vertices_to_remove, axis=0)\n",
    "adjacency_matrix_largest_cc = np.delete(adjacency_matrix_largest_cc, vertices_to_remove, axis=1)\n",
    "normalized_laplacian = get_normalised_laplacian(adjacency_matrix_largest_cc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fde46fb6ce701081"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute first K eigenvectors of the normalized Laplacian\n",
    "K=6\n",
    "normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "idx = normalized_L_eigenvalues.argsort()\n",
    "normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "degrees = np.sum(adjacency_matrix_largest_cc, axis = 0)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "\n",
    "indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "for i in range(K):\n",
    "    combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "        combined_indicator_vectors[:, i])\n",
    "    for j in range(i):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                    combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                         i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "rayleigh_quotients = compute_rayleigh_quotients(normalized_laplacian, combined_indicator_vectors, K)\n",
    "rayleigh_quotients = np.sort(rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51cb0fb9f2d86e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, normalized_L_eigenvalues[0:7], K)\n",
    "true_val_matrix_combined = normalized_L_eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ normalized_L_eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "fashion_mnist_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "fashion_mnist_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(adjacency_matrix_largest_cc))\n",
    "fashion_mnist_dataset_results[\"N\"] = len(adjacency_matrix_largest_cc)\n",
    "print(\"Number of edges M: \", np.sum(np.sum(adjacency_matrix_largest_cc, axis=1), axis=0))\n",
    "fashion_mnist_dataset_results[\"M\"] = np.sum(np.sum(adjacency_matrix_largest_cc, axis=1), axis=0)\n",
    "print(\"True value: \", true_val_combined)\n",
    "fashion_mnist_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \",\n",
    "      max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors)) / normalized_L_eigenvalues[K])\n",
    "fashion_mnist_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors)) / \\\n",
    "                                          normalized_L_eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients) / (K * normalized_L_eigenvalues[K]))\n",
    "fashion_mnist_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients) / (K * normalized_L_eigenvalues[K])\n",
    "print(\"Recursive ST: \", np.sum(v) / K)\n",
    "fashion_mnist_dataset_results[\"Recursive ST\"] = np.sum(v) / K\n",
    "\n",
    "dataset_results[\"Fashion MNIST\"] = fashion_mnist_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d23f9a217075294"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Air Quality Dataset\n",
    "Pulled from: https://www.kaggle.com/datasets/mujtabamatin/air-quality-and-pollution-assessment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6ddfecac45aabd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pollution_df = pd.read_csv(\"Data/updated_pollution_dataset.csv\")\n",
    "pollution_df = pollution_df.sort_values(by='Air Quality').reset_index(drop=True)\n",
    "\n",
    "# Separate targets and features\n",
    "pollution_df_targets = pollution_df['Air Quality']\n",
    "pollution_df_features = pollution_df.drop(columns=['Air Quality'])\n",
    "\n",
    "# Normalize features\n",
    "pollution_df_features_normalized = (pollution_df_features - pollution_df_features.mean()) / pollution_df_features.std()\n",
    "\n",
    "# Calculate covariance matrix\n",
    "pollution_df_features_cov = np.cov(pollution_df_features_normalized)\n",
    "\n",
    "# Threshold covariance matrix\n",
    "pollution_df_features_cov_thresholded = pollution_df_features_cov.copy()\n",
    "pollution_df_features_cov_thresholded[pollution_df_features_cov_thresholded < 0.4] = 0\n",
    "pollution_df_features_cov_thresholded[pollution_df_features_cov_thresholded >= 0.4] = 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cb337c3694fd597"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing disconnected components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "696929baf359520e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove zero rows and columns\n",
    "non_zero_indices = np.where(~(pollution_df_features_cov_thresholded == 0).all(axis=1))[0]\n",
    "pollution_df_features_cov_thresholded = pollution_df_features_cov_thresholded[non_zero_indices][:, non_zero_indices]\n",
    "\n",
    "# Adjacency matrix\n",
    "adjacency_matrix = pollution_df_features_cov_thresholded.copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f510a59f6ba07ebd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalized_laplacian = get_normalised_laplacian(adjacency_matrix)\n",
    "\n",
    "# compute first K eigenvectors of the normalized Laplacian\n",
    "K = 3\n",
    "normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "idx = normalized_L_eigenvalues.argsort()\n",
    "normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "degrees = np.sum(adjacency_matrix, axis=0)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "\n",
    "indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "for i in range(K):\n",
    "    combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "        combined_indicator_vectors[:, i])\n",
    "    for j in range(i):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                     i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "rayleigh_quotients = compute_rayleigh_quotients(normalized_laplacian, combined_indicator_vectors, K)\n",
    "rayleigh_quotients = np.sort(rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d04caba4a591815b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_val_matrix_combined = normalized_L_eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ normalized_L_eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "rayleigh_quotients = np.round(rayleigh_quotients, 6)\n",
    "normalized_L_eigenvalues = np.round(normalized_L_eigenvalues, 6)\n",
    "\n",
    "\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, normalized_L_eigenvalues[0:K+1], K)\n",
    "\n",
    "\n",
    "air_quality_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "air_quality_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(adjacency_matrix))\n",
    "air_quality_dataset_results[\"N\"] = len(adjacency_matrix)\n",
    "print(\"Number of edges M: \", np.sum(np.sum(adjacency_matrix, axis=1), axis=0))\n",
    "air_quality_dataset_results[\"M\"] = np.sum(np.sum(adjacency_matrix, axis=1), axis=0)\n",
    "print(\"True value: \", true_val_combined)\n",
    "air_quality_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \", max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors))/normalized_L_eigenvalues[K])\n",
    "air_quality_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors))/normalized_L_eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients)/(K * normalized_L_eigenvalues[K]))\n",
    "air_quality_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients)/(K * normalized_L_eigenvalues[K])\n",
    "print(\"Recursive ST: \", v/K)\n",
    "air_quality_dataset_results[\"Recursive ST\"] = v/K\n",
    "\n",
    "dataset_results[\"Air Quality\"] = air_quality_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7941dbdbecdf52b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Twitch\n",
    "From SNAP."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e28ef3b299e4e25a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 1: Define the file path\n",
    "edge_list_path = \"Data/twitch_gamers/large_twitch_edges.csv\"\n",
    "df = pd.read_csv(edge_list_path)\n",
    "df.to_csv(\"Data/twitch_gamers/twitch_cleaned_edges.csv\", index=False, header=False)\n",
    "# Step 2: Load the Deezer graph\n",
    "# Assuming the edge list is in CSV format with two columns: source and target\n",
    "G = nx.read_edgelist(\"Data/twitch_gamers/twitch_cleaned_edges.csv\", delimiter=\",\", nodetype=int, data=False)\n",
    "\n",
    "laplacian_sparse = nx.normalized_laplacian_matrix(G).astype(np.float64)\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "inv_sqrt_degrees = np.array([1 / np.sqrt(degrees[node]) if degrees[node] != 0 else 0 for node in G.nodes()])\n",
    "\n",
    "# Step 4: Compute the smallest 6 eigenvalues of the Laplacian (for example)\n",
    "num_eigenvalues = 20\n",
    "eigenvalues, eigenvectors = eigsh(laplacian_sparse, k=num_eigenvalues, which=\"SM\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3a9892a2d7ffd91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 2\n",
    "# Perform K-means clustering on the eigenvectors\n",
    "kmeans = KMeans(n_clusters=K, n_init=10, random_state=42)\n",
    "scaled_eigenvectors = inv_sqrt_degrees[:, np.newaxis] * eigenvectors\n",
    "\n",
    "clusters = kmeans.fit_predict(scaled_eigenvectors[:, :K])\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "sqrt_degrees = np.array([np.sqrt(degrees[node]) for node in G.nodes()])\n",
    "\n",
    "# Compute Rayleigh quotients for each cluster\n",
    "rayleigh_quotients = []\n",
    "indicator_vectors = np.zeros(eigenvectors.shape)\n",
    "\n",
    "for cluster_idx in range(K):\n",
    "    # Create indicator vector for the cluster\n",
    "    indicator_vector = (clusters == cluster_idx).astype(float) * sqrt_degrees\n",
    "    indicator_vector = indicator_vector / np.linalg.norm(indicator_vector, 2)\n",
    "    indicator_vectors[:, cluster_idx] = indicator_vector\n",
    "    # Compute Rayleigh quotient\n",
    "    numerator = indicator_vector.T @ laplacian_sparse @ indicator_vector\n",
    "    denominator = indicator_vector.T @ indicator_vector\n",
    "    rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "rayleigh_quotients = sorted(rayleigh_quotients)\n",
    "beta_K_by_K = indicator_vectors.T @ scaled_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "combined_indicator_vectors = gram_schmidt(combined_indicator_vectors)\n",
    "combined_rayleigh_quotients = []\n",
    "for i in range(K):\n",
    "    numerator = combined_indicator_vectors[:, i].T @ laplacian_sparse @ combined_indicator_vectors[:, i]\n",
    "    denominator = combined_indicator_vectors[:, i].T @ combined_indicator_vectors[:, i]\n",
    "    combined_rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "combined_rayleigh_quotients = sorted(combined_rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "143c1521098c0a65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_val_matrix_combined = eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "rayleigh_quotients = np.round(rayleigh_quotients, 6)\n",
    "normalized_L_eigenvalues = np.round(eigenvalues, 6)\n",
    "\n",
    "#b, v = apply_recursive_st_search(rayleigh_quotients[0:4], normalized_L_eigenvalues[0:4], 0, error = 0, bound_indices = [], values = [])\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, eigenvalues[0:K+1], K)\n",
    "\n",
    "\n",
    "twitch_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "twitch_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(eigenvectors))\n",
    "twitch_dataset_results[\"N\"] = len(eigenvectors)\n",
    "print(\"Number of edges M: \", np.sum(list(degrees.values())) / 2)\n",
    "twitch_dataset_results[\"M\"] = np.sum(list(degrees.values())) / 2\n",
    "print(\"True value: \", true_val_combined)\n",
    "twitch_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \", max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K])\n",
    "twitch_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients)/(K * eigenvalues[K]))\n",
    "twitch_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients)/(K * eigenvalues[K])\n",
    "print(\"Recursive ST: \", v/K)\n",
    "twitch_dataset_results[\"Recursive ST\"] = v/K\n",
    "\n",
    "dataset_results[\"Twitch\"] = twitch_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b00e02f8545d8b31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Last Fm Asia"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3977b0b9488674b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 1: Define the file path\n",
    "edge_list_path = \"Data/lasftm_asia/lastfm_asia_edges.csv\"\n",
    "df = pd.read_csv(edge_list_path)\n",
    "df.to_csv(\"Data/lasftm_asia/lastfm_asia_edges.csv\", index=False, header=False)\n",
    "# Step 2: Load the Deezer graph\n",
    "# Assuming the edge list is in CSV format with two columns: source and target\n",
    "G = nx.read_edgelist(\"Data/lasftm_asia/lastfm_asia_edges.csv\", delimiter=\",\", nodetype=int, data=False)\n",
    "\n",
    "laplacian_sparse = nx.normalized_laplacian_matrix(G).astype(np.float64)\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "inv_sqrt_degrees = np.array([1 / np.sqrt(degrees[node]) if degrees[node] != 0 else 0 for node in G.nodes()])\n",
    "\n",
    "# Step 4: Compute the smallest 6 eigenvalues of the Laplacian (for example)\n",
    "num_eigenvalues = 20\n",
    "eigenvalues, eigenvectors = eigsh(laplacian_sparse, k=num_eigenvalues, which=\"SM\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f69fede9155acd7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 2\n",
    "# Perform K-means clustering on the eigenvectors\n",
    "kmeans = KMeans(n_clusters=K, n_init=10, random_state=42)\n",
    "scaled_eigenvectors = inv_sqrt_degrees[:, np.newaxis] * eigenvectors\n",
    "\n",
    "clusters = kmeans.fit_predict(scaled_eigenvectors[:, :K])\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "sqrt_degrees = np.array([np.sqrt(degrees[node]) for node in G.nodes()])\n",
    "\n",
    "# Compute Rayleigh quotients for each cluster\n",
    "rayleigh_quotients = []\n",
    "indicator_vectors = np.zeros(eigenvectors.shape)\n",
    "\n",
    "for cluster_idx in range(K):\n",
    "    # Create indicator vector for the cluster\n",
    "    indicator_vector = (clusters == cluster_idx).astype(float) * sqrt_degrees\n",
    "    indicator_vector = indicator_vector / np.linalg.norm(indicator_vector, 2)\n",
    "    indicator_vectors[:, cluster_idx] = indicator_vector\n",
    "    # Compute Rayleigh quotient\n",
    "    numerator = indicator_vector.T @ laplacian_sparse @ indicator_vector\n",
    "    denominator = indicator_vector.T @ indicator_vector\n",
    "    rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "rayleigh_quotients = sorted(rayleigh_quotients)\n",
    "beta_K_by_K = indicator_vectors.T @ scaled_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "combined_indicator_vectors = gram_schmidt(combined_indicator_vectors)\n",
    "combined_rayleigh_quotients = []\n",
    "for i in range(K):\n",
    "    numerator = combined_indicator_vectors[:, i].T @ laplacian_sparse @ combined_indicator_vectors[:, i]\n",
    "    denominator = combined_indicator_vectors[:, i].T @ combined_indicator_vectors[:, i]\n",
    "    combined_rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "combined_rayleigh_quotients = sorted(combined_rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "193220d844d778e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lastfm_dataset_results = {}\n",
    "true_val_matrix_combined = eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "rayleigh_quotients = np.round(rayleigh_quotients, 6)\n",
    "normalized_L_eigenvalues = np.round(eigenvalues, 6)\n",
    "\n",
    "#b, v = apply_recursive_st_search(rayleigh_quotients[0:4], normalized_L_eigenvalues[0:4], 0, error = 0, bound_indices = [], values = [])\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, eigenvalues[0:K+1], K)\n",
    "\n",
    "\n",
    "lastfm_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "lastfm_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(eigenvectors))\n",
    "lastfm_dataset_results[\"N\"] = len(eigenvectors)\n",
    "print(\"Number of edges M: \", np.sum(list(degrees.values())) / 2)\n",
    "lastfm_dataset_results[\"M\"] = np.sum(list(degrees.values())) / 2\n",
    "print(\"True value: \", true_val_combined)\n",
    "lastfm_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \", max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K])\n",
    "lastfm_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients)/(K * eigenvalues[K]))\n",
    "lastfm_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients)/(K * eigenvalues[K])\n",
    "print(\"Recursive ST: \", v/K)\n",
    "lastfm_dataset_results[\"Recursive ST\"] = v/K\n",
    "\n",
    "dataset_results[\"LastFM\"] = lastfm_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8ddd7459fcf5b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gemsec Facebook - Athletes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbbbb209d7b64f97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 1: Define the file path\n",
    "edge_list_path = \"Data/facebook_clean_data/athletes_edges.csv\"\n",
    "df = pd.read_csv(edge_list_path)\n",
    "df.to_csv(\"Data/facebook_clean_data/athletes_edges.csv\", index=False, header=False)\n",
    "# Step 2: Load the Deezer graph\n",
    "# Assuming the edge list is in CSV format with two columns: source and target\n",
    "G = nx.read_edgelist(\"Data/facebook_clean_data/athletes_edges.csv\", delimiter=\",\", nodetype=int, data=False)\n",
    "\n",
    "laplacian_sparse = nx.normalized_laplacian_matrix(G).astype(np.float64)\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "inv_sqrt_degrees = np.array([1 / np.sqrt(degrees[node]) if degrees[node] != 0 else 0 for node in G.nodes()])\n",
    "\n",
    "# Step 4: Compute the smallest 6 eigenvalues of the Laplacian (for example)\n",
    "num_eigenvalues = 20\n",
    "eigenvalues, eigenvectors = eigsh(laplacian_sparse, k=num_eigenvalues, which=\"SM\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "184a6ac8ad5ca18f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 3\n",
    "# Perform K-means clustering on the eigenvectors\n",
    "kmeans = KMeans(n_clusters=K, n_init=10, random_state=42)\n",
    "scaled_eigenvectors = inv_sqrt_degrees[:, np.newaxis] * eigenvectors\n",
    "\n",
    "clusters = kmeans.fit_predict(scaled_eigenvectors[:, :K])\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "sqrt_degrees = np.array([np.sqrt(degrees[node]) for node in G.nodes()])\n",
    "\n",
    "# Compute Rayleigh quotients for each cluster\n",
    "rayleigh_quotients = []\n",
    "indicator_vectors = np.zeros(eigenvectors.shape)\n",
    "\n",
    "for cluster_idx in range(K):\n",
    "    # Create indicator vector for the cluster\n",
    "    indicator_vector = (clusters == cluster_idx).astype(float) * sqrt_degrees\n",
    "    indicator_vector = indicator_vector / np.linalg.norm(indicator_vector, 2)\n",
    "    indicator_vectors[:, cluster_idx] = indicator_vector\n",
    "    # Compute Rayleigh quotient\n",
    "    numerator = indicator_vector.T @ laplacian_sparse @ indicator_vector\n",
    "    denominator = indicator_vector.T @ indicator_vector\n",
    "    rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "rayleigh_quotients = sorted(rayleigh_quotients)\n",
    "beta_K_by_K = indicator_vectors.T @ scaled_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "combined_indicator_vectors = gram_schmidt(combined_indicator_vectors)\n",
    "\n",
    "combined_rayleigh_quotients = []\n",
    "for i in range(K):\n",
    "    numerator = combined_indicator_vectors[:, i].T @ laplacian_sparse @ combined_indicator_vectors[:, i]\n",
    "    denominator = combined_indicator_vectors[:, i].T @ combined_indicator_vectors[:, i]\n",
    "    combined_rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "combined_rayleigh_quotients = sorted(combined_rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e695a0d490ab1fa7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "facebook_athletes_dataset_results = {}\n",
    "true_val_matrix_combined = eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "rayleigh_quotients = np.round(rayleigh_quotients, 6)\n",
    "normalized_L_eigenvalues = np.round(eigenvalues, 6)\n",
    "\n",
    "#b, v = apply_recursive_st_search(rayleigh_quotients[0:4], normalized_L_eigenvalues[0:4], 0, error = 0, bound_indices = [], values = [])\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, eigenvalues[0:K+1], K)\n",
    "\n",
    "\n",
    "facebook_athletes_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "facebook_athletes_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(eigenvectors))\n",
    "facebook_athletes_dataset_results[\"N\"] = len(eigenvectors)\n",
    "print(\"Number of edges M: \", np.sum(list(degrees.values())) / 2)\n",
    "facebook_athletes_dataset_results[\"M\"] = np.sum(list(degrees.values())) / 2\n",
    "print(\"True value: \", true_val_combined)\n",
    "facebook_athletes_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \", max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K])\n",
    "facebook_athletes_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients)/(K * eigenvalues[K]))\n",
    "facebook_athletes_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients)/(K * eigenvalues[K])\n",
    "print(\"Recursive ST: \", v/K)\n",
    "facebook_athletes_dataset_results[\"Recursive ST\"] = v/K\n",
    "\n",
    "dataset_results[\"Facebook (Athletes)\"] = facebook_athletes_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cf6d15798d7f735"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collaborations CA-CondMat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f71033631f91d3a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Define the file path\n",
    "# Define the file path for the input and output\n",
    "input_file = \"Data/ca-CondMat.txt/CA-CondMat.txt\"  # Replace with your actual file path\n",
    "output_file = \"Data/ca-CondMat.txt/edge_list.txt\"\n",
    "\n",
    "# Initialize a set to store unique edges (to ensure undirected representation)\n",
    "edge_set = set()\n",
    "\n",
    "# Open the input file and process each line\n",
    "with open(input_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Skip comment lines starting with '#'\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "\n",
    "        # Split the line into nodes\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            node1, node2 = parts\n",
    "\n",
    "            # Add the edge in a sorted order to avoid duplicates (undirected graph)\n",
    "            edge = tuple(sorted((int(node1), int(node2))))\n",
    "            edge_set.add(edge)\n",
    "\n",
    "# Write the unique edges to the output file\n",
    "with open(output_file, \"w\") as file:\n",
    "    for edge in sorted(edge_set):  # Sort edges for consistency\n",
    "        file.write(f\"{edge[0]}\\t{edge[1]}\\n\")\n",
    "\n",
    "print(f\"Edge list extracted and saved to {output_file}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0e9fc35e6172bfd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Load the Deezer graph\n",
    "# Assuming the edge list is in CSV format with two columns: source and target\n",
    "G = nx.read_edgelist(\"Data/ca-CondMat.txt/edge_list.txt\", nodetype=int)\n",
    "LCC_nodes = max(nx.connected_components(G), key=len)\n",
    "G = G.subgraph(LCC_nodes)\n",
    "laplacian_sparse = nx.normalized_laplacian_matrix(G).astype(np.float64)\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "inv_sqrt_degrees = np.array([1 / np.sqrt(degrees[node]) if degrees[node] != 0 else 0 for node in G.nodes()])\n",
    "\n",
    "# Step 4: Compute the smallest 6 eigenvalues of the Laplacian (for example)\n",
    "num_eigenvalues = 10\n",
    "eigenvalues, eigenvectors = eigsh(laplacian_sparse, k=num_eigenvalues, which=\"SM\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7a96a06f953175"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 3\n",
    "# Perform K-means clustering on the eigenvectors\n",
    "kmeans = KMeans(n_clusters=K, n_init=10, random_state=42)\n",
    "scaled_eigenvectors = inv_sqrt_degrees[:, np.newaxis] * eigenvectors\n",
    "\n",
    "clusters = kmeans.fit_predict(scaled_eigenvectors[:, :K])\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "sqrt_degrees = np.array([np.sqrt(degrees[node]) for node in G.nodes()])\n",
    "\n",
    "# Compute Rayleigh quotients for each cluster\n",
    "rayleigh_quotients = []\n",
    "indicator_vectors = np.zeros(eigenvectors.shape)\n",
    "\n",
    "for cluster_idx in range(K):\n",
    "    # Create indicator vector for the cluster\n",
    "    indicator_vector = (clusters == cluster_idx).astype(float) * sqrt_degrees\n",
    "    indicator_vector = indicator_vector / np.linalg.norm(indicator_vector, 2)\n",
    "    indicator_vectors[:, cluster_idx] = indicator_vector\n",
    "    # Compute Rayleigh quotient\n",
    "    numerator = indicator_vector.T @ laplacian_sparse @ indicator_vector\n",
    "    denominator = indicator_vector.T @ indicator_vector\n",
    "    rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "rayleigh_quotients = sorted(rayleigh_quotients)\n",
    "beta_K_by_K = indicator_vectors.T @ scaled_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "combined_indicator_vectors = gram_schmidt(combined_indicator_vectors)\n",
    "combined_rayleigh_quotients = []\n",
    "for i in range(K):\n",
    "    numerator = combined_indicator_vectors[:, i].T @ laplacian_sparse @ combined_indicator_vectors[:, i]\n",
    "    denominator = combined_indicator_vectors[:, i].T @ combined_indicator_vectors[:, i]\n",
    "    combined_rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "combined_rayleigh_quotients = sorted(combined_rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed6a7fd599be8169"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collaborations_dataset_results = {}\n",
    "true_val_matrix_combined = eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "rayleigh_quotients = np.round(rayleigh_quotients, 6)\n",
    "normalized_L_eigenvalues = np.round(eigenvalues, 6)\n",
    "\n",
    "#b, v = apply_recursive_st_search(rayleigh_quotients[0:4], normalized_L_eigenvalues[0:4], 0, error = 0, bound_indices = [], values = [])\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, eigenvalues[0:K+1], K)\n",
    "\n",
    "\n",
    "collaborations_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "collaborations_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(eigenvectors))\n",
    "collaborations_dataset_results[\"N\"] = len(eigenvectors)\n",
    "print(\"Number of edges M: \", np.sum(list(degrees.values())) / 2)\n",
    "collaborations_dataset_results[\"M\"] = np.sum(list(degrees.values())) / 2\n",
    "print(\"True value: \", true_val_combined)\n",
    "collaborations_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \", max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K])\n",
    "collaborations_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients)/(K * eigenvalues[K]))\n",
    "collaborations_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients)/(K * eigenvalues[K])\n",
    "print(\"Recursive ST: \", v/K)\n",
    "collaborations_dataset_results[\"Recursive ST\"] = v/K\n",
    "\n",
    "dataset_results[\"Collaborations (CA-CondMat)\"] = collaborations_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20dd8b7985267616"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d57bc9cf28c3f34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset_results).T"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "329f8b92ea33d7a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1 style=\"font-size: 50px\">Directed Graphs</h1>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad51f13334a10530"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62dd8cc6f944a9a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import urllib.request\n",
    "from helpers import compute_volume, compute_weight_between_sets, Psi, compute_theta, compute_new_bound, compute_ls_bounds"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7197a352d889450b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DSBMs\n",
    "First we consider a DSBM with a 4 cycle for its cluster structure where we add noise to the clusters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad0f95ec43e3ec27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_hermitian_adjacency_matrix(N, edges, root_of_unity):\n",
    "    A = np.zeros((N, N), dtype = np.complex128)\n",
    "    for edge in edges:\n",
    "        u, v = edge\n",
    "        A[u, v] = np.exp(2* 1j * np.pi / root_of_unity)\n",
    "        A[v, u] = np.conj(A[u, v])\n",
    "    return A\n",
    "\n",
    "def get_adjacency_matrix(N, edges):\n",
    "    A = np.zeros((N, N))\n",
    "    for edge in edges:\n",
    "        u, v = edge\n",
    "        A[u, v] = 1\n",
    "    return A"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85641f6259d18c24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# construct directed graph with 10 cycle\n",
    "n = [0, 100, 200, 300, 400]\n",
    "N = n[-1]\n",
    "k = 4\n",
    "\n",
    "bounds = {}\n",
    "\n",
    "partition = []\n",
    "for i in range(k):\n",
    "    partition.append([j for j in range(n[i], n[i + 1])])\n",
    "### FOR CYCLE\n",
    "C = [(i, (i + 1) % k) for i in range(k)]\n",
    "C_path = [(i, i + 1) for i in range(k - 1)]\n",
    "\n",
    "F = np.array([[0.5, 1.0, 0.5, 0.0],\n",
    "              [0.0, 0.5, 1.0, 0.5],\n",
    "              [0.5, 0.0, 0.5, 1.0],\n",
    "              [1.0, 0.5, 0.0, 0.5]])\n",
    "\n",
    "def generate_P_cycle(eps=0.01):\n",
    "    P = [[eps, 1.0, eps, 1.0],\n",
    "         [1.0, eps, 1.0, eps],\n",
    "         [eps, 1.0, eps, 1.0],\n",
    "         [1.0, eps, 1.0, eps]]\n",
    "    return np.array(P)\n",
    "\n",
    "sample_size = 10 # SET TO 1 FOR TESTING\n",
    "\n",
    "true_vals = []\n",
    "new_bounds = []\n",
    "new_bound_rayleigh_quotients = []\n",
    "ls_bound_1s = []\n",
    "ls_bound_2s = []\n",
    "psis = []\n",
    "lambda_1s = []\n",
    "lambda_2s = []\n",
    "rayleigh_quotients = []\n",
    "\n",
    "for eps in np.arange(0, 0.2, 0.01):\n",
    "    P = generate_P_cycle(eps)\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "    \n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                if i == j:\n",
    "                    prob_existing_edge = P[i, j]\n",
    "                    for index, u in enumerate(partition[i]):\n",
    "                        for v in partition[i][index + 1:]:\n",
    "                            if u == v:\n",
    "                                continue\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                if np.random.rand() <= F[i, j]:\n",
    "                                    edges.append((u, v))\n",
    "                                else:\n",
    "                                    edges.append((v, u))\n",
    "                else:\n",
    "                    prob_existing_edge = P[i, j]\n",
    "                    for u in partition[i]:\n",
    "                        for v in partition[j]:\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                if np.random.rand() <= F[i, j]:\n",
    "                                    edges.append((u, v))\n",
    "                                else:\n",
    "                                    edges.append((v, u))\n",
    "    \n",
    "    \n",
    "        A = get_hermitian_adjacency_matrix(N = N, edges = edges, root_of_unity=k)\n",
    "        degrees = np.sum(np.abs(A), axis=1)\n",
    "        D = np.diag(degrees)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "        norm_L = np.eye(int(N)) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "        \n",
    "        # construct indicator vector\n",
    "        ind_vector = np.zeros((N,), dtype = np.complex128)\n",
    "        for i in range(k):\n",
    "            ind_vector[n[i]:n[i+1]] = np.exp(-1j * 2 * np.pi * i / k)\n",
    "        ind_vector = D_sqrt @ ind_vector\n",
    "        ind_vector = ind_vector / np.linalg.norm(ind_vector)\n",
    "        \n",
    "        # compute rayleigh quotient of indicator vector\n",
    "        chi_rayleigh_quotient = np.real((np.conj(ind_vector).T @ norm_L @ ind_vector) / (np.conj(ind_vector).T @ ind_vector))\n",
    "        \n",
    "    \n",
    "        #Now computing using Laenen & Sun's choice of roots of unity\n",
    "        k_ls = int(np.ceil(2 * np.pi * k))\n",
    "        A_ls = get_hermitian_adjacency_matrix(N = N, edges = edges, root_of_unity=k_ls)\n",
    "        D_ls = np.sum(np.abs(A_ls), axis=1)\n",
    "        D_inv_sqrt_ls = np.diag(1 / np.sqrt(D_ls))\n",
    "        norm_L_ls = np.eye(int(N)) - D_inv_sqrt_ls @ A_ls @ D_inv_sqrt_ls\n",
    "    \n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = eigvals.argsort()\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        ls_eigvals, ls_eigvecs = np.linalg.eigh(norm_L_ls)\n",
    "        # sort\n",
    "        idx = ls_eigvals.argsort()\n",
    "        ls_eigvals = ls_eigvals[idx]\n",
    "        ls_eigvecs = ls_eigvecs[:, idx]\n",
    "        A_standard = get_adjacency_matrix(N, edges)\n",
    "        ups = Psi(partition, A_standard, degrees, C)\n",
    "        theta = compute_theta(partition, A_standard, degrees, C_path)\n",
    "        \n",
    "        # computing true value\n",
    "        true_val = np.linalg.norm(eigvecs[:,0] - (np.conj(ind_vector).T @ eigvecs[:,0]) * ind_vector)**2\n",
    "        \n",
    "        new_bound_rayleigh_quotient = min((chi_rayleigh_quotient - eigvals[0])/ (eigvals[1] - eigvals[0]), chi_rayleigh_quotient / eigvals[1])\n",
    "        new_bound = compute_new_bound(eigvals, ups)\n",
    "        ls_bound_1, ls_bound_2 = compute_ls_bounds(ls_eigvals, theta, k)\n",
    "        \n",
    "        true_vals.append(true_val)\n",
    "        new_bounds.append(new_bound)\n",
    "        new_bound_rayleigh_quotients.append(new_bound_rayleigh_quotient)\n",
    "        ls_bound_1s.append(ls_bound_1)\n",
    "        ls_bound_2s.append(ls_bound_2)\n",
    "        psis.append(ups)\n",
    "        lambda_1s.append(eigvals[0])\n",
    "        lambda_2s.append(eigvals[1])\n",
    "        rayleigh_quotients.append(chi_rayleigh_quotient)\n",
    "\n",
    "    true_val = np.mean(true_vals)\n",
    "    new_bound = np.mean(new_bounds)\n",
    "    new_bound_rayleigh_quotient = np.mean(new_bound_rayleigh_quotients)\n",
    "    ls_bound_1 = np.mean(ls_bound_1s)\n",
    "    ls_bound_2 = np.mean(ls_bound_2s)\n",
    "    ups = np.mean(psis)\n",
    "    lambda_1 = np.mean(lambda_1s)\n",
    "    lambda_2 = np.mean(lambda_2s)\n",
    "    chi_rayleigh_quotient = np.mean(rayleigh_quotients)\n",
    "    \n",
    "    bounds[eps] = {'true_val': true_val,\n",
    "                   'new_bound': new_bound,\n",
    "                   'new_bound_rayleigh_quotient':new_bound_rayleigh_quotient,\n",
    "                   'ls_bound_1': ls_bound_1,\n",
    "                   'ls_bound_2': ls_bound_2,\n",
    "                   'Psi': ups,\n",
    "                   'lambda_1': eigvals[0],\n",
    "                   'lambda_2': eigvals[1],\n",
    "                   'rayleigh quotient': chi_rayleigh_quotient}\n",
    "bounds_df = pd.DataFrame(bounds).T\n",
    "bounds_df.columns = [r'$\\|f_1 - \\alpha \\chi\\|^2$',\n",
    "                     r'$\\frac{4 \\Psi - \\lambda_1}{\\lambda_2 - \\lambda_1}$',\n",
    "                      r'$\\frac{\\chi^* \\mathcal{L} \\chi - \\lambda_1}{\\lambda_2 - \\lambda_1}$',\n",
    "                     r'$\\frac{1}{\\eta_k}$',\n",
    "                     r'$\\frac{1}{\\eta_4-1}$',\n",
    "                     'ups',\n",
    "                     'lambda_1',\n",
    "                     'lambda_2',\n",
    "                     'rayleigh quotient']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "406b554f2362bff0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_for_plot = bounds_df[[r'$\\|f_1 - \\alpha \\chi\\|^2$', r'$\\frac{4 \\Psi - \\lambda_1}{\\lambda_2 - \\lambda_1}$', r'$\\frac{\\chi^* \\mathcal{L} \\chi - \\lambda_1}{\\lambda_2 - \\lambda_1}$', r'$\\frac{1}{\\eta_4-1}$']][\n",
    "    (bounds_df[r'$\\frac{4 \\Psi - \\lambda_1}{\\lambda_2 - \\lambda_1}$'] <= 0.8) + (bounds_df[r'$\\frac{1}{\\eta_4-1}$'] <= 0.8)].iloc[1:,:]\n",
    "df_for_plot.columns = ['Actual Error'+ r'\\n$\\|f_1 - \\alpha \\chi\\|^2$',  \n",
    "                       'Our bound ' + r'\\n$\\frac{4 \\Psi - \\lambda_1}{\\lambda_2 - \\lambda_1}$', \n",
    "                       'Our bound (Rayleigh Quotient) '+ r'\\n$\\frac{\\chi^* \\mathcal{L} \\chi - \\lambda_1}{\\lambda_2 - \\lambda_1}$',\n",
    "                       'LS bound ' + r'\\n$\\frac{1}{\\eta_4-1}$']\n",
    "\n",
    "# Plot the dataframe\n",
    "ax = df_for_plot.plot(\n",
    "     marker='o', linestyle='-', color=['purple','blue','green', 'red'],\n",
    "    figsize=(8\n",
    "             ,5), logy=True) #title=r'Cyclic DSBM (k=4)',\n",
    "\n",
    "# Set labels and grid\n",
    "plt.xlabel(r'Noise $\\epsilon$', fontsize=20)\n",
    "plt.ylabel(r'Error', fontsize=20)\n",
    "plt.grid(True)\n",
    "\n",
    "legend_labels = [\n",
    "    'Actual',  \n",
    "    'Ours' + r' ($\\Psi$)', \n",
    "    'Ours (Rayleigh)',\n",
    "    'LS bound'\n",
    "    ]\n",
    "# Set custom legend with smaller font for equations\n",
    "plt.legend(\n",
    "    legend_labels, fontsize=20, loc='upper left', bbox_to_anchor=(1, 1),labelspacing=1.5, ncol=1)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Figures/4_cluster_cycle_noise.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ab3dc863bdc1695"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we consider a similar graph but where the cluster structure is a path of 4 clusters rather than a cycle."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddf0ce4f0bee45a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# construct directed graph with 10 cycle\n",
    "n = [0, 100, 200, 300, 400]\n",
    "N = n[-1]\n",
    "k = 4\n",
    "\n",
    "bounds = {}\n",
    "\n",
    "partition = []\n",
    "for i in range(k):\n",
    "    partition.append([j for j in range(n[i], n[i + 1])])\n",
    "### FOR CYCLE\n",
    "C = [(i, (i + 1) % k) for i in range(k)]\n",
    "C_path = [(i, i + 1) for i in range(k - 1)]\n",
    "\n",
    "F = np.array([[0.5, 1.0, 0.5, 0.5],\n",
    "              [0.0, 0.5, 1.0, 0.5],\n",
    "              [0.5, 0.0, 0.5, 1.0],\n",
    "              [0.5, 0.5, 0.0, 0.5]])\n",
    "\n",
    "def generate_P_cycle(eps=0.01):\n",
    "    P = [[eps, 1.0, eps, eps],\n",
    "         [1.0, eps, 1.0, eps],\n",
    "         [eps, 1.0, eps, 1.0],\n",
    "         [eps, eps, 1.0, eps]]\n",
    "    return np.array(P)\n",
    "\n",
    "sample_size = 10 # SET TO 1 FOR TESTING\n",
    "\n",
    "true_vals = []\n",
    "new_bounds = []\n",
    "new_bound_rayleigh_quotients = []\n",
    "ls_bound_1s = []\n",
    "ls_bound_2s = []\n",
    "psis = []\n",
    "lambda_1s = []\n",
    "lambda_2s = []\n",
    "rayleigh_quotients = []\n",
    "\n",
    "for eps in np.arange(0, 0.2, 0.01):\n",
    "    P = generate_P_cycle(eps)\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "    \n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                if i == j:\n",
    "                    prob_existing_edge = P[i, j]\n",
    "                    for index, u in enumerate(partition[i]):\n",
    "                        for v in partition[i][index + 1:]:\n",
    "                            if u == v:\n",
    "                                continue\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                if np.random.rand() <= F[i, j]:\n",
    "                                    edges.append((u, v))\n",
    "                                else:\n",
    "                                    edges.append((v, u))\n",
    "                else:\n",
    "                    prob_existing_edge = P[i, j]\n",
    "                    for u in partition[i]:\n",
    "                        for v in partition[j]:\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                if np.random.rand() <= F[i, j]:\n",
    "                                    edges.append((u, v))\n",
    "                                else:\n",
    "                                    edges.append((v, u))\n",
    "    \n",
    "    \n",
    "        A = get_hermitian_adjacency_matrix(N = N, edges = edges, root_of_unity=k)\n",
    "        degrees = np.sum(np.abs(A), axis=1)\n",
    "        D = np.diag(degrees)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "        norm_L = np.eye(int(N)) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "        \n",
    "        # construct indicator vector\n",
    "        ind_vector = np.zeros((N,), dtype = np.complex128)\n",
    "        for i in range(k):\n",
    "            ind_vector[n[i]:n[i+1]] = np.exp(-1j * 2 * np.pi * i / k)\n",
    "        ind_vector = D_sqrt @ ind_vector\n",
    "        ind_vector = ind_vector / np.linalg.norm(ind_vector)\n",
    "        \n",
    "        # compute rayleigh quotient of indicator vector\n",
    "        chi_rayleigh_quotient = np.real((np.conj(ind_vector).T @ norm_L @ ind_vector) / (np.conj(ind_vector).T @ ind_vector))\n",
    "    \n",
    "\n",
    "        #Now computing using Laenen & Sun's choice of roots of unity\n",
    "        k_ls = int(np.ceil(2 * np.pi * k))\n",
    "        A_ls = get_hermitian_adjacency_matrix(N = N, edges = edges, root_of_unity=k_ls)\n",
    "        D_ls = np.sum(np.abs(A_ls), axis=1)\n",
    "        D_inv_sqrt_ls = np.diag(1 / np.sqrt(D_ls))\n",
    "        norm_L_ls = np.eye(int(N)) - D_inv_sqrt_ls @ A_ls @ D_inv_sqrt_ls\n",
    "    \n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = eigvals.argsort()\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        ls_eigvals, ls_eigvecs = np.linalg.eigh(norm_L_ls)\n",
    "        # sort\n",
    "        idx = ls_eigvals.argsort()\n",
    "        ls_eigvals = ls_eigvals[idx]\n",
    "        ls_eigvecs = ls_eigvecs[:, idx]\n",
    "        A_standard = get_adjacency_matrix(N, edges)\n",
    "        ups = Psi(partition, A_standard, degrees, C)\n",
    "        theta = compute_theta(partition, A_standard, degrees, C_path)\n",
    "        \n",
    "        # computing true value\n",
    "        true_val = np.linalg.norm(eigvecs[:,0] - (np.conj(ind_vector).T @ eigvecs[:,0]) * ind_vector)**2\n",
    "    \n",
    "        new_bound_rayleigh_quotient = min((chi_rayleigh_quotient - eigvals[0])/ (eigvals[1] - eigvals[0]), chi_rayleigh_quotient / eigvals[1])\n",
    "        new_bound = compute_new_bound(eigvals, ups)\n",
    "        ls_bound_1, ls_bound_2 = compute_ls_bounds(ls_eigvals, theta, k)\n",
    "        \n",
    "        true_vals.append(true_val)\n",
    "        new_bounds.append(new_bound)\n",
    "        new_bound_rayleigh_quotients.append(new_bound_rayleigh_quotient)\n",
    "        ls_bound_1s.append(ls_bound_1)\n",
    "        ls_bound_2s.append(ls_bound_2)\n",
    "        psis.append(ups)\n",
    "        lambda_1s.append(eigvals[0])\n",
    "        lambda_2s.append(eigvals[1])\n",
    "        rayleigh_quotients.append(chi_rayleigh_quotient)\n",
    "\n",
    "    true_val = np.mean(true_vals)\n",
    "    new_bound = np.mean(new_bounds)\n",
    "    new_bound_rayleigh_quotient = np.mean(new_bound_rayleigh_quotients)\n",
    "    ls_bound_1 = np.mean(ls_bound_1s)\n",
    "    ls_bound_2 = np.mean(ls_bound_2s)\n",
    "    ups = np.mean(psis)\n",
    "    lambda_1 = np.mean(lambda_1s)\n",
    "    lambda_2 = np.mean(lambda_2s)\n",
    "    chi_rayleigh_quotient = np.mean(rayleigh_quotients)\n",
    "    \n",
    "    bounds[eps] = {'true_val': true_val,\n",
    "                   'new_bound': new_bound,\n",
    "                   'new_bound_rayleigh_quotient':new_bound_rayleigh_quotient,\n",
    "                   'ls_bound_1': ls_bound_1,\n",
    "                   'ls_bound_2': ls_bound_2,\n",
    "                   'Psi': ups,\n",
    "                   'lambda_1': eigvals[0],\n",
    "                   'lambda_2': eigvals[1],\n",
    "                   'rayleigh quotient': chi_rayleigh_quotient}\n",
    "bounds_df = pd.DataFrame(bounds).T\n",
    "bounds_df.columns = [r'$\\|f_1 - \\alpha \\chi\\|^2$',\n",
    "                     r'$\\frac{4 \\Psi - \\lambda_1}{\\lambda_2 - \\lambda_1}$',\n",
    "                      r'$\\frac{\\chi^* \\mathcal{L} \\chi - \\lambda_1}{\\lambda_2 - \\lambda_1}$',\n",
    "                     r'$\\frac{1}{\\eta_k}$',\n",
    "                     r'$\\frac{1}{\\eta_4-1}$',\n",
    "                     'ups',\n",
    "                     'lambda_1',\n",
    "                     'lambda_2',\n",
    "                     'rayleigh quotient']\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdde844e853d8ef4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_for_plot = bounds_df[[r'$\\|f_1 - \\alpha \\chi\\|^2$', r'$\\frac{4 \\Psi - \\lambda_1}{\\lambda_2 - \\lambda_1}$', r'$\\frac{\\chi^* \\mathcal{L} \\chi - \\lambda_1}{\\lambda_2 - \\lambda_1}$', r'$\\frac{1}{\\eta_4-1}$']][\n",
    "    (bounds_df[r'$\\frac{4 \\Psi - \\lambda_1}{\\lambda_2 - \\lambda_1}$'] <= 0.8) + (bounds_df[r'$\\frac{1}{\\eta_4-1}$'] <= 0.8)].iloc[1:,:]\n",
    "df_for_plot.columns = ['Actual Error'+ r'\\n$\\|f_1 - \\alpha \\chi\\|^2$',  \n",
    "                       'Our bound ' + r'\\n$\\frac{4 \\Psi - \\lambda_1}{\\lambda_2 - \\lambda_1}$', \n",
    "                       'Our bound (Rayleigh Quotient) '+ r'\\n$\\frac{\\chi^* \\mathcal{L} \\chi - \\lambda_1}{\\lambda_2 - \\lambda_1}$',\n",
    "                       'LS bound ' + r'\\n$\\frac{1}{\\eta_4-1}$']\n",
    "\n",
    "# Plot the dataframe\n",
    "ax = df_for_plot.plot(\n",
    "     marker='o', linestyle='-', color=['purple','blue','green', 'red'],\n",
    "    figsize=(8\n",
    "             ,5), logy=True) #title=r'Path DSBM (k=4)',\n",
    "\n",
    "# Set labels and grid\n",
    "plt.xlabel(r'Noise $\\epsilon$', fontsize=20)\n",
    "plt.ylabel(r'Error', fontsize=20)\n",
    "plt.grid(True)\n",
    "\n",
    "legend_labels = [\n",
    "    'Actual',  \n",
    "    'Ours' + r' ($\\Psi$)', \n",
    "    'Ours (Rayleigh)',\n",
    "    'LS bound'\n",
    "    ]\n",
    "\n",
    "# Set custom legend with smaller font for equations\n",
    "plt.legend(\n",
    "    legend_labels, fontsize=20, loc='upper left', bbox_to_anchor=(1, 1),labelspacing=1.5, ncol=1)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('Figures/4_cluster_path_noise.png',bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aad77def9c1767f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# YellowStone Graph\n",
    "In this section we produce the plot the bounds and some figures for our yellowstone graph. Since k is small for this graph (and the others), we will search for the best permutation of the clusters by brute force for computing $\\Psi$ and $\\eta$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8651e4a066befec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Nodes and edges for the second image\n",
    "nodes = [\n",
    "    \"Plants,\\n Flowers,\\n Nuts,\\n Seeds,\\n Fruit,\\n Insects\", \"Pika\", \"Red-breasted\\n nuthatch\",\n",
    "    \"Pacific\\n tree frog\", \"Edith's\\n checkerspot\", \"Douglas'\\n squirrel\", \"Mule deer\",\n",
    "    \"Black-tipped\\n jackrabbit\", \"Pine marten\", \"Western\\n whiptail\", \"Raven\",\n",
    "    \"Ringtail\", \"Coyote\", \"Mountain\\n lion\", \"Bobcat\"\n",
    "]\n",
    "\n",
    "# Initialize the adjacency matrix with zeros\n",
    "adj_matrix = np.zeros((len(nodes), len(nodes)), dtype=int)\n",
    "\n",
    "# Map the nodes to matrix index\n",
    "index_map = {species: i for i, species in enumerate(nodes)}\n",
    "\n",
    "partition = {\"Producers and Decomposer\": [\"Plants,\\n Flowers,\\n Nuts,\\n Seeds,\\n Fruit,\\n Insects\"],\n",
    "             \"Primary Consumers\": [\"Pika\", \"Red-breasted\\n nuthatch\", \"Pacific\\n tree frog\", \"Edith's\\n checkerspot\",\n",
    "                                   \"Douglas'\\n squirrel\", \"Mule deer\"],\n",
    "             \"Secondary Consumers\": [\"Black-tipped\\n jackrabbit\", \"Pine marten\", \"Western\\n whiptail\", \"Raven\", \"Ringtail\"],\n",
    "             \"Tertiary Consumers\": [\"Coyote\", \"Mountain\\n lion\", \"Bobcat\"]}\n",
    "\n",
    "partition_numbered = [[index_map[species] for species in community] for community in partition.values()]\n",
    "# Manually add edges based on the image (directed edges where energy is transferred)\n",
    "edges = [\n",
    "    (\"Plants,\\n Flowers,\\n Nuts,\\n Seeds,\\n Fruit,\\n Insects\", \"Pika\"),\n",
    "    (\"Plants,\\n Flowers,\\n Nuts,\\n Seeds,\\n Fruit,\\n Insects\", \"Red-breasted\\n nuthatch\"),\n",
    "    (\"Plants,\\n Flowers,\\n Nuts,\\n Seeds,\\n Fruit,\\n Insects\", \"Pacific\\n tree frog\"),\n",
    "    (\"Plants,\\n Flowers,\\n Nuts,\\n Seeds,\\n Fruit,\\n Insects\", \"Edith's\\n checkerspot\"),\n",
    "    (\"Plants,\\n Flowers,\\n Nuts,\\n Seeds,\\n Fruit,\\n Insects\", \"Douglas'\\n squirrel\"),\n",
    "    (\"Plants,\\n Flowers,\\n Nuts,\\n Seeds,\\n Fruit,\\n Insects\", \"Mule deer\"),\n",
    "\n",
    "    (\"Pika\", \"Ringtail\"),\n",
    "    (\"Pika\", \"Western\\n whiptail\"),\n",
    "    (\"Pika\", \"Raven\"),\n",
    "    (\"Pika\", \"Black-tipped\\n jackrabbit\"),\n",
    "    (\"Pika\", \"Pine marten\"),\n",
    "\n",
    "    (\"Red-breasted\\n nuthatch\", \"Western\\n whiptail\"),\n",
    "\n",
    "    (\"Pacific\\n tree frog\", \"Ringtail\"),\n",
    "    (\"Pacific\\n tree frog\", \"Western\\n whiptail\"),\n",
    "    (\"Pacific\\n tree frog\", \"Raven\"),\n",
    "    (\"Pacific\\n tree frog\", \"Black-tipped\\n jackrabbit\"),\n",
    "    (\"Pacific\\n tree frog\", \"Pine marten\"),\n",
    "\n",
    "    (\"Edith's\\n checkerspot\", \"Western\\n whiptail\"),\n",
    "    (\"Edith's\\n checkerspot\", \"Raven\"),\n",
    "    (\"Edith's\\n checkerspot\", \"Black-tipped\\n jackrabbit\"),\n",
    "\n",
    "    (\"Douglas'\\n squirrel\", \"Ringtail\"),\n",
    "    (\"Douglas'\\n squirrel\", \"Raven\"),\n",
    "    (\"Douglas'\\n squirrel\", \"Black-tipped\\n jackrabbit\"),\n",
    "    (\"Douglas'\\n squirrel\", \"Pine marten\"),\n",
    "\n",
    "    (\"Mule deer\", \"Mountain\\n lion\"),\n",
    "    (\"Mule deer\", \"Coyote\"),\n",
    "\n",
    "    (\"Ringtail\", \"Coyote\"),\n",
    "    (\"Ringtail\", \"Mountain\\n lion\"),\n",
    "    (\"Ringtail\", \"Bobcat\"),\n",
    "\n",
    "    (\"Western\\n whiptail\", \"Mountain\\n lion\"),\n",
    "    (\"Western\\n whiptail\", \"Bobcat\"),\n",
    "    (\"Western\\n whiptail\", \"Coyote\"),\n",
    "\n",
    "    (\"Black-tipped\\n jackrabbit\", \"Mountain\\n lion\"),\n",
    "    (\"Black-tipped\\n jackrabbit\", \"Coyote\"),\n",
    "    (\"Black-tipped\\n jackrabbit\", \"Bobcat\"),\n",
    "\n",
    "    (\"Pine marten\", \"Mountain\\n lion\"),\n",
    "    (\"Pine marten\", \"Bobcat\")\n",
    "]\n",
    "\n",
    "# Now we can fill the adjacency matrix again based on these edges\n",
    "for source, target in edges:\n",
    "    adj_matrix[index_map[source], index_map[target]] = 1\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "adj_matrix_df = pd.DataFrame(adj_matrix, index=nodes, columns=nodes)\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges to the graph from the previous trophic relationships\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "pos = {\n",
    "    \"Plants,\\n Flowers,\\n Nuts,\\n Seeds,\\n Fruit,\\n Insects\": (0, 6),\n",
    "\n",
    "    \"Pika\": (-6, 4), \"Red-breasted\\n nuthatch\": (-3, 4), \"Pacific\\n tree frog\": (0, 4), \n",
    "    \"Edith's\\n checkerspot\": (3, 4), \"Douglas'\\n squirrel\": (6, 4), \"Mule deer\": (9, 4),\n",
    "\n",
    "    \"Black-tipped\\n jackrabbit\": (-7, 2), \"Pine marten\": (-4, 2), \n",
    "    \"Western\\n whiptail\": (-1, 2), \"Raven\": (2, 2), \"Ringtail\": (5, 2),\n",
    "\n",
    "    \"Coyote\": (-5, 0), \"Mountain\\n lion\": (-2, 0), \"Bobcat\": (1, 0)\n",
    "}\n",
    "\n",
    "\n",
    "# Color mapping for different communities\n",
    "color_map = {\n",
    "    \"Producers and Decomposer\": \"green\",\n",
    "    \"Primary Consumers\": \"blue\",\n",
    "    \"Secondary Consumers\": \"orange\",\n",
    "    \"Tertiary Consumers\": \"red\"\n",
    "}\n",
    "\n",
    "# Assign colors based on community\n",
    "node_colors = []\n",
    "for node in G.nodes():\n",
    "    for community, members in partition.items():\n",
    "        if node in members:\n",
    "            node_colors.append(color_map[community])\n",
    "\n",
    "# Plot the graph using networkx\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw_networkx(G, pos, with_labels=True, node_size=5000, \n",
    "                 font_size=10, font_color='black', font_weight='bold',node_color='white', edgecolors=node_colors, arrows=True, alpha=0.65)\n",
    "\n",
    "# Display the plot\n",
    "#plt.title('Directed Graph of Yellowstone Trophic Cascade')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('Figures/YellowstoneTrophicCascade2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ad9158b13efdd12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute hermitian adjacency matrix\n",
    "k = 4\n",
    "k_ls = int(np.ceil(2 * np.pi * k))\n",
    "A = adj_matrix * np.exp(1j * 2 * np.pi / 4) + adj_matrix.T * np.exp(-1j * 2 * np.pi / 4)\n",
    "A_ls = adj_matrix * np.exp(1j * 2 * np.pi / k_ls) + adj_matrix.T * np.exp(-1j * 2 * np.pi / k_ls)\n",
    "# compute degree matrix\n",
    "degrees = np.sum(np.abs(A), axis=1)\n",
    "D = np.diag(degrees)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "norm_L = np.eye(len(nodes)) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "\n",
    "#Compute indicator vector\n",
    "ind_vector = np.zeros((len(nodes),), dtype = np.complex128)\n",
    "for i in range(k):\n",
    "    for node in partition[list(partition.keys())[i]]:\n",
    "        ind_vector[index_map[node]] = np.exp(-1j * 2 * np.pi * i / k)\n",
    "ind_vector = D_sqrt @ ind_vector\n",
    "ind_vector = ind_vector / np.linalg.norm(ind_vector)\n",
    "\n",
    "rayleigh_quotient = np.real((np.conj(ind_vector).T @ norm_L @ ind_vector) / (np.conj(ind_vector).T @ ind_vector))\n",
    "\n",
    "#Now computing using Laenen & Sun's choice of roots of unity\n",
    "D_ls = np.sum(np.abs(A_ls), axis=1)\n",
    "D_inv_sqrt_ls = np.diag(1 / np.sqrt(D_ls))\n",
    "norm_L_ls = np.eye(len(nodes)) - D_inv_sqrt_ls @ A_ls @ D_inv_sqrt_ls\n",
    "evals, evecs = np.linalg.eigh(norm_L)\n",
    "evals_ls, evecs_ls = np.linalg.eigh(norm_L_ls)\n",
    "# plot first evec\n",
    "\n",
    "thetas = []\n",
    "for perm in itertools.permutations(range(4)):\n",
    "    C = [(perm[i], perm[i + 1]) for i in range(3)]\n",
    "    theta = compute_theta(partition_numbered, adj_matrix, degrees, C)\n",
    "    thetas.append(theta)\n",
    "\n",
    "theta = max(thetas)\n",
    "max_perm_theta = list(itertools.permutations(range(4)))[np.argmax(thetas)]\n",
    "\n",
    "C = [(0, 1), (1, 2), (2, 3), (3, 0)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f40bd9faa3a81597"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(evecs[:, 0].real, evecs[:, 0].imag, '+', label='standard', alpha=0.5)\n",
    "# add labels\n",
    "for i, txt in enumerate(nodes):\n",
    "    plt.annotate(txt, (evecs[i, 0].real, evecs[i, 0].imag))\n",
    "\n",
    "# colour by community\n",
    "for i, community in enumerate(partition_numbered):\n",
    "    plt.plot(evecs[community, 0].real, evecs[community, 0].imag, 'o', label=f'community {i}', alpha=0.5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21d86c0796b64dbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_val = np.linalg.norm(evecs[:, 0] - (np.conj(ind_vector).T @ evecs[:, 0]) * ind_vector) ** 2\n",
    "theta = compute_theta(partition_numbered, adj_matrix, degrees, [(i, i + 1) for i in range(4)])\n",
    "ls_1, ls_2 = compute_ls_bounds(evals_ls, theta, k)\n",
    "print(\"Theta is \", theta)\n",
    "print(\"1/eta_k is \", ls_1)\n",
    "print(\"1/(eta_k-1) is \", ls_2)\n",
    "psi = Psi(partition_numbered, adj_matrix, degrees, C)\n",
    "new_bound = compute_new_bound(evals, psi)\n",
    "new_bound_with_rayleigh_quotient = (rayleigh_quotient - evals[0]) / (evals[1] - evals[0])\n",
    "print(\"Rayleigh Quotient is \", rayleigh_quotient)\n",
    "print(\"Psi is \", psi)\n",
    "print(\"True value is \", true_val)\n",
    "print(\"New bound is \", new_bound)\n",
    "print(\"New bound with rayleigh quotient is \", new_bound_with_rayleigh_quotient)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3ee29d4a36f5f86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Covid Infection Graph\n",
    "In this section we compute the covid infection graph. Since the graph fits a perfect 4-cycle, we can state exactly what the cycle C should be. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d1a57533f9a4e29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "patient_info_df = pd.read_csv(\"Data/PatientInfo.csv\")\n",
    "patient_info_df.head()\n",
    "edges_df = patient_info_df[[\"infected_by\", \"patient_id\"]].dropna(subset=[\"infected_by\"])\n",
    "edges_df.columns = [\"source\", \"target\"]\n",
    "edges_df = edges_df.astype(str)\n",
    "# create adjacency matrix\n",
    "index = list(set(edges_df[\"source\"].values) | set(edges_df[\"target\"].values))\n",
    "index.sort()\n",
    "adjacency_matrix = np.zeros((len(index), len(index)))\n",
    "for i, row in edges_df.iterrows():\n",
    "    source = row[\"source\"]\n",
    "    target = row[\"target\"]\n",
    "    source_index = index.index(source)\n",
    "    target_index = index.index(target)\n",
    "    adjacency_matrix[source_index, target_index] = 1\n",
    "adjacency_matrix_df = pd.DataFrame(adjacency_matrix, index=index, columns=index)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bba3e16d0d322b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def largest_connected_component_adjacency(df):\n",
    "    # Convert the DataFrame to a NumPy array\n",
    "    adj_matrix = df\n",
    "\n",
    "    # Create a directed graph from the adjacency matrix\n",
    "    G = nx.from_pandas_adjacency(adj_matrix, create_using=nx.DiGraph)\n",
    "\n",
    "    # Find the largest strongly connected component\n",
    "    largest_wcc = max(nx.weakly_connected_components(G), key=len)\n",
    "\n",
    "    # Create a subgraph from the largest SCC\n",
    "    largest_wcc_subgraph = G.subgraph(largest_wcc)\n",
    "\n",
    "    # Get the adjacency matrix of the largest SCC subgraph\n",
    "    largest_wcc_adj_matrix = nx.to_numpy_array(largest_wcc_subgraph, nodelist=sorted(largest_wcc))\n",
    "\n",
    "    # Create a new DataFrame for the adjacency matrix of the largest SCC\n",
    "    largest_wcc_df = pd.DataFrame(largest_wcc_adj_matrix,\n",
    "                                  index=sorted(largest_wcc),\n",
    "                                  columns=sorted(largest_wcc))\n",
    "\n",
    "    return largest_wcc_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a79498eb09ea844"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "largest_wcc_df = largest_connected_component_adjacency(adjacency_matrix_df)\n",
    "# draw graph\n",
    "G = nx.from_pandas_adjacency(largest_wcc_df, create_using=nx.DiGraph)\n",
    "\n",
    "k = 4\n",
    "herm_A = np.exp(1j * 2 * np.pi / k) * largest_wcc_df.values + np.exp(-1j * 2 * np.pi / k) * largest_wcc_df.values.T\n",
    "degrees = np.sum(np.abs(herm_A), axis=1)\n",
    "D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "norm_L = np.eye(len(herm_A)) - D_inv_sqrt @ herm_A @ D_inv_sqrt\n",
    "\n",
    "k_ls = int(np.ceil(2 * np.pi * k))\n",
    "herm_A_ls = np.exp(1j * 2 * np.pi / k_ls) * largest_wcc_df.values + np.exp(\n",
    "    -1j * 2 * np.pi / k_ls) * largest_wcc_df.values.T\n",
    "degrees_ls = np.sum(np.abs(herm_A_ls), axis=1)\n",
    "D_inv_sqrt_ls = np.diag(1 / np.sqrt(degrees_ls))\n",
    "norm_L_ls = np.eye(len(herm_A_ls)) - D_inv_sqrt_ls @ herm_A_ls @ D_inv_sqrt_ls\n",
    "\n",
    "# Compute Eigenvectors and Eigenvalues\n",
    "e_vals, e_vecs = np.linalg.eigh(norm_L)\n",
    "e_vals_ls, e_vecs_ls = np.linalg.eigh(norm_L_ls)\n",
    "\n",
    "\n",
    "datapoints = np.column_stack((D_inv_sqrt @ e_vecs[:, 0].real, D_inv_sqrt @ e_vecs[:, 0].imag))\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(datapoints)\n",
    "partition_numbered = [np.where(kmeans.labels_ == i)[0] for i in range(4)]\n",
    "\n",
    "color_map = []\n",
    "for i, node in enumerate(largest_wcc_df.index):\n",
    "    if i in partition_numbered[0]:\n",
    "        color_map.append('blue')\n",
    "    elif i in partition_numbered[1]:\n",
    "        color_map.append('red')\n",
    "    elif i in partition_numbered[2]:\n",
    "        color_map.append('green')\n",
    "    elif i in partition_numbered[3]:\n",
    "        color_map.append('yellow')\n",
    "    else:\n",
    "        color_map.append('black')\n",
    "G = nx.from_pandas_adjacency(largest_wcc_df, create_using=nx.DiGraph)\n",
    "pos = {}\n",
    "partition_sizes = [len(partition) for partition in partition_numbered]\n",
    "scale = 60\n",
    "for i, node_number in enumerate(largest_wcc_df.index):\n",
    "    if i in partition_numbered[0]:\n",
    "        index_in_partition = np.where(partition_numbered[0] == i)[0][0]\n",
    "        pos[node_number] = (10 + scale * index_in_partition / partition_sizes[0], 0)\n",
    "    elif i in partition_numbered[1]:\n",
    "        index_in_partition = np.where(partition_numbered[1] == i)[0][0]\n",
    "        pos[node_number] = (0, 10 + scale * index_in_partition / partition_sizes[0])\n",
    "    elif i in partition_numbered[2]:\n",
    "        index_in_partition = np.where(partition_numbered[2] == i)[0][0]\n",
    "        pos[node_number] = (0, -10 + scale * index_in_partition / partition_sizes[0])\n",
    "    elif i in partition_numbered[3]:\n",
    "        index_in_partition = np.where(partition_numbered[3] == i)[0][0]\n",
    "        pos[node_number] = (-10 + scale * index_in_partition / partition_sizes[0], 0)\n",
    "    else:\n",
    "        pos[node_number] = (0, 0)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "nx.draw(G, pos, node_color=color_map, with_labels=False, node_size=15, width=0.5, connectionstyle='arc3, rad = 0.1')\n",
    "#plt.title(\"Partitioning of Largest Connected Component of DS4C Infection Network\")\n",
    "plt.savefig('Figures/DS4CInfectionNetwork.png', bbox_inches='tight')\n",
    "# compute weight between every pair of clusters\n",
    "\n",
    "C = [(2, 0), (0, 1), (1, 3), (3, 2)]\n",
    "\n",
    "theta = compute_theta(partition_numbered, largest_wcc_df.values, degrees, C)\n",
    "compute_ls_bounds(e_vals_ls, theta, k)\n",
    "eta_k = e_vals_ls[1] / (1 - (4 / k) * theta)\n",
    "\n",
    "print(\"Number of vertices: \", len(largest_wcc_df))\n",
    "print(\"Number of Edges: \", np.sum(np.sum(largest_wcc_df, axis=1)))\n",
    "\n",
    "print(\"Theta\", theta)\n",
    "print(\"eta_k\", e_vals_ls[1] / (1 - (4 / k) * theta))\n",
    "print(\"1/eta_k\", 1/eta_k)\n",
    "print(\"1/(eta_k-1)\", 1/(eta_k - 1))\n",
    "\n",
    "print(\"Psi\", Psi(partition_numbered, largest_wcc_df.values, degrees, C))\n",
    "print(\"New bound\", compute_new_bound(e_vals, Psi(partition_numbered, largest_wcc_df.values, degrees, C)))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e314293de5cf1797"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some Other Real-World Directed Graphs\n",
    "We now consider some real world directed graphs (ecological networks) from the cosinproject. We start with the Ythan Estuary dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a04d692d392c608e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# import edge list from http://cosinproject.eu/extra/data/foodwebs/ythan.txt\n",
    "\n",
    "urllib.request.urlretrieve(\"http://cosinproject.eu/extra/data/foodwebs/ythan.txt\", \"Data/ythan.txt\")\n",
    "# construct adjacency matrix\n",
    "with open(\"Data/ythan.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    edges = [tuple(map(int, line.strip().split())) for line in lines]\n",
    "\n",
    "edges_df = pd.DataFrame(edges, columns=[\"source\", \"target\"])\n",
    "edges_df_unweighted = edges_df.applymap(int)\n",
    "edges_df_unweighted.head(10)\n",
    "# convert edges_df_unweighted to adjacency matrix\n",
    "index = list(set(edges_df_unweighted[\"source\"].values) | set(edges_df_unweighted[\"target\"].values))\n",
    "index.sort()\n",
    "adj_matrix = np.zeros((len(index), len(index)))\n",
    "for i, row in edges_df_unweighted.iterrows():\n",
    "    source = row[\"source\"]\n",
    "    target = row[\"target\"]\n",
    "    source_index = index.index(source)\n",
    "    target_index = index.index(target)\n",
    "    adj_matrix[source_index, target_index] = 1\n",
    "\n",
    "k = 4\n",
    "k_ls = int(np.ceil(2 * np.pi * k))\n",
    "herm_A = np.exp(1j * 2 * np.pi / k) * adj_matrix + np.exp(-1j * 2 * np.pi / k) * adj_matrix.T\n",
    "herm_A_ls = np.exp(1j * 2 * np.pi / k_ls) * adj_matrix + np.exp(-1j * 2 * np.pi / k_ls) * adj_matrix.T\n",
    "degrees = np.sum(np.abs(herm_A), axis=1)\n",
    "D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "norm_L = np.eye(len(adj_matrix)) - D_inv_sqrt @ herm_A @ D_inv_sqrt\n",
    "norm_L_ls = np.eye(len(adj_matrix)) - D_inv_sqrt @ herm_A_ls @ D_inv_sqrt\n",
    "\n",
    "eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "idx = eigvals.argsort()\n",
    "eigvals = eigvals[idx]\n",
    "eigvecs = eigvecs[:, idx]\n",
    "\n",
    "eigvals_ls, eigvecs_ls = np.linalg.eigh(norm_L_ls)\n",
    "idx = eigvals_ls.argsort()\n",
    "eigvals_ls = eigvals_ls[idx]\n",
    "eigvecs_ls = eigvecs_ls[:, idx]\n",
    "\n",
    "# cluster using first eigenvector\n",
    "datapoints = np.column_stack((D_inv_sqrt @ eigvecs[:, 0].real, D_inv_sqrt @ eigvecs[:, 0].imag))\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(datapoints)\n",
    "partition_numbered = [np.where(kmeans.labels_ == i)[0] for i in range(k)]\n",
    "\n",
    "rayleigh_quotients_all = []\n",
    "for perm in itertools.permutations(range(k)):\n",
    "    # construct indicator vector of clusters\n",
    "    ind_vector = np.zeros((len(adj_matrix),), dtype=np.complex128)\n",
    "\n",
    "    for i, cluster in enumerate(partition_numbered):\n",
    "        ind_vector[cluster] = np.exp(2 * np.pi * 1j * perm[i] / k)\n",
    "\n",
    "    ind_vector = D_sqrt @ ind_vector\n",
    "    ind_vector = ind_vector / np.linalg.norm(ind_vector)\n",
    "\n",
    "    # rotate indicator vector to align with first eigenvector\n",
    "    alpha = np.conj(ind_vector).T @ eigvecs[:, 0]\n",
    "    ind_vector = alpha * ind_vector\n",
    "    ind_vector = ind_vector / np.linalg.norm(ind_vector)\n",
    "\n",
    "    rayleigh_quotients_all.append(np.real(np.conj(ind_vector).T @ norm_L @ ind_vector))\n",
    "\n",
    "\n",
    "# set indicator vector to be the one with the minimum rayleigh quotient\n",
    "min_perm = list(itertools.permutations(range(k)))[rayleigh_quotients_all.index(min(rayleigh_quotients_all))]\n",
    "min_ind_vector = np.zeros((len(adj_matrix),), dtype=np.complex128)\n",
    "\n",
    "for i, cluster in enumerate(partition_numbered):\n",
    "    min_ind_vector[cluster] = np.exp(2 * np.pi * 1j * min_perm[i] / k)\n",
    "\n",
    "min_ind_vector = D_sqrt @ min_ind_vector\n",
    "min_ind_vector = min_ind_vector / np.linalg.norm(min_ind_vector)\n",
    "\n",
    "# compute Psi\n",
    "Psis = []\n",
    "for perm in itertools.permutations(range(k)):\n",
    "    C = [(perm[i], perm[i + 1]) for i in range(k - 1)] + [(perm[k - 1], perm[0])]\n",
    "    Psis.append(Psi(partition_numbered, adj_matrix, degrees, C))\n",
    "\n",
    "min_perm_psi = list(itertools.permutations(range(k)))[Psis.index(min(Psis))]\n",
    "\n",
    "# rotate indicator vector to align with first eigenvector\n",
    "alpha = np.conj(min_ind_vector).T @ eigvecs[:, 0]\n",
    "min_ind_vector = alpha * min_ind_vector\n",
    "\n",
    "thetas = []\n",
    "\n",
    "for perm in itertools.permutations(range(k)):\n",
    "    C = [(perm[i], perm[i + 1]) for i in range(len(perm) - 1)]\n",
    "    C.append((perm[-1], perm[0]))\n",
    "    theta = compute_theta(partition_numbered, adj_matrix, degrees, C)\n",
    "    thetas.append(theta)\n",
    "    \n",
    "max_perm_theta = list(itertools.permutations(range(k)))[thetas.index(max(thetas))]\n",
    "print(\"Max perm theta \", max_perm_theta)\n",
    "C = [(max_perm_theta[i], max_perm_theta[i + 1]) for i in range(len(max_perm_theta) - 1)]\n",
    "theta = compute_theta(partition_numbered, adj_matrix, degrees, C)\n",
    "\n",
    "compute_ls_bounds(eigvals_ls, theta, k)\n",
    "eigvals_ls[1] / (1 - (4 / k) * theta)\n",
    "eta = eigvals_ls[1] / (1 - (4 / k) * theta)\n",
    "\n",
    "print(\"Number of vertices: \", len(adj_matrix))\n",
    "print(\"Number of Edges: \", np.sum(np.sum(adj_matrix, axis=1)))\n",
    "print(\"Theta\", theta)\n",
    "print(\"1/eta_k\", 1 / eta)\n",
    "print(\"1/(eta_k-1)\", 1 / (eta - 1))\n",
    "print(\"Psi\", min(Psis))\n",
    "print(\"rayleigh quotient\", min(rayleigh_quotients_all))\n",
    "print(\"Our bound (Rayleigh Quotient) \",(min(rayleigh_quotients_all) - eigvals[0]) / (eigvals[1] - eigvals[0]))\n",
    "print(\"Our bound (Psi) \", (4*min(Psis) - eigvals[0])/ (eigvals[1] - eigvals[0]))\n",
    "print(\"True value \", np.linalg.norm(min_ind_vector - eigvecs[:, 0]) ** 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ece2701971a23ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## St.Martins Island\n",
    "using k = 4."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "935f63a6195dae82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "urllib.request.urlretrieve(\"http://cosinproject.eu/extra/data/foodwebs/stmartin.txt\", \"Data/stmartin.txt\")\n",
    "# construct adjacency matrix\n",
    "with open(\"Data/stmartin.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    edges = [tuple(map(int, line.strip().split())) for line in lines]\n",
    "\n",
    "edges_df = pd.DataFrame(edges, columns=[\"source\", \"target\"])\n",
    "edges_df_unweighted = edges_df.applymap(int)\n",
    "edges_df_unweighted.head(10)\n",
    "# convert edges_df_unweighted to adjacency matrix\n",
    "index = list(set(edges_df_unweighted[\"source\"].values) | set(edges_df_unweighted[\"target\"].values))\n",
    "index.sort()\n",
    "adj_matrix = np.zeros((len(index), len(index)))\n",
    "for i, row in edges_df_unweighted.iterrows():\n",
    "    source = row[\"source\"]\n",
    "    target = row[\"target\"]\n",
    "    source_index = index.index(source)\n",
    "    target_index = index.index(target)\n",
    "    adj_matrix[source_index, target_index] = 1\n",
    "\n",
    "k = 4\n",
    "k_ls = int(np.ceil(2 * np.pi * k))\n",
    "herm_A = np.exp(1j * 2 * np.pi / k) * adj_matrix + np.exp(-1j * 2 * np.pi / k) * adj_matrix.T\n",
    "herm_A_ls = np.exp(1j * 2 * np.pi / k_ls) * adj_matrix + np.exp(-1j * 2 * np.pi / k_ls) * adj_matrix.T\n",
    "degrees = np.sum(np.abs(herm_A), axis=1)\n",
    "D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "norm_L = np.eye(len(adj_matrix)) - D_inv_sqrt @ herm_A @ D_inv_sqrt\n",
    "norm_L_ls = np.eye(len(adj_matrix)) - D_inv_sqrt @ herm_A_ls @ D_inv_sqrt\n",
    "\n",
    "eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "idx = eigvals.argsort()\n",
    "eigvals = eigvals[idx]\n",
    "eigvecs = eigvecs[:, idx]\n",
    "\n",
    "eigvals_ls, eigvecs_ls = np.linalg.eigh(norm_L_ls)\n",
    "idx = eigvals_ls.argsort()\n",
    "eigvals_ls = eigvals_ls[idx]\n",
    "eigvecs_ls = eigvecs_ls[:, idx]\n",
    "\n",
    "# cluster using first eigenvector\n",
    "datapoints = np.column_stack((D_inv_sqrt @ eigvecs[:, 0].real, D_inv_sqrt @ eigvecs[:, 0].imag))\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(datapoints)\n",
    "partition_numbered = [np.where(kmeans.labels_ == i)[0] for i in range(k)]\n",
    "\n",
    "rayleigh_quotients_all = []\n",
    "for perm in itertools.permutations(range(k)):\n",
    "    # construct indicator vector of clusters\n",
    "    ind_vector = np.zeros((len(adj_matrix),), dtype=np.complex128)\n",
    "\n",
    "    for i, cluster in enumerate(partition_numbered):\n",
    "        ind_vector[cluster] = np.exp(2 * np.pi * 1j * perm[i] / k)\n",
    "\n",
    "    ind_vector = D_sqrt @ ind_vector\n",
    "    ind_vector = ind_vector / np.linalg.norm(ind_vector)\n",
    "\n",
    "    # rotate indicator vector to align with first eigenvector\n",
    "    alpha = np.conj(ind_vector).T @ eigvecs[:, 0]\n",
    "    ind_vector = alpha * ind_vector\n",
    "    ind_vector = ind_vector / np.linalg.norm(ind_vector)\n",
    "\n",
    "    rayleigh_quotients_all.append(np.real(np.conj(ind_vector).T @ norm_L @ ind_vector))\n",
    "\n",
    "\n",
    "# set indicator vector to be the one with the minimum rayleigh quotient\n",
    "min_perm = list(itertools.permutations(range(k)))[rayleigh_quotients_all.index(min(rayleigh_quotients_all))]\n",
    "min_ind_vector = np.zeros((len(adj_matrix),), dtype=np.complex128)\n",
    "\n",
    "for i, cluster in enumerate(partition_numbered):\n",
    "    min_ind_vector[cluster] = np.exp(2 * np.pi * 1j * min_perm[i] / k)\n",
    "\n",
    "min_ind_vector = D_sqrt @ min_ind_vector\n",
    "min_ind_vector = min_ind_vector / np.linalg.norm(min_ind_vector)\n",
    "\n",
    "# compute Psi\n",
    "Psis = []\n",
    "for perm in itertools.permutations(range(k)):\n",
    "    C = [(perm[i], perm[i + 1]) for i in range(k - 1)] + [(perm[k - 1], perm[0])]\n",
    "    Psis.append(Psi(partition_numbered, adj_matrix, degrees, C))\n",
    "\n",
    "min_perm_psi = list(itertools.permutations(range(k)))[Psis.index(min(Psis))]\n",
    "\n",
    "# rotate indicator vector to align with first eigenvector\n",
    "alpha = np.conj(min_ind_vector).T @ eigvecs[:, 0]\n",
    "min_ind_vector = alpha * min_ind_vector\n",
    "\n",
    "thetas = []\n",
    "\n",
    "for perm in itertools.permutations(range(k)):\n",
    "    C = [(perm[i], perm[i + 1]) for i in range(len(perm) - 1)]\n",
    "    C.append((perm[-1], perm[0]))\n",
    "    theta = compute_theta(partition_numbered, adj_matrix, degrees, C)\n",
    "    thetas.append(theta)\n",
    "    \n",
    "max_perm_theta = list(itertools.permutations(range(k)))[thetas.index(max(thetas))]\n",
    "print(\"Max perm theta \", max_perm_theta)\n",
    "C = [(max_perm_theta[i], max_perm_theta[i + 1]) for i in range(len(max_perm_theta) - 1)]\n",
    "theta = compute_theta(partition_numbered, adj_matrix, degrees, C)\n",
    "\n",
    "compute_ls_bounds(eigvals_ls, theta, k)\n",
    "eigvals_ls[1] / (1 - (4 / k) * theta)\n",
    "eta = eigvals_ls[1] / (1 - (4 / k) * theta)\n",
    "\n",
    "print(\"Number of vertices: \", len(adj_matrix))\n",
    "print(\"Number of Edges: \", np.sum(np.sum(adj_matrix, axis=1)))\n",
    "print(\"Theta\", theta)\n",
    "print(\"1/eta_k\", 1 / eta)\n",
    "print(\"1/(eta_k-1)\", 1 / (eta - 1))\n",
    "print(\"Psi\", min(Psis))\n",
    "print(\"rayleigh quotient\", min(rayleigh_quotients_all))\n",
    "print(\"Our bound (Rayleigh Quotient) \",(min(rayleigh_quotients_all) - eigvals[0]) / (eigvals[1] - eigvals[0]))\n",
    "print(\"Our bound (Psi) \", (4*min(Psis) - eigvals[0])/ (eigvals[1] - eigvals[0]))\n",
    "print(\"True value \", np.linalg.norm(min_ind_vector - eigvecs[:, 0]) ** 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23691783ffc4a63"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## St. Marks Seagrass\n",
    "Using k = 5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79239f351364ae54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "urllib.request.urlretrieve(\"http://cosinproject.eu/extra/data/foodwebs/seagrass.txt\", \"Data/seagrass.txt\")\n",
    "# construct adjacency matrix\n",
    "with open(\"Data/seagrass.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    edges = [tuple(map(int, line.strip().split())) for line in lines]\n",
    "\n",
    "edges_df = pd.DataFrame(edges, columns=[\"source\", \"target\"])\n",
    "edges_df_unweighted = edges_df.applymap(int)\n",
    "edges_df_unweighted.head(10)\n",
    "# convert edges_df_unweighted to adjacency matrix\n",
    "index = list(set(edges_df_unweighted[\"source\"].values) | set(edges_df_unweighted[\"target\"].values))\n",
    "index.sort()\n",
    "adj_matrix = np.zeros((len(index), len(index)))\n",
    "for i, row in edges_df_unweighted.iterrows():\n",
    "    source = row[\"source\"]\n",
    "    target = row[\"target\"]\n",
    "    source_index = index.index(source)\n",
    "    target_index = index.index(target)\n",
    "    adj_matrix[source_index, target_index] = 1\n",
    "\n",
    "k = 5\n",
    "k_ls = int(np.ceil(2 * np.pi * k))\n",
    "herm_A = np.exp(1j * 2 * np.pi / k) * adj_matrix + np.exp(-1j * 2 * np.pi / k) * adj_matrix.T\n",
    "herm_A_ls = np.exp(1j * 2 * np.pi / k_ls) * adj_matrix + np.exp(-1j * 2 * np.pi / k_ls) * adj_matrix.T\n",
    "degrees = np.sum(np.abs(herm_A), axis=1)\n",
    "D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "norm_L = np.eye(len(adj_matrix)) - D_inv_sqrt @ herm_A @ D_inv_sqrt\n",
    "norm_L_ls = np.eye(len(adj_matrix)) - D_inv_sqrt @ herm_A_ls @ D_inv_sqrt\n",
    "\n",
    "eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "idx = eigvals.argsort()\n",
    "eigvals = eigvals[idx]\n",
    "eigvecs = eigvecs[:, idx]\n",
    "\n",
    "eigvals_ls, eigvecs_ls = np.linalg.eigh(norm_L_ls)\n",
    "idx = eigvals_ls.argsort()\n",
    "eigvals_ls = eigvals_ls[idx]\n",
    "eigvecs_ls = eigvecs_ls[:, idx]\n",
    "\n",
    "# cluster using first eigenvector\n",
    "datapoints = np.column_stack((D_inv_sqrt @ eigvecs[:, 0].real, D_inv_sqrt @ eigvecs[:, 0].imag))\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(datapoints)\n",
    "partition_numbered = [np.where(kmeans.labels_ == i)[0] for i in range(k)]\n",
    "\n",
    "rayleigh_quotients_all = []\n",
    "for perm in itertools.permutations(range(k)):\n",
    "    # construct indicator vector of clusters\n",
    "    ind_vector = np.zeros((len(adj_matrix),), dtype=np.complex128)\n",
    "\n",
    "    for i, cluster in enumerate(partition_numbered):\n",
    "        ind_vector[cluster] = np.exp(2 * np.pi * 1j * perm[i] / k)\n",
    "\n",
    "    ind_vector = D_sqrt @ ind_vector\n",
    "    ind_vector = ind_vector / np.linalg.norm(ind_vector)\n",
    "\n",
    "    # rotate indicator vector to align with first eigenvector\n",
    "    alpha = np.conj(ind_vector).T @ eigvecs[:, 0]\n",
    "    ind_vector = alpha * ind_vector\n",
    "    ind_vector = ind_vector / np.linalg.norm(ind_vector)\n",
    "\n",
    "    rayleigh_quotients_all.append(np.real(np.conj(ind_vector).T @ norm_L @ ind_vector))\n",
    "\n",
    "\n",
    "# set indicator vector to be the one with the minimum rayleigh quotient\n",
    "min_perm = list(itertools.permutations(range(k)))[rayleigh_quotients_all.index(min(rayleigh_quotients_all))]\n",
    "min_ind_vector = np.zeros((len(adj_matrix),), dtype=np.complex128)\n",
    "\n",
    "for i, cluster in enumerate(partition_numbered):\n",
    "    min_ind_vector[cluster] = np.exp(2 * np.pi * 1j * min_perm[i] / k)\n",
    "\n",
    "min_ind_vector = D_sqrt @ min_ind_vector\n",
    "min_ind_vector = min_ind_vector / np.linalg.norm(min_ind_vector)\n",
    "\n",
    "# compute Psi\n",
    "Psis = []\n",
    "for perm in itertools.permutations(range(k)):\n",
    "    C = [(perm[i], perm[i + 1]) for i in range(k - 1)] + [(perm[k - 1], perm[0])]\n",
    "    Psis.append(Psi(partition_numbered, adj_matrix, degrees, C))\n",
    "\n",
    "min_perm_psi = list(itertools.permutations(range(k)))[Psis.index(min(Psis))]\n",
    "\n",
    "# rotate indicator vector to align with first eigenvector\n",
    "alpha = np.conj(min_ind_vector).T @ eigvecs[:, 0]\n",
    "min_ind_vector = alpha * min_ind_vector\n",
    "\n",
    "thetas = []\n",
    "\n",
    "for perm in itertools.permutations(range(k)):\n",
    "    C = [(perm[i], perm[i + 1]) for i in range(len(perm) - 1)]\n",
    "    C.append((perm[-1], perm[0]))\n",
    "    theta = compute_theta(partition_numbered, adj_matrix, degrees, C)\n",
    "    thetas.append(theta)\n",
    "    \n",
    "max_perm_theta = list(itertools.permutations(range(k)))[thetas.index(max(thetas))]\n",
    "print(\"Max perm theta \", max_perm_theta)\n",
    "C = [(max_perm_theta[i], max_perm_theta[i + 1]) for i in range(len(max_perm_theta) - 1)]\n",
    "theta = compute_theta(partition_numbered, adj_matrix, degrees, C)\n",
    "\n",
    "compute_ls_bounds(eigvals_ls, theta, k)\n",
    "eigvals_ls[1] / (1 - (4 / k) * theta)\n",
    "eta = eigvals_ls[1] / (1 - (4 / k) * theta)\n",
    "print(\"Number of vertices: \", len(adj_matrix))\n",
    "print(\"Number of Edges: \", np.sum(np.sum(adj_matrix, axis=1)))\n",
    "print(\"Theta\", theta)\n",
    "print(\"1/eta_k\", 1 / eta)\n",
    "print(\"1/(eta_k-1)\", 1 / (eta - 1))\n",
    "print(\"Psi\", min(Psis))\n",
    "print(\"rayleigh quotient\", min(rayleigh_quotients_all))\n",
    "print(\"Our bound (Rayleigh Quotient) \",(min(rayleigh_quotients_all) - eigvals[0]) / (eigvals[1] - eigvals[0]))\n",
    "print(\"Our bound (Psi) \", (4*min(Psis) - eigvals[0])/ (eigvals[1] - eigvals[0]))\n",
    "print(\"True value \", np.linalg.norm(min_ind_vector - eigvecs[:, 0]) ** 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c1f8e72d60e3862"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c47da5d8776bc67c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
