{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports and Settings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67c950ce93f96d12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse.linalg\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_openml\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from graph import Graph\n",
    "from helpers import get_hermitian_adjacency_matrix, get_adjacency_matrix, get_degree_matrix, get_laplacian_matrix, compute_k_way_estimate, get_normalised_projected_indicator_vectors, degree_correction, k_means_indicator_vectors, compute_rayleigh_quotients, apply_recursive_st_brute_force, gram_schmidt, get_thresholded_correlation_matrix, largest_connected_component, get_normalised_laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make text larger\n",
    "plt.rc('font', size=20)\n",
    "# set random seed\n",
    "np.random.seed(9)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7dbe90e1495360e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating Graphs From Gaussians\n",
    "We create a plot of 4 gaussian clusters with 100 nodes each."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6b9dc119608b300"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X1 = np.random.multivariate_normal(mean = [0,0], cov = [[1,0.5],[0.5,1]], size = 100)\n",
    "X2 = np.random.multivariate_normal(mean = [0,5], cov = [[1,-0.5],[-0.5,1]], size = 100)\n",
    "X3 = np.random.multivariate_normal(mean = [8,0], cov = [[1.5,0],[0,1]], size = 100)\n",
    "X4 = np.random.multivariate_normal(mean = [7,5], cov = [[1,0.2],[0.2,1]], size = 100)\n",
    "X = np.concatenate((X1, X2, X3, X4))\n",
    "# plot with each cluster in a different color\n",
    "plt.scatter(X1[:,0], X1[:,1], color = 'r')\n",
    "plt.scatter(X2[:,0], X2[:,1], color = 'b')\n",
    "plt.scatter(X3[:,0], X3[:,1], color = 'g')\n",
    "plt.scatter(X4[:,0], X4[:,1], color = 'y')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "#plt.title('4 Gaussian Clusters')\n",
    "plt.savefig('Figures/4GaussianClusters.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29eb7b4f4fac44f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# construct a graph from the data points using a threshold\n",
    "def construct_graph(X, threshold):\n",
    "    N = X.shape[0]\n",
    "    A = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i != j:\n",
    "                dist = np.linalg.norm(X[i] - X[j])\n",
    "                if dist < threshold:\n",
    "                    A[i, j] = 1\n",
    "    return A\n",
    "\n",
    "A = construct_graph(X, 4)\n",
    "plt.figure()\n",
    "G = nx.from_numpy_array(A)\n",
    "pos = X\n",
    "# make edges thin and transparent. \n",
    "colormap = ['r'] * 100 + ['b'] * 100 + ['g'] * 100 + ['y'] * 100\n",
    "nx.draw(G, pos = pos, node_size = 10, edge_color = 'grey', width = 0.2, node_color = colormap)\n",
    "#plt.title('Graph Constructed from 4 Gaussian Clusters \\n Threshold = 4')\n",
    "plt.savefig('Figures/4GaussianClustersGraphThreshold4.png', bbox_inches = 'tight')\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b10431b7d978d08c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the normalized laplacian\n",
    "\n",
    "degrees = np.sum(A, axis = 1)\n",
    "D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "norm_L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "idx = np.argsort(eigvals)\n",
    "eigvals = eigvals[idx]\n",
    "eigvecs = eigvecs[:, idx]\n",
    "\n",
    "# plot first 10 eigenvalues\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "plt.grid(True)\n",
    "plt.scatter(range(1,9), eigvals[:8])\n",
    "plt.xlabel('Index')\n",
    "plt.xlim(1, 8.6)\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "plt.yticks(np.arange(0, 1.1, 0.1), minor=True)\n",
    "#plt.title(r\"First 8 Eigenvalues of $\\mathcal{L}$\")\n",
    "for i in range(8):\n",
    "    plt.text(i + 1, eigvals[i], f'{eigvals[i]:.2f}', fontsize=10)\n",
    "plt.savefig('Figures/4GaussianClusters8Eigenvalues.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c72c0874363de8a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bounds for Geometric Graphs\n",
    "We take 4 multivariate normally distributed random variables $X_1, X_2,X_3$ and $X_4$ with following distributions:\n",
    "$X_1 \\sim \\mathcal{N}((0,0), I)$,\n",
    "$X_2 \\sim \\mathcal{N}((0,5), I)$,\n",
    "$X_3 \\sim \\mathcal{N}((8,0), I)$,\n",
    "$X_4 \\sim \\mathcal{N}((8,5), I)$\n",
    "We study our bounds for varied number of samples of each distribution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a32e8bbcac3ac2a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rst_bounds = {}\n",
    "st_bounds = {}\n",
    "k_way_bounds = {}\n",
    "true_values = {}\n",
    "n_clusters = 4\n",
    "sample_size = 10\n",
    "for n in np.arange(200,1000,100):\n",
    "    rst_bounds_temp = []\n",
    "    st_bounds_temp = []\n",
    "    k_way_bounds_temp = []\n",
    "    true_values_temp = []\n",
    "    print(f\"commencing computation for n = {n}\")\n",
    "    for p in range(sample_size):\n",
    "    \n",
    "        X1 = np.random.multivariate_normal(mean = [0,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X2 = np.random.multivariate_normal(mean = [0,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X3 = np.random.multivariate_normal(mean = [8,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X4 = np.random.multivariate_normal(mean = [8,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X = np.concatenate((X1, X2, X3, X4))\n",
    "        \n",
    "        A = construct_graph(X, 4)\n",
    "        degrees = np.sum(A, axis = 1)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "        norm_L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        labels = kmeans.fit_predict(eigvecs[:, 0:n_clusters])\n",
    "        \n",
    "        #compute exact indicator vectors (from clusters from distributions not k means)\n",
    "        exact_indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[i*n:(i+1)*n,i] = 1\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[:, i] = D_sqrt @ exact_indicator_vectors[:, i]\n",
    "            exact_indicator_vectors[:, i] = exact_indicator_vectors[:, i] / np.linalg.norm(exact_indicator_vectors[:, i])\n",
    "        \n",
    "        true_val = np.linalg.norm(eigvecs[:,:n_clusters] - exact_indicator_vectors @ (exact_indicator_vectors.T @ eigvecs[:,:n_clusters]), ord = 'fro')**2\n",
    "        \n",
    "        # indicator vectors from k means\n",
    "        indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = (labels == i).astype(int)\n",
    "        \n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = D_sqrt @ indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i] / np.linalg.norm(indicator_vectors[:, i])\n",
    "        \n",
    "        # project indicator vectors onto the eigenvectors\n",
    "        beta_K_by_K = (indicator_vectors.T @ eigvecs[:,:n_clusters])\n",
    "        \n",
    "        combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "                combined_indicator_vectors[:, i])\n",
    "            for j in range(i):\n",
    "                combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                            combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                                 i]) * combined_indicator_vectors[:, j]\n",
    "            \n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(combined_indicator_vectors[:, i])\n",
    "            \n",
    "            \n",
    "        # compute the rayleigh quotients\n",
    "        rayleigh_quotients = []\n",
    "        for i in range(n_clusters):\n",
    "            indicator = combined_indicator_vectors[:, i]\n",
    "            val = (indicator.T @ norm_L @ indicator) / (indicator.T @ indicator)\n",
    "            rayleigh_quotients.append(val)\n",
    "        \n",
    "        # sort the rayleigh quotients\n",
    "        sorted_rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        B_1 = (sorted_rayleigh_quotients[0] + sorted_rayleigh_quotients[1])/eigvals[2]\n",
    "        B_2 = ((sorted_rayleigh_quotients[2] + sorted_rayleigh_quotients[3]) - 2*eigvals[2] + eigvals[4]*B_1)/(eigvals[4] - eigvals[2])\n",
    "        \n",
    "        rst_bound = B_1 + B_2\n",
    "        st_bound = (np.sum(sorted_rayleigh_quotients))/(eigvals[4])\n",
    "        rho = compute_k_way_estimate(norm_L, indicator_vectors, 4)\n",
    "        k_way_bound = 4*rho/eigvals[4]\n",
    "        \n",
    "        rst_bounds_temp.append(rst_bound)\n",
    "        st_bounds_temp.append(st_bound)\n",
    "        k_way_bounds_temp.append(k_way_bound)\n",
    "        true_values_temp.append(true_val)\n",
    "    \n",
    "    rst_bounds[n] = np.mean(rst_bounds_temp)\n",
    "    st_bounds[n] = np.mean(st_bounds_temp)\n",
    "    k_way_bounds[n] = np.mean(k_way_bounds_temp)\n",
    "    true_values[n] = np.mean(true_values_temp)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc6914d0fcd75086"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds_df = pd.DataFrame([rst_bounds, st_bounds, k_way_bounds, true_values]).T\n",
    "bounds_df.columns = [r'Theorem 4',r'Theorem 1', r'MacGregor & Sun', r'True Value']\n",
    "bounds_df.to_csv(\"Review-Data/4GeometricClusters2Pairs.csv\")\n",
    "\n",
    "df_copy = pd.read_csv(\"Review-Data/4GeometricClusters2Pairs.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "(df_copy / 4).plot(marker = 'o', xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10))\n",
    "plt.xlabel(r'Cluster size $n$', fontsize = 20)\n",
    "plt.ylabel(r'Error', fontsize = 20)\n",
    "plt.grid(True)\n",
    "# make legend larger\n",
    "plt.legend(fontsize='large', bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Review-Data/4GeometricClusters2Pairs.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd620164e342e3b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "No we try a similar experiment but rather than driving the means apart we drive the means of the distributions apart. So,\n",
    "$X_1 \\sim \\mathcal{N}((0,0), I)$,\n",
    "$X_2 \\sim \\mathcal{N}((0,5), I)$,\n",
    "$X_3 \\sim \\mathcal{N}((d,0), I)$,\n",
    "$X_4 \\sim \\mathcal{N}((d,5), I)$\n",
    "d is a parameter that is varied and cluster size n is fixed at 100.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c394c5e82cec1304"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9cff1278d6e281aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rst_bounds = {}\n",
    "st_bounds = {}\n",
    "k_way_bounds = {}\n",
    "true_values = {}\n",
    "n_clusters = 4\n",
    "n = 100\n",
    "sample_size = 1\n",
    "for d in np.arange(5, 12, 0.5):\n",
    "    rst_bounds_temp = []\n",
    "    st_bounds_temp = []\n",
    "    k_way_bounds_temp = []\n",
    "    true_values_temp = []\n",
    "    print(f\"commencing computation for d = {d}\")\n",
    "    for p in range(sample_size):\n",
    "    \n",
    "        X1 = np.random.multivariate_normal(mean = [0,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X2 = np.random.multivariate_normal(mean = [0,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X3 = np.random.multivariate_normal(mean = [d,0], cov = [[1,0],[0,1]], size = n)\n",
    "        X4 = np.random.multivariate_normal(mean = [d,5], cov = [[1,0],[0,1]], size = n)\n",
    "        X = np.concatenate((X1, X2, X3, X4))\n",
    "        \n",
    "        A = construct_graph(X, 4)\n",
    "        degrees = np.sum(A, axis = 1)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        D_inv_sqrt = np.diag(1 / np.sqrt(degrees))\n",
    "        norm_L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        labels = kmeans.fit_predict(eigvecs[:, 0:n_clusters])\n",
    "        \n",
    "        #compute exact indicator vectors (from clusters from distributions not k means)\n",
    "        exact_indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[i*n:(i+1)*n,i] = 1\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            exact_indicator_vectors[:, i] = D_sqrt @ exact_indicator_vectors[:, i]\n",
    "            exact_indicator_vectors[:, i] = exact_indicator_vectors[:, i] / np.linalg.norm(exact_indicator_vectors[:, i])\n",
    "        \n",
    "        true_val = np.linalg.norm(eigvecs[:,:n_clusters] - exact_indicator_vectors @ (exact_indicator_vectors.T @ eigvecs[:,:n_clusters]), ord = 'fro')**2\n",
    "        \n",
    "        # indicator vectors from k means\n",
    "        indicator_vectors = np.zeros((eigvecs.shape[0], n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = (labels == i).astype(int)\n",
    "        \n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            indicator_vectors[:, i] = D_sqrt @ indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i]\n",
    "            indicator_vectors[:, i] = indicator_vectors[:, i] / np.linalg.norm(indicator_vectors[:, i])\n",
    "        \n",
    "        # project indicator vectors onto the eigenvectors\n",
    "        beta_K_by_K = (indicator_vectors.T @ eigvecs[:,:n_clusters])\n",
    "        \n",
    "        combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "                combined_indicator_vectors[:, i])\n",
    "            for j in range(i):\n",
    "                combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                            combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                                 i]) * combined_indicator_vectors[:, j]\n",
    "            \n",
    "        for i in range(n_clusters):\n",
    "            combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(combined_indicator_vectors[:, i])\n",
    "            \n",
    "            \n",
    "        # compute the rayleigh quotients\n",
    "        rayleigh_quotients = []\n",
    "        for i in range(n_clusters):\n",
    "            indicator = combined_indicator_vectors[:, i]\n",
    "            val = (indicator.T @ norm_L @ indicator) / (indicator.T @ indicator)\n",
    "            rayleigh_quotients.append(val)\n",
    "        \n",
    "        # sort the rayleigh quotients\n",
    "        sorted_rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        B_1 = (sorted_rayleigh_quotients[0] + sorted_rayleigh_quotients[1])/eigvals[2]\n",
    "        B_2 = ((sorted_rayleigh_quotients[2] + sorted_rayleigh_quotients[3]) - 2*eigvals[2] + eigvals[4]*B_1)/(eigvals[4] - eigvals[2])\n",
    "        \n",
    "        rst_bound = B_1 + B_2\n",
    "        st_bound = (np.sum(sorted_rayleigh_quotients))/(eigvals[4])\n",
    "        rho = compute_k_way_estimate(norm_L, indicator_vectors, 4)\n",
    "        k_way_bound = 4*rho/eigvals[4]\n",
    "        \n",
    "        rst_bounds_temp.append(rst_bound)\n",
    "        st_bounds_temp.append(st_bound)\n",
    "        k_way_bounds_temp.append(k_way_bound)\n",
    "        true_values_temp.append(true_val)\n",
    "    \n",
    "    rst_bounds[d] = np.mean(rst_bounds_temp)\n",
    "    st_bounds[d] = np.mean(st_bounds_temp)\n",
    "    k_way_bounds[d] = np.mean(k_way_bounds_temp)\n",
    "    true_values[d] = np.mean(true_values_temp)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8514e208bb18d310"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds_df = pd.DataFrame([rst_bounds, st_bounds, k_way_bounds, true_values]).T\n",
    "bounds_df.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "bounds_df.to_csv(\"Review-Data/4GeometricClusters2PairsDriftingApart.csv\")\n",
    "\n",
    "bounds_df = pd.read_csv(\"Review-Data/4GeometricClusters2PairsDriftingApart.csv\", usecols=lambda column: column != \"Unnamed: 0\")\n",
    "\n",
    "fig = plt.figure()\n",
    "(bounds_df / 4).plot(marker='x', markersize=10, figsize=(12, 10))\n",
    "\n",
    "#plt.legend(bbox_to_anchor=(1,1.05))\n",
    "plt.xlabel(\"Distance, d\", fontsize=30)\n",
    "plt.ylabel(\"Error\", fontsize=30)\n",
    "plt.legend(fontsize=25)\n",
    "plt.grid(visible=True, which='both', linewidth=1.5)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1), fontsize=25)\n",
    "# plt.title(\n",
    "#     r\"Bounds for $\\frac{1}{4}\\sum_{i=1}^4\\|f_i - \\hat{g}_i\\|^2$ generated from Gaussian mixture model (4 clusters, 2 pairs)\" + \"\\n Distance between pairs of clusters increased\",\n",
    "#     y=1.03)\n",
    "plt.savefig(\"Review-Data/BoundsGaussianMixtureModel2PairsDriftingApart.png\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d0566a4d47b9fcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bounds for Stochastic Block Models\n",
    "Our initial choice is an SBM with 4 clusters and where two pairs have a high affinity for each other. We use \n",
    "$$\n",
    "P = \\begin{pmatrix}\n",
    "    0.5 & 0.4 & 0.1 & 0.1 \\\\\n",
    "    0.4 & 0.5 & 0.1 & 0.1 \\\\\n",
    "    0.1 & 0.1 & 0.5 & 0.4 \\\\\n",
    "    0.1 & 0.1 & 0.4 & 0.5 \\\\\n",
    "    \\end{pmatrix}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d61fde072491fb1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "P = np.array([[0.5, 0.4, 0.1, 0.1],\n",
    "              [0.4, 0.5, 0.1, 0.1],\n",
    "              [0.1, 0.1, 0.5, 0.4],\n",
    "              [0.1, 0.1, 0.4, 0.5]])\n",
    "K = 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9065f801e076b6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds = {}\n",
    "sample_size = 10\n",
    "for n in [200,300,400,500,600,700,800,900]:\n",
    "    bounds[n] = 0\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i,K):\n",
    "                prob_existing_edge = P[i,j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u+1,n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "                                \n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "        \n",
    "        true_clusters = [list(range(i*n, (i+1)*n)) for i in range(K)]                        \n",
    "        G = Graph(vertices = list(range(n * K)), edges = edges)\n",
    "        norm_L = get_laplacian_matrix(G, normalized=True)\n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        eigvecs = eigvecs[:, 0:K]\n",
    "        \n",
    "        D = get_degree_matrix(G)\n",
    "        degrees = np.diag(D)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        indicator_vectors = k_means_indicator_vectors(eigvecs, K)\n",
    "        degree_corrected_indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "        combined_indicator_vectors = get_normalised_projected_indicator_vectors(eigvecs,degree_corrected_indicator_vectors, K)\n",
    "        rayleigh_quotients = compute_rayleigh_quotients(norm_L, combined_indicator_vectors, K)\n",
    "        rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        \n",
    "        true_value = (1/K) * np.linalg.norm(eigvecs - degree_corrected_indicator_vectors @ (degree_corrected_indicator_vectors.T @ eigvecs), ord = 'fro')**2\n",
    "        general_ST = (1/K) * (np.sum(rayleigh_quotients))/eigvals[K]\n",
    "        ST_standard = compute_k_way_estimate(norm_L, degree_corrected_indicator_vectors, K)/ eigvals[K]\n",
    "        min_split_indices, min_split = apply_recursive_st_brute_force(rayleigh_quotients,eigvals, K)\n",
    "        recursive_ST = (1/K) * min_split\n",
    "        \n",
    "        \n",
    "        \n",
    "        bounds[n] = bounds[n] + pd.Series({'Recursive ST': recursive_ST,\n",
    "            'General ST': general_ST,\n",
    "            'ST Standard': ST_standard,\n",
    "            'True Value': true_value} )\n",
    "        \n",
    "    bounds[n] = bounds[n] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dd66be83da6818e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "columns = ['Recursive ST', 'General ST', r'$\\frac{\\rho(4)}{\\lambda_5}$', 'True Value']\n",
    "df.columns = columns\n",
    "#df = df.drop(columns = ['General ST'])\n",
    "df.to_csv('Review-Data/Bounds4Clusters2PairsRST.csv')\n",
    "\n",
    "df_copy = pd.read_csv(\"Review-Data/Bounds4Clusters2PairsRST.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "df_copy.plot(marker='x', markersize=10, xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10))\n",
    "\n",
    "plt.xlabel(r'Cluster size $n$', fontsize=30)\n",
    "plt.ylabel(r'Error',fontsize=30)\n",
    "# add ticks\n",
    "#plt.xticks(np.arange(200,1000,100))\n",
    "#plt.yticks(np.arange(0,1.2,0.1))\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0,1.1,0.1),fontsize=25)\n",
    "plt.grid(visible=True, which='both', linewidth=1.5)\n",
    "# make legend larger\n",
    "plt.legend(fontsize=25, bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Review-Data/Bounds4Clusters2PairsRST.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abab89c8120c9197"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Repeating the experiment for an SBM with 8 clusters with a single pair that have an affinity for each other."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb9c802d40a06121"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 8\n",
    "p1 = 0.5\n",
    "p2 = 0.3\n",
    "q = 0.05\n",
    "P = np.ones((8,8))\n",
    "P = q*P\n",
    "np.fill_diagonal(P,p1)\n",
    "P[0,1] = p2\n",
    "P[1,0] = p2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e8cd03ea5ae6abe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bounds = {}\n",
    "sample_size = 1\n",
    "for n in [200,300,400,500,600,700,800,900]:\n",
    "    bounds[n] = 0\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i,K):\n",
    "                prob_existing_edge = P[i,j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u+1,n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "                                \n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "        \n",
    "        true_clusters = [list(range(i*n, (i+1)*n)) for i in range(K)]                        \n",
    "        G = Graph(vertices = list(range(n * K)), edges = edges)\n",
    "        norm_L = get_laplacian_matrix(G, normalized=True)\n",
    "        eigvals, eigvecs = np.linalg.eigh(norm_L)\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        eigvecs = eigvecs[:, 0:K]\n",
    "        \n",
    "        D = get_degree_matrix(G)\n",
    "        degrees = np.diag(D)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        indicator_vectors = k_means_indicator_vectors(eigvecs, K)\n",
    "        degree_corrected_indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "        combined_indicator_vectors = get_normalised_projected_indicator_vectors(eigvecs,degree_corrected_indicator_vectors, K)\n",
    "        rayleigh_quotients = compute_rayleigh_quotients(norm_L, combined_indicator_vectors, K)\n",
    "        rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        \n",
    "        true_value = (1/K) * np.linalg.norm(eigvecs - degree_corrected_indicator_vectors @ (degree_corrected_indicator_vectors.T @ eigvecs), ord = 'fro')**2\n",
    "        general_ST = (1/K) * (np.sum(rayleigh_quotients))/eigvals[K]\n",
    "        ST_standard = compute_k_way_estimate(norm_L, degree_corrected_indicator_vectors, K)/ eigvals[K]\n",
    "        min_split_indices, min_split = apply_recursive_st_brute_force(rayleigh_quotients,eigvals, K)\n",
    "        recursive_ST = (1/K) * min_split\n",
    "        \n",
    "        \n",
    "        \n",
    "        bounds[n] = bounds[n] + pd.Series({'Recursive ST': recursive_ST,\n",
    "            'General ST': general_ST,\n",
    "            'ST Standard': ST_standard,\n",
    "            'True Value': true_value} )\n",
    "        \n",
    "    bounds[n] = bounds[n] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bacbf4567eac890"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "\n",
    "columns = ['Recursive ST', 'General ST', r'$\\frac{\\rho(8)}{\\lambda_9}$', 'True Value']\n",
    "df.columns = columns\n",
    "#df = df.drop(columns = ['General ST'])\n",
    "df.to_csv(\"Review-Data/Bounds8Clusters1PairRST.csv\")\n",
    "df_copy = pd.read_csv(\"Review-Data/Bounds8Clusters1PairRST.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "df_copy.plot(marker = 'x', markersize=10, xlabel = 'n', ylabel = 'Bound Value', figsize = (12,10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$', fontsize = 30)\n",
    "plt.ylabel(r'Error', fontsize = 30)\n",
    "plt.grid(True, which='both', linewidth=1.5)\n",
    "# add ticks\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0,1.1,0.1), fontsize=25)\n",
    "# make legend larger\n",
    "plt.legend(fontsize=25, bbox_to_anchor = (1.0,1.05))\n",
    "plt.savefig('Review-Data/Bounds8Clusters1PairRST.png', bbox_inches = \"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0916cd549fb2d30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Repeating the experiment for an  SBM with 12 clusters and the following probability matrix:\n",
    "$$P = \\begin{pmatrix}\n",
    "0.9 & 0.7 & 0.2 & 0.2 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.7 & 0.9 & 0.2 & 0.2 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.2 & 0.2 & 0.9 & 0.7 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.2 & 0.2 & 0.7 & 0.9 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.9 & 0.7 & 0.2 & 0.2 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.7 & 0.9 & 0.2 & 0.2 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.2 & 0.2 & 0.9 & 0.7 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.2 & 0.2 & 0.7 & 0.9 & 0.05 & 0.05 & 0.05 & 0.05 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.9 & 0.7 & 0.2 & 0.2 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.7 & 0.9 & 0.2 & 0.2 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.2 & 0.2 & 0.9 & 0.7 \\\\\n",
    "0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 & 0.2 & 0.2 & 0.7 & 0.9 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbaecf77287be34e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p1, p2, p3, p4 = 0.9, 0.7, 0.2, 0.05\n",
    "K = 12\n",
    "P = np.array([\n",
    "    [p1, p2, p3, p3, p4, p4, p4, p4, p4, p4, p4, p4],\n",
    "    [p2, p1, p3, p3, p4, p4, p4, p4, p4, p4, p4, p4],\n",
    "    [p3, p3, p1, p2, p4, p4, p4, p4, p4, p4, p4, p4],\n",
    "    [p3, p3, p2, p1, p4, p4, p4, p4, p4, p4, p4, p4],\n",
    "    [p4, p4, p4, p4, p1, p2, p3, p3, p4, p4, p4, p4],\n",
    "    [p4, p4, p4, p4, p2, p1, p3, p3, p4, p4, p4, p4],\n",
    "    [p4, p4, p4, p4, p3, p3, p1, p2, p4, p4, p4, p4],\n",
    "    [p4, p4, p4, p4, p3, p3, p2, p1, p4, p4, p4, p4],\n",
    "    [p4, p4, p4, p4, p4, p4, p4, p4, p1, p2, p3, p3],\n",
    "    [p4, p4, p4, p4, p4, p4, p4, p4, p2, p1, p3, p3],\n",
    "    [p4, p4, p4, p4, p4, p4, p4, p4, p3, p3, p1, p2],\n",
    "    [p4, p4, p4, p4, p4, p4, p4, p4, p3, p3, p2, p1]\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "939a7e62ac571092"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy\n",
    "bounds = {}\n",
    "sample_size = 5\n",
    "for n in [200,300,400,500,600,700]:\n",
    "    bounds[n] = 0\n",
    "    for _ in range(sample_size):\n",
    "        edges = []\n",
    "        for i in range(K):\n",
    "            for j in range(i,K):\n",
    "                prob_existing_edge = P[i,j]\n",
    "                if i == j:\n",
    "                    for u in range(n):\n",
    "                        for v in range(u+1,n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "                                \n",
    "                else:\n",
    "                    for u in range(n):\n",
    "                        for v in range(n):\n",
    "                            if np.random.rand() <= prob_existing_edge:\n",
    "                                edges.append((i * n + u, j * n + v))\n",
    "        \n",
    "        true_clusters = [list(range(i*n, (i+1)*n)) for i in range(K)]                        \n",
    "        G = Graph(vertices = list(range(n * K)), edges = edges)\n",
    "        norm_L = get_laplacian_matrix(G, normalized=True)\n",
    "        eigvals, eigvecs = scipy.sparse.linalg.eigsh(norm_L,15, which='SM')\n",
    "        idx = np.argsort(eigvals)\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        eigvecs = eigvecs[:, 0:K]\n",
    "        \n",
    "        D = get_degree_matrix(G)\n",
    "        degrees = np.diag(D)\n",
    "        D_sqrt = np.diag(np.sqrt(degrees))\n",
    "        indicator_vectors = k_means_indicator_vectors(eigvecs, K)\n",
    "        degree_corrected_indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "        combined_indicator_vectors = get_normalised_projected_indicator_vectors(eigvecs,degree_corrected_indicator_vectors, K)\n",
    "        rayleigh_quotients = compute_rayleigh_quotients(norm_L, combined_indicator_vectors, K)\n",
    "        rayleigh_quotients = np.sort(rayleigh_quotients)\n",
    "        \n",
    "        true_value = (1/K) * np.linalg.norm(eigvecs - degree_corrected_indicator_vectors @ (degree_corrected_indicator_vectors.T @ eigvecs), ord = 'fro')**2\n",
    "        general_ST = (1/K) * (np.sum(rayleigh_quotients))/eigvals[K]\n",
    "        ST_standard = compute_k_way_estimate(norm_L, degree_corrected_indicator_vectors, K)/ eigvals[K]\n",
    "        min_split_indices, min_split = apply_recursive_st_brute_force(rayleigh_quotients,eigvals, K)\n",
    "        recursive_ST = (1/K) * min_split\n",
    "        \n",
    "        \n",
    "        \n",
    "        bounds[n] = bounds[n] + pd.Series({'Recursive ST': recursive_ST,\n",
    "            'General ST': general_ST,\n",
    "            'ST Standard': ST_standard,\n",
    "            'True Value': true_value} )\n",
    "        \n",
    "    bounds[n] = bounds[n] / sample_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7737feb8ade2b9b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bounds).T\n",
    "\n",
    "columns = ['Recursive ST', 'General ST', r'$\\frac{\\rho(8)}{\\lambda_9}$', 'True Value']\n",
    "df.columns = columns\n",
    "#df = df.drop(columns = ['General ST'])\n",
    "df.to_csv(\"Review-Data/Bounds12Clusters3QuartetsRST.csv\")\n",
    "df_copy = pd.read_csv(\"Review-Data/Bounds12Clusters3QuartetsRST.csv\")\n",
    "df_copy = df_copy.set_index([\"Unnamed: 0\"])\n",
    "df_copy.columns = [\"Theorem 4\", \"Theorem 1\", \"Macgregor & Sun\", \"True Value\"]\n",
    "df_copy.plot(marker='x', markersize=10, xlabel='n', ylabel='Bound Value', figsize=(12, 10))\n",
    "# plt.title(r'Bounds of $\\frac{1}{8}\\sum_{i=1}^8\\|f_i - \\hat{g}_i\\|^2$ for SBM with 8 clusters (with one pair)', y=1.03)\n",
    "plt.xlabel(r'Cluster size $n$', fontsize=30)\n",
    "plt.ylabel(r'Error', fontsize=30)\n",
    "plt.grid(True, which='both', linewidth=1.5)\n",
    "# add ticks\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1), fontsize=25)\n",
    "# make legend larger\n",
    "plt.legend(fontsize=25, bbox_to_anchor=(1.0, 1.05))\n",
    "plt.savefig('Review-Data/Bounds12Clusters3QuartetsRST.png', bbox_inches=\"tight\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e033821854503237"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Real-World Networks\n",
    "In this section we produce our bounds for a collection of real-world networks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dc057242c1fca91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MNIST Dataset\n",
    "Sourced from openml. We sample 500 images each of digits 0,1,2,3 and 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3af90e5730fe4c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_results = {}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a9889693fec249a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the MNIST dataset using OpenML\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "x_data, y_data = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Select 3 digits (e.g., 0, 1, and 2) and restrict to 200 samples each\n",
    "digits = [0, 1, 2, 3, 4]\n",
    "samples_per_digit = 500\n",
    "selected_samples = []\n",
    "selected_labels = []\n",
    "\n",
    "# taking a sample of each digit\n",
    "for digit in digits:\n",
    "    indices = np.where(y_data == digit)[0][:samples_per_digit]\n",
    "    selected_samples.append(x_data[indices])\n",
    "    selected_labels.extend(y_data[indices])\n",
    "\n",
    "# Combine the samples into a single dataset\n",
    "selected_samples = np.vstack(selected_samples) / 255.0  # Normalize pixel values\n",
    "selected_labels = np.array(selected_labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74fe69ebcb3e43c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a graph using the correlation matrix\n",
    "threshold = 0.7  # Define a threshold for edge creation\n",
    "adjacency_matrix = get_thresholded_correlation_matrix(selected_samples, threshold)\n",
    "adjacency_matrix_largest_cc, largest_cc = largest_connected_component(adjacency_matrix) \n",
    "normalized_laplacian = get_normalised_laplacian(adjacency_matrix_largest_cc)\n",
    "\n",
    "# compute first K eigenvectors of the normalized Laplacian\n",
    "K=5\n",
    "normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "idx = normalized_L_eigenvalues.argsort()\n",
    "normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "degrees = np.sum(adjacency_matrix_largest_cc, axis = 0)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "\n",
    "indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "for i in range(K):\n",
    "    combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "        combined_indicator_vectors[:, i])\n",
    "    for j in range(i):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                    combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                         i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "rayleigh_quotients = compute_rayleigh_quotients(normalized_laplacian, combined_indicator_vectors, K)\n",
    "rayleigh_quotients = np.sort(rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5b5a655f9aee4fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_val_matrix_combined = normalized_L_eigenvectors[:, :K] - indicator_vectors @ (\n",
    "            indicator_vectors.T @ normalized_L_eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, normalized_L_eigenvalues[0:K+1], K)\n",
    "\n",
    "mnist_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "mnist_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(adjacency_matrix_largest_cc))\n",
    "mnist_dataset_results[\"N\"] = len(adjacency_matrix_largest_cc)\n",
    "print(\"Number of edges M: \", np.sum(np.sum(adjacency_matrix_largest_cc, axis=1), axis=0))\n",
    "mnist_dataset_results[\"M\"] = np.sum(np.sum(adjacency_matrix_largest_cc, axis=1), axis=0)\n",
    "print(\"True value: \", true_val_combined)\n",
    "mnist_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \",\n",
    "      max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors)) / normalized_L_eigenvalues[K])\n",
    "mnist_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors)) / \\\n",
    "                                  normalized_L_eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients) / (K * normalized_L_eigenvalues[K]))\n",
    "mnist_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients) / (K * normalized_L_eigenvalues[K])\n",
    "print(\"Recursive ST: \", v / K)\n",
    "mnist_dataset_results[\"Recursive ST\"] = v / K\n",
    "\n",
    "dataset_results[\"MNIST\"] = mnist_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec3856f5b367edc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fashion MNIST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322bc8fcc4e8ff0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the MNIST dataset using OpenML\n",
    "mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "x_data, y_data = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Select 3 digits (e.g., 0, 1, and 2) and restrict to 200 samples each\n",
    "digits = [0, 1, 2]\n",
    "samples_per_digit = 1000\n",
    "selected_samples = []\n",
    "selected_labels = []\n",
    "\n",
    "# taking a sample of each digit\n",
    "for digit in digits:\n",
    "    indices = np.where(y_data == digit)[0][:samples_per_digit]\n",
    "    selected_samples.append(x_data[indices])\n",
    "    selected_labels.extend(y_data[indices])\n",
    "\n",
    "# Combine the samples into a single dataset\n",
    "selected_samples = np.vstack(selected_samples) / 255.0  # Normalize pixel values\n",
    "selected_labels = np.array(selected_labels)\n",
    "\n",
    "# Create a graph using the correlation matrix\n",
    "threshold = 0.7  # Define a threshold for edge creation\n",
    "adjacency_matrix = get_thresholded_correlation_matrix(selected_samples, threshold)\n",
    "adjacency_matrix_largest_cc, largest_cc = largest_connected_component(adjacency_matrix) \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49e450ad8a9f342"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This dataset had some vertices that "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15331d734ac5adfa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vertices_to_remove = [318,319,320]\n",
    "adjacency_matrix_largest_cc = np.delete(adjacency_matrix_largest_cc, vertices_to_remove, axis=0)\n",
    "adjacency_matrix_largest_cc = np.delete(adjacency_matrix_largest_cc, vertices_to_remove, axis=1)\n",
    "normalized_laplacian = get_normalised_laplacian(adjacency_matrix_largest_cc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fde46fb6ce701081"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute first K eigenvectors of the normalized Laplacian\n",
    "K=6\n",
    "normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "idx = normalized_L_eigenvalues.argsort()\n",
    "normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "degrees = np.sum(adjacency_matrix_largest_cc, axis = 0)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "\n",
    "indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "for i in range(K):\n",
    "    combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "        combined_indicator_vectors[:, i])\n",
    "    for j in range(i):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                    combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                         i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "rayleigh_quotients = compute_rayleigh_quotients(normalized_laplacian, combined_indicator_vectors, K)\n",
    "rayleigh_quotients = np.sort(rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51cb0fb9f2d86e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, normalized_L_eigenvalues[0:7], K)\n",
    "true_val_matrix_combined = normalized_L_eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ normalized_L_eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "fashion_mnist_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "fashion_mnist_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(adjacency_matrix_largest_cc))\n",
    "fashion_mnist_dataset_results[\"N\"] = len(adjacency_matrix_largest_cc)\n",
    "print(\"Number of edges M: \", np.sum(np.sum(adjacency_matrix_largest_cc, axis=1), axis=0))\n",
    "fashion_mnist_dataset_results[\"M\"] = np.sum(np.sum(adjacency_matrix_largest_cc, axis=1), axis=0)\n",
    "print(\"True value: \", true_val_combined)\n",
    "fashion_mnist_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \",\n",
    "      max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors)) / normalized_L_eigenvalues[K])\n",
    "fashion_mnist_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors)) / \\\n",
    "                                          normalized_L_eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients) / (K * normalized_L_eigenvalues[K]))\n",
    "fashion_mnist_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients) / (K * normalized_L_eigenvalues[K])\n",
    "print(\"Recursive ST: \", np.sum(v) / K)\n",
    "fashion_mnist_dataset_results[\"Recursive ST\"] = np.sum(v) / K\n",
    "\n",
    "dataset_results[\"Fashion MNIST\"] = fashion_mnist_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d23f9a217075294"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Air Quality Dataset\n",
    "Pulled from: https://www.kaggle.com/datasets/mujtabamatin/air-quality-and-pollution-assessment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6ddfecac45aabd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pollution_df = pd.read_csv(\"Data/updated_pollution_dataset.csv\")\n",
    "pollution_df = pollution_df.sort_values(by='Air Quality').reset_index(drop=True)\n",
    "\n",
    "# Separate targets and features\n",
    "pollution_df_targets = pollution_df['Air Quality']\n",
    "pollution_df_features = pollution_df.drop(columns=['Air Quality'])\n",
    "\n",
    "# Normalize features\n",
    "pollution_df_features_normalized = (pollution_df_features - pollution_df_features.mean()) / pollution_df_features.std()\n",
    "\n",
    "# Calculate covariance matrix\n",
    "pollution_df_features_cov = np.cov(pollution_df_features_normalized)\n",
    "\n",
    "# Threshold covariance matrix\n",
    "pollution_df_features_cov_thresholded = pollution_df_features_cov.copy()\n",
    "pollution_df_features_cov_thresholded[pollution_df_features_cov_thresholded < 0.4] = 0\n",
    "pollution_df_features_cov_thresholded[pollution_df_features_cov_thresholded >= 0.4] = 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cb337c3694fd597"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing disconnected components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "696929baf359520e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove zero rows and columns\n",
    "non_zero_indices = np.where(~(pollution_df_features_cov_thresholded == 0).all(axis=1))[0]\n",
    "pollution_df_features_cov_thresholded = pollution_df_features_cov_thresholded[non_zero_indices][:, non_zero_indices]\n",
    "\n",
    "# Adjacency matrix\n",
    "adjacency_matrix = pollution_df_features_cov_thresholded.copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f510a59f6ba07ebd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalized_laplacian = get_normalised_laplacian(adjacency_matrix)\n",
    "\n",
    "# compute first K eigenvectors of the normalized Laplacian\n",
    "K = 3\n",
    "normalized_L_eigenvalues, normalized_L_eigenvectors = np.linalg.eigh(normalized_laplacian)\n",
    "idx = normalized_L_eigenvalues.argsort()\n",
    "normalized_L_eigenvalues = normalized_L_eigenvalues[idx]\n",
    "normalized_L_eigenvectors = normalized_L_eigenvectors[:, idx]\n",
    "\n",
    "indicator_vectors = k_means_indicator_vectors(normalized_L_eigenvectors[:, 0:K], K)\n",
    "degrees = np.sum(adjacency_matrix, axis=0)\n",
    "D_sqrt = np.diag(np.sqrt(degrees))\n",
    "\n",
    "indicator_vectors = degree_correction(indicator_vectors, D_sqrt)\n",
    "beta_K_by_K = indicator_vectors.T @ normalized_L_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "for i in range(K):\n",
    "    combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] / np.linalg.norm(\n",
    "        combined_indicator_vectors[:, i])\n",
    "    for j in range(i):\n",
    "        combined_indicator_vectors[:, i] = combined_indicator_vectors[:, i] - (\n",
    "                combined_indicator_vectors[:, j].T @ combined_indicator_vectors[:,\n",
    "                                                     i]) * combined_indicator_vectors[:, j]\n",
    "\n",
    "rayleigh_quotients = compute_rayleigh_quotients(normalized_laplacian, combined_indicator_vectors, K)\n",
    "rayleigh_quotients = np.sort(rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d04caba4a591815b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_val_matrix_combined = normalized_L_eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ normalized_L_eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "rayleigh_quotients = np.round(rayleigh_quotients, 6)\n",
    "normalized_L_eigenvalues = np.round(normalized_L_eigenvalues, 6)\n",
    "\n",
    "\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, normalized_L_eigenvalues[0:K+1], K)\n",
    "\n",
    "\n",
    "air_quality_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "air_quality_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(adjacency_matrix))\n",
    "air_quality_dataset_results[\"N\"] = len(adjacency_matrix)\n",
    "print(\"Number of edges M: \", np.sum(np.sum(adjacency_matrix, axis=1), axis=0))\n",
    "air_quality_dataset_results[\"M\"] = np.sum(np.sum(adjacency_matrix, axis=1), axis=0)\n",
    "print(\"True value: \", true_val_combined)\n",
    "air_quality_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \", max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors))/normalized_L_eigenvalues[K])\n",
    "air_quality_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ normalized_laplacian @ indicator_vectors))/normalized_L_eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients)/(K * normalized_L_eigenvalues[K]))\n",
    "air_quality_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients)/(K * normalized_L_eigenvalues[K])\n",
    "print(\"Recursive ST: \", v/K)\n",
    "air_quality_dataset_results[\"Recursive ST\"] = v/K\n",
    "\n",
    "dataset_results[\"Air Quality\"] = air_quality_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7941dbdbecdf52b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Twitch\n",
    "From SNAP."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e28ef3b299e4e25a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 1: Define the file path\n",
    "edge_list_path = \"Data/twitch_gamers/large_twitch_edges.csv\"\n",
    "df = pd.read_csv(edge_list_path)\n",
    "df.to_csv(\"Data/twitch_gamers/twitch_cleaned_edges.csv\", index=False, header=False)\n",
    "# Step 2: Load the Deezer graph\n",
    "# Assuming the edge list is in CSV format with two columns: source and target\n",
    "G = nx.read_edgelist(\"Data/twitch_gamers/twitch_cleaned_edges.csv\", delimiter=\",\", nodetype=int, data=False)\n",
    "\n",
    "laplacian_sparse = nx.normalized_laplacian_matrix(G).astype(np.float64)\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "inv_sqrt_degrees = np.array([1 / np.sqrt(degrees[node]) if degrees[node] != 0 else 0 for node in G.nodes()])\n",
    "\n",
    "# Step 4: Compute the smallest 6 eigenvalues of the Laplacian (for example)\n",
    "num_eigenvalues = 20\n",
    "eigenvalues, eigenvectors = eigsh(laplacian_sparse, k=num_eigenvalues, which=\"SM\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3a9892a2d7ffd91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 2\n",
    "# Perform K-means clustering on the eigenvectors\n",
    "kmeans = KMeans(n_clusters=K, n_init=10, random_state=42)\n",
    "scaled_eigenvectors = inv_sqrt_degrees[:, np.newaxis] * eigenvectors\n",
    "\n",
    "clusters = kmeans.fit_predict(scaled_eigenvectors[:, :K])\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "sqrt_degrees = np.array([np.sqrt(degrees[node]) for node in G.nodes()])\n",
    "\n",
    "# Compute Rayleigh quotients for each cluster\n",
    "rayleigh_quotients = []\n",
    "indicator_vectors = np.zeros(eigenvectors.shape)\n",
    "\n",
    "for cluster_idx in range(K):\n",
    "    # Create indicator vector for the cluster\n",
    "    indicator_vector = (clusters == cluster_idx).astype(float) * sqrt_degrees\n",
    "    indicator_vector = indicator_vector / np.linalg.norm(indicator_vector, 2)\n",
    "    indicator_vectors[:, cluster_idx] = indicator_vector\n",
    "    # Compute Rayleigh quotient\n",
    "    numerator = indicator_vector.T @ laplacian_sparse @ indicator_vector\n",
    "    denominator = indicator_vector.T @ indicator_vector\n",
    "    rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "rayleigh_quotients = sorted(rayleigh_quotients)\n",
    "beta_K_by_K = indicator_vectors.T @ scaled_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "combined_indicator_vectors = gram_schmidt(combined_indicator_vectors)\n",
    "combined_rayleigh_quotients = []\n",
    "for i in range(K):\n",
    "    numerator = combined_indicator_vectors[:, i].T @ laplacian_sparse @ combined_indicator_vectors[:, i]\n",
    "    denominator = combined_indicator_vectors[:, i].T @ combined_indicator_vectors[:, i]\n",
    "    combined_rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "combined_rayleigh_quotients = sorted(combined_rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "143c1521098c0a65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "true_val_matrix_combined = eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "rayleigh_quotients = np.round(rayleigh_quotients, 6)\n",
    "normalized_L_eigenvalues = np.round(eigenvalues, 6)\n",
    "\n",
    "#b, v = apply_recursive_st_search(rayleigh_quotients[0:4], normalized_L_eigenvalues[0:4], 0, error = 0, bound_indices = [], values = [])\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, eigenvalues[0:K+1], K)\n",
    "\n",
    "\n",
    "twitch_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "twitch_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(eigenvectors))\n",
    "twitch_dataset_results[\"N\"] = len(eigenvectors)\n",
    "print(\"Number of edges M: \", np.sum(list(degrees.values())) / 2)\n",
    "twitch_dataset_results[\"M\"] = np.sum(list(degrees.values())) / 2\n",
    "print(\"True value: \", true_val_combined)\n",
    "twitch_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \", max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K])\n",
    "twitch_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients)/(K * eigenvalues[K]))\n",
    "twitch_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients)/(K * eigenvalues[K])\n",
    "print(\"Recursive ST: \", v/K)\n",
    "twitch_dataset_results[\"Recursive ST\"] = v/K\n",
    "\n",
    "dataset_results[\"Twitch\"] = twitch_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b00e02f8545d8b31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Last Fm Asia"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3977b0b9488674b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 1: Define the file path\n",
    "edge_list_path = \"Data/lasftm_asia/lastfm_asia_edges.csv\"\n",
    "df = pd.read_csv(edge_list_path)\n",
    "df.to_csv(\"Data/lasftm_asia/lastfm_asia_edges.csv\", index=False, header=False)\n",
    "# Step 2: Load the Deezer graph\n",
    "# Assuming the edge list is in CSV format with two columns: source and target\n",
    "G = nx.read_edgelist(\"Data/lasftm_asia/lastfm_asia_edges.csv\", delimiter=\",\", nodetype=int, data=False)\n",
    "\n",
    "laplacian_sparse = nx.normalized_laplacian_matrix(G).astype(np.float64)\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "inv_sqrt_degrees = np.array([1 / np.sqrt(degrees[node]) if degrees[node] != 0 else 0 for node in G.nodes()])\n",
    "\n",
    "# Step 4: Compute the smallest 6 eigenvalues of the Laplacian (for example)\n",
    "num_eigenvalues = 20\n",
    "eigenvalues, eigenvectors = eigsh(laplacian_sparse, k=num_eigenvalues, which=\"SM\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f69fede9155acd7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 2\n",
    "# Perform K-means clustering on the eigenvectors\n",
    "kmeans = KMeans(n_clusters=K, n_init=10, random_state=42)\n",
    "scaled_eigenvectors = inv_sqrt_degrees[:, np.newaxis] * eigenvectors\n",
    "\n",
    "clusters = kmeans.fit_predict(scaled_eigenvectors[:, :K])\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "sqrt_degrees = np.array([np.sqrt(degrees[node]) for node in G.nodes()])\n",
    "\n",
    "# Compute Rayleigh quotients for each cluster\n",
    "rayleigh_quotients = []\n",
    "indicator_vectors = np.zeros(eigenvectors.shape)\n",
    "\n",
    "for cluster_idx in range(K):\n",
    "    # Create indicator vector for the cluster\n",
    "    indicator_vector = (clusters == cluster_idx).astype(float) * sqrt_degrees\n",
    "    indicator_vector = indicator_vector / np.linalg.norm(indicator_vector, 2)\n",
    "    indicator_vectors[:, cluster_idx] = indicator_vector\n",
    "    # Compute Rayleigh quotient\n",
    "    numerator = indicator_vector.T @ laplacian_sparse @ indicator_vector\n",
    "    denominator = indicator_vector.T @ indicator_vector\n",
    "    rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "rayleigh_quotients = sorted(rayleigh_quotients)\n",
    "beta_K_by_K = indicator_vectors.T @ scaled_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "combined_indicator_vectors = gram_schmidt(combined_indicator_vectors)\n",
    "combined_rayleigh_quotients = []\n",
    "for i in range(K):\n",
    "    numerator = combined_indicator_vectors[:, i].T @ laplacian_sparse @ combined_indicator_vectors[:, i]\n",
    "    denominator = combined_indicator_vectors[:, i].T @ combined_indicator_vectors[:, i]\n",
    "    combined_rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "combined_rayleigh_quotients = sorted(combined_rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "193220d844d778e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lastfm_dataset_results = {}\n",
    "true_val_matrix_combined = eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "rayleigh_quotients = np.round(rayleigh_quotients, 6)\n",
    "normalized_L_eigenvalues = np.round(eigenvalues, 6)\n",
    "\n",
    "#b, v = apply_recursive_st_search(rayleigh_quotients[0:4], normalized_L_eigenvalues[0:4], 0, error = 0, bound_indices = [], values = [])\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, eigenvalues[0:K+1], K)\n",
    "\n",
    "\n",
    "lastfm_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "lastfm_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(eigenvectors))\n",
    "lastfm_dataset_results[\"N\"] = len(eigenvectors)\n",
    "print(\"Number of edges M: \", np.sum(list(degrees.values())) / 2)\n",
    "lastfm_dataset_results[\"M\"] = np.sum(list(degrees.values())) / 2\n",
    "print(\"True value: \", true_val_combined)\n",
    "lastfm_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \", max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K])\n",
    "lastfm_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients)/(K * eigenvalues[K]))\n",
    "lastfm_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients)/(K * eigenvalues[K])\n",
    "print(\"Recursive ST: \", v/K)\n",
    "lastfm_dataset_results[\"Recursive ST\"] = v/K\n",
    "\n",
    "dataset_results[\"LastFM\"] = lastfm_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8ddd7459fcf5b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gemsec Facebook - Athletes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbbbb209d7b64f97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 1: Define the file path\n",
    "edge_list_path = \"Data/facebook_clean_data/athletes_edges.csv\"\n",
    "df = pd.read_csv(edge_list_path)\n",
    "df.to_csv(\"Data/facebook_clean_data/athletes_edges.csv\", index=False, header=False)\n",
    "# Step 2: Load the Deezer graph\n",
    "# Assuming the edge list is in CSV format with two columns: source and target\n",
    "G = nx.read_edgelist(\"Data/facebook_clean_data/athletes_edges.csv\", delimiter=\",\", nodetype=int, data=False)\n",
    "\n",
    "laplacian_sparse = nx.normalized_laplacian_matrix(G).astype(np.float64)\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "inv_sqrt_degrees = np.array([1 / np.sqrt(degrees[node]) if degrees[node] != 0 else 0 for node in G.nodes()])\n",
    "\n",
    "# Step 4: Compute the smallest 6 eigenvalues of the Laplacian (for example)\n",
    "num_eigenvalues = 20\n",
    "eigenvalues, eigenvectors = eigsh(laplacian_sparse, k=num_eigenvalues, which=\"SM\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "184a6ac8ad5ca18f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 3\n",
    "# Perform K-means clustering on the eigenvectors\n",
    "kmeans = KMeans(n_clusters=K, n_init=10, random_state=42)\n",
    "scaled_eigenvectors = inv_sqrt_degrees[:, np.newaxis] * eigenvectors\n",
    "\n",
    "clusters = kmeans.fit_predict(scaled_eigenvectors[:, :K])\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "sqrt_degrees = np.array([np.sqrt(degrees[node]) for node in G.nodes()])\n",
    "\n",
    "# Compute Rayleigh quotients for each cluster\n",
    "rayleigh_quotients = []\n",
    "indicator_vectors = np.zeros(eigenvectors.shape)\n",
    "\n",
    "for cluster_idx in range(K):\n",
    "    # Create indicator vector for the cluster\n",
    "    indicator_vector = (clusters == cluster_idx).astype(float) * sqrt_degrees\n",
    "    indicator_vector = indicator_vector / np.linalg.norm(indicator_vector, 2)\n",
    "    indicator_vectors[:, cluster_idx] = indicator_vector\n",
    "    # Compute Rayleigh quotient\n",
    "    numerator = indicator_vector.T @ laplacian_sparse @ indicator_vector\n",
    "    denominator = indicator_vector.T @ indicator_vector\n",
    "    rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "rayleigh_quotients = sorted(rayleigh_quotients)\n",
    "beta_K_by_K = indicator_vectors.T @ scaled_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "combined_indicator_vectors = gram_schmidt(combined_indicator_vectors)\n",
    "\n",
    "combined_rayleigh_quotients = []\n",
    "for i in range(K):\n",
    "    numerator = combined_indicator_vectors[:, i].T @ laplacian_sparse @ combined_indicator_vectors[:, i]\n",
    "    denominator = combined_indicator_vectors[:, i].T @ combined_indicator_vectors[:, i]\n",
    "    combined_rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "combined_rayleigh_quotients = sorted(combined_rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e695a0d490ab1fa7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "facebook_athletes_dataset_results = {}\n",
    "true_val_matrix_combined = eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "rayleigh_quotients = np.round(rayleigh_quotients, 6)\n",
    "normalized_L_eigenvalues = np.round(eigenvalues, 6)\n",
    "\n",
    "#b, v = apply_recursive_st_search(rayleigh_quotients[0:4], normalized_L_eigenvalues[0:4], 0, error = 0, bound_indices = [], values = [])\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, eigenvalues[0:K+1], K)\n",
    "\n",
    "\n",
    "facebook_athletes_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "facebook_athletes_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(eigenvectors))\n",
    "facebook_athletes_dataset_results[\"N\"] = len(eigenvectors)\n",
    "print(\"Number of edges M: \", np.sum(list(degrees.values())) / 2)\n",
    "facebook_athletes_dataset_results[\"M\"] = np.sum(list(degrees.values())) / 2\n",
    "print(\"True value: \", true_val_combined)\n",
    "facebook_athletes_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \", max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K])\n",
    "facebook_athletes_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients)/(K * eigenvalues[K]))\n",
    "facebook_athletes_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients)/(K * eigenvalues[K])\n",
    "print(\"Recursive ST: \", v/K)\n",
    "facebook_athletes_dataset_results[\"Recursive ST\"] = v/K\n",
    "\n",
    "dataset_results[\"Facebook (Athletes)\"] = facebook_athletes_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cf6d15798d7f735"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collaborations CA-CondMat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f71033631f91d3a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Define the file path\n",
    "# Define the file path for the input and output\n",
    "input_file = \"Data/ca-CondMat.txt/CA-CondMat.txt\"  # Replace with your actual file path\n",
    "output_file = \"Data/ca-CondMat.txt/edge_list.txt\"\n",
    "\n",
    "# Initialize a set to store unique edges (to ensure undirected representation)\n",
    "edge_set = set()\n",
    "\n",
    "# Open the input file and process each line\n",
    "with open(input_file, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Skip comment lines starting with '#'\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "\n",
    "        # Split the line into nodes\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            node1, node2 = parts\n",
    "\n",
    "            # Add the edge in a sorted order to avoid duplicates (undirected graph)\n",
    "            edge = tuple(sorted((int(node1), int(node2))))\n",
    "            edge_set.add(edge)\n",
    "\n",
    "# Write the unique edges to the output file\n",
    "with open(output_file, \"w\") as file:\n",
    "    for edge in sorted(edge_set):  # Sort edges for consistency\n",
    "        file.write(f\"{edge[0]}\\t{edge[1]}\\n\")\n",
    "\n",
    "print(f\"Edge list extracted and saved to {output_file}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0e9fc35e6172bfd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Load the Deezer graph\n",
    "# Assuming the edge list is in CSV format with two columns: source and target\n",
    "G = nx.read_edgelist(\"Data/ca-CondMat.txt/edge_list.txt\", nodetype=int)\n",
    "LCC_nodes = max(nx.connected_components(G), key=len)\n",
    "G = G.subgraph(LCC_nodes)\n",
    "laplacian_sparse = nx.normalized_laplacian_matrix(G).astype(np.float64)\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "inv_sqrt_degrees = np.array([1 / np.sqrt(degrees[node]) if degrees[node] != 0 else 0 for node in G.nodes()])\n",
    "\n",
    "# Step 4: Compute the smallest 6 eigenvalues of the Laplacian (for example)\n",
    "num_eigenvalues = 10\n",
    "eigenvalues, eigenvectors = eigsh(laplacian_sparse, k=num_eigenvalues, which=\"SM\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7a96a06f953175"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "K = 3\n",
    "# Perform K-means clustering on the eigenvectors\n",
    "kmeans = KMeans(n_clusters=K, n_init=10, random_state=42)\n",
    "scaled_eigenvectors = inv_sqrt_degrees[:, np.newaxis] * eigenvectors\n",
    "\n",
    "clusters = kmeans.fit_predict(scaled_eigenvectors[:, :K])\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "# Compute the square root of the degrees\n",
    "sqrt_degrees = np.array([np.sqrt(degrees[node]) for node in G.nodes()])\n",
    "\n",
    "# Compute Rayleigh quotients for each cluster\n",
    "rayleigh_quotients = []\n",
    "indicator_vectors = np.zeros(eigenvectors.shape)\n",
    "\n",
    "for cluster_idx in range(K):\n",
    "    # Create indicator vector for the cluster\n",
    "    indicator_vector = (clusters == cluster_idx).astype(float) * sqrt_degrees\n",
    "    indicator_vector = indicator_vector / np.linalg.norm(indicator_vector, 2)\n",
    "    indicator_vectors[:, cluster_idx] = indicator_vector\n",
    "    # Compute Rayleigh quotient\n",
    "    numerator = indicator_vector.T @ laplacian_sparse @ indicator_vector\n",
    "    denominator = indicator_vector.T @ indicator_vector\n",
    "    rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "rayleigh_quotients = sorted(rayleigh_quotients)\n",
    "beta_K_by_K = indicator_vectors.T @ scaled_eigenvectors[:, 0:K]\n",
    "combined_indicator_vectors = indicator_vectors @ beta_K_by_K\n",
    "combined_indicator_vectors = gram_schmidt(combined_indicator_vectors)\n",
    "combined_rayleigh_quotients = []\n",
    "for i in range(K):\n",
    "    numerator = combined_indicator_vectors[:, i].T @ laplacian_sparse @ combined_indicator_vectors[:, i]\n",
    "    denominator = combined_indicator_vectors[:, i].T @ combined_indicator_vectors[:, i]\n",
    "    combined_rayleigh_quotients.append(numerator / denominator)\n",
    "\n",
    "combined_rayleigh_quotients = sorted(combined_rayleigh_quotients)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed6a7fd599be8169"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collaborations_dataset_results = {}\n",
    "true_val_matrix_combined = eigenvectors[:, :K] - indicator_vectors @ (\n",
    "        indicator_vectors.T @ eigenvectors[:, :K])\n",
    "true_val_combined = (1 / K) * np.linalg.norm(true_val_matrix_combined)\n",
    "\n",
    "rayleigh_quotients = np.round(rayleigh_quotients, 6)\n",
    "normalized_L_eigenvalues = np.round(eigenvalues, 6)\n",
    "\n",
    "#b, v = apply_recursive_st_search(rayleigh_quotients[0:4], normalized_L_eigenvalues[0:4], 0, error = 0, bound_indices = [], values = [])\n",
    "split, v = apply_recursive_st_brute_force(rayleigh_quotients, eigenvalues[0:K+1], K)\n",
    "\n",
    "\n",
    "collaborations_dataset_results = {}\n",
    "print(\"K: \", K)\n",
    "collaborations_dataset_results[\"K\"] = K\n",
    "print(\"Number of vertices N: \", len(eigenvectors))\n",
    "collaborations_dataset_results[\"N\"] = len(eigenvectors)\n",
    "print(\"Number of edges M: \", np.sum(list(degrees.values())) / 2)\n",
    "collaborations_dataset_results[\"M\"] = np.sum(list(degrees.values())) / 2\n",
    "print(\"True value: \", true_val_combined)\n",
    "collaborations_dataset_results[\"True Value\"] = true_val_combined\n",
    "print(\"M&S ST: \", max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K])\n",
    "collaborations_dataset_results[\"M&S ST\"] = max(np.diag(indicator_vectors.T @ laplacian_sparse @ indicator_vectors))/eigenvalues[K]\n",
    "print(\"General ST: \", np.sum(rayleigh_quotients)/(K * eigenvalues[K]))\n",
    "collaborations_dataset_results[\"General ST\"] = np.sum(rayleigh_quotients)/(K * eigenvalues[K])\n",
    "print(\"Recursive ST: \", v/K)\n",
    "collaborations_dataset_results[\"Recursive ST\"] = v/K\n",
    "\n",
    "dataset_results[\"Collaborations (CA-CondMat)\"] = collaborations_dataset_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20dd8b7985267616"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d57bc9cf28c3f34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset_results).T"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "329f8b92ea33d7a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
